{
  "model": "grok-4",
  "service": "github",
  "task": "missing-semester__assign_contributor_labels",
  "runs": {
    "run-1": {
      "success": false,
      "error_message": null,
      "execution_time": 302.0962641239166,
      "token_usage": {
        "input_tokens": 2054255,
        "output_tokens": 1138,
        "total_tokens": 2063271,
        "reasoning_tokens": 7878
      },
      "turn_count": 20
    },
    "run-2": {
      "success": false,
      "error_message": "litellm.BadRequestError: XaiException - {\"code\":\"Client specified an invalid argument\",\"error\":\"This model's maximum prompt length is 256000 but the request contains 256312 tokens.\"}",
      "execution_time": 547.5941860675812,
      "token_usage": {
        "input_tokens": 976064,
        "output_tokens": 1514,
        "total_tokens": 984695,
        "reasoning_tokens": 7117
      },
      "turn_count": 6
    },
    "run-3": {
      "success": false,
      "error_message": "litellm.BadRequestError: XaiException - {\"code\":\"Client specified an invalid argument\",\"error\":\"This model's maximum prompt length is 256000 but the request contains 259008 tokens.\"}",
      "execution_time": 731.2305688858032,
      "token_usage": {
        "input_tokens": 534497,
        "output_tokens": 24012,
        "total_tokens": 564384,
        "reasoning_tokens": 5875
      },
      "turn_count": 5
    },
    "run-4": {
      "success": false,
      "error_message": null,
      "execution_time": 181.8811445236206,
      "token_usage": {
        "input_tokens": 383482,
        "output_tokens": 851,
        "total_tokens": 390086,
        "reasoning_tokens": 5753
      },
      "turn_count": 7
    }
  }
}