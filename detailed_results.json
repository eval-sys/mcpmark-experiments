{
  "experiment_name": "GITHUB-MULTI-RUN",
  "k_runs": 4,
  "tasks": {
    "release_management_workflow": {
      "task_name": "harmony/release_management_workflow",
      "category": "harmony",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Invalid JSON input for tool get_file_contents: {\"owner\":\"mcpmark-eval\",\"path\":\"src/encoding.rs\",\"repo\":\"harmony\"}{\"owner\":\"mcpmark-eval\",\"path\":\"src/registry.rs\",\"repo\":\"harmony\"}{\"owner\":\"mcpmark-eval\",\"path\":\"demo/harmony-demo/src/lib/utils.ts\",\"repo\":\"harmony\"}{\"owner\":\"mcpmark-eval\",\"path\":\".gitignore\",\"repo\":\"harmony\"}{\"owner\":\"mcpmark-eval\",\"path\":\"Cargo.toml\",\"repo\":\"harmony\"}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 8191,
                "output_tokens": 1838,
                "total_tokens": 10029
              },
              "turn_count": 1
            },
            "run-2": {
              "success": false,
              "error_message": "Invalid JSON input for tool get_file_contents: {\"owner\":\"mcpmark-eval\",\"path\":\"src/encoding.rs\",\"ref\":\"release-v1.1.0\",\"repo\":\"harmony\"}{\"owner\":\"mcpmark-eval\",\"path\":\"src/registry.rs\",\"ref\":\"release-v1.1.0\",\"repo\":\"harmony\"}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 8191,
                "output_tokens": 1079,
                "total_tokens": 9270
              },
              "turn_count": 1
            },
            "run-3": {
              "success": false,
              "error_message": "Invalid JSON input for tool get_file_contents: {\"owner\":\"mcpmark-eval\",\"path\":\"src/encoding.rs\",\"ref\":\"release-v1.1.0\",\"repo\":\"harmony\"}{\"owner\":\"mcpmark-eval\",\"path\":\"src/registry.rs\",\"ref\":\"release-v1.1.0\",\"repo\":\"harmony\"}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4773,
                "output_tokens": 1621,
                "total_tokens": 6394
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": "Invalid JSON input for tool get_file_contents: {\"owner\":\"mcpmark-eval\",\"path\":\"src/encoding.rs\",\"repo\":\"harmony\"}{\"owner\":\"mcpmark-eval\",\"path\":\"src/registry.rs\",\"repo\":\"harmony\"}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4773,
                "output_tokens": 1642,
                "total_tokens": 6415
              },
              "turn_count": 1
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_or_update_file: {\"owner\": \"mcpmark-eval\", \"repo\": \"harmony\", \"branch\": \"release-v1.1.0\", \"path\": \"src/encoding.rs\", \"message\": \"Fix MetaSep token mapping bug - change from channel to meta_sep\"",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 565378,
                "output_tokens": 1229,
                "total_tokens": 566607
              },
              "turn_count": 9
            },
            "run-2": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_or_update_file: {\"owner\": \"mcpmark-eval\", \"repo\": \"harmony\", \"path\": \"src/encoding.rs\", \"branch\": \"release-v1.1.0\", \"message\": \"fix: correct MetaSep token mapping to use <|meta_sep|>\\n\\nPreviously MetaSep was incorrectly mapped to <|channel|> token.\\nThis fix ensures MetaSep formatting token maps to the correct <|meta_sep|> value.\"",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 333759,
                "output_tokens": 917,
                "total_tokens": 334676
              },
              "turn_count": 6
            },
            "run-3": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_or_update_file: {\"owner\": \"mcpmark-eval\", \"repo\": \"harmony\", \"branch\": \"release-v1.1.0\", \"path\": \"src/encoding.rs\", \"message\": \"Fix MetaSep token mapping from channel to meta_sep\"",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 359595,
                "output_tokens": 881,
                "total_tokens": 360476
              },
              "turn_count": 7
            },
            "run-4": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_or_update_file: {\"owner\": \"mcpmark-eval\", \"repo\": \"harmony\", \"path\": \"src/encoding.rs\", \"branch\": \"release-v1.1.0\", \"sha\": \"afe1fce7b01641d088e4e4fbf9ae442616f2f91b\", \"message\": \"fix: correct MetaSep token mapping to use <|meta_sep|> instead of <|channel|>\"",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 450646,
                "output_tokens": 1011,
                "total_tokens": 451657
              },
              "turn_count": 8
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 544062,
                "output_tokens": 23596,
                "total_tokens": 567658
              },
              "turn_count": 10
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1069380,
                "output_tokens": 28676,
                "total_tokens": 1098056
              },
              "turn_count": 17
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1183025,
                "output_tokens": 45468,
                "total_tokens": 1228493
              },
              "turn_count": 17
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 694583,
                "output_tokens": 26446,
                "total_tokens": 721029
              },
              "turn_count": 13
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 246977,
                "output_tokens": 403,
                "total_tokens": 247380
              },
              "turn_count": 6
            },
            "run-2": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 98996,
                "output_tokens": 327,
                "total_tokens": 99323
              },
              "turn_count": 3
            },
            "run-3": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 92332,
                "output_tokens": 245,
                "total_tokens": 92577
              },
              "turn_count": 3
            },
            "run-4": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 99179,
                "output_tokens": 245,
                "total_tokens": 99424
              },
              "turn_count": 3
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 251565,
                "output_tokens": 10644,
                "total_tokens": 262209
              },
              "turn_count": 6
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 169917,
                "output_tokens": 331,
                "total_tokens": 170248
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 662123,
                "output_tokens": 18270,
                "total_tokens": 680393
              },
              "turn_count": 12
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 783795,
                "output_tokens": 11076,
                "total_tokens": 794871
              },
              "turn_count": 14
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 555287,
                "output_tokens": 486,
                "total_tokens": 555773
              },
              "turn_count": 10
            },
            "run-2": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 685596,
                "output_tokens": 570,
                "total_tokens": 686166
              },
              "turn_count": 12
            },
            "run-3": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 584175,
                "output_tokens": 501,
                "total_tokens": 584676
              },
              "turn_count": 10
            },
            "run-4": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 124865,
                "output_tokens": 316,
                "total_tokens": 125181
              },
              "turn_count": 3
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 460689,
                "output_tokens": 7302,
                "total_tokens": 467991
              },
              "turn_count": 14
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 422964,
                "output_tokens": 8860,
                "total_tokens": 431824
              },
              "turn_count": 14
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 222786,
                "output_tokens": 4238,
                "total_tokens": 227024
              },
              "turn_count": 9
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 399513,
                "output_tokens": 6016,
                "total_tokens": 405529
              },
              "turn_count": 13
            }
          }
        }
      }
    },
    "qwen3_issue_management": {
      "task_name": "easyr1/qwen3_issue_management",
      "category": "easyr1",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 12500,
                "output_tokens": 5072,
                "total_tokens": 17572
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 17304,
                "output_tokens": 7022,
                "total_tokens": 24326
              },
              "turn_count": 2
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 12444,
                "output_tokens": 6584,
                "total_tokens": 19028
              },
              "turn_count": 2
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4381,
                "output_tokens": 1898,
                "total_tokens": 6279
              },
              "turn_count": 1
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 118838,
                "output_tokens": 1302,
                "total_tokens": 120140
              },
              "turn_count": 6
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 235243,
                "output_tokens": 1827,
                "total_tokens": 237070
              },
              "turn_count": 10
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 235285,
                "output_tokens": 1938,
                "total_tokens": 237223
              },
              "turn_count": 10
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 177263,
                "output_tokens": 1644,
                "total_tokens": 178907
              },
              "turn_count": 8
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 55595,
                "output_tokens": 3244,
                "total_tokens": 58839
              },
              "turn_count": 5
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 55459,
                "output_tokens": 2945,
                "total_tokens": 58404
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 55598,
                "output_tokens": 4354,
                "total_tokens": 59952
              },
              "turn_count": 5
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 70221,
                "output_tokens": 4414,
                "total_tokens": 74635
              },
              "turn_count": 6
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 116515,
                "output_tokens": 807,
                "total_tokens": 117322
              },
              "turn_count": 7
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 132991,
                "output_tokens": 883,
                "total_tokens": 133874
              },
              "turn_count": 7
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 113148,
                "output_tokens": 808,
                "total_tokens": 113956
              },
              "turn_count": 6
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 131070,
                "output_tokens": 776,
                "total_tokens": 131846
              },
              "turn_count": 7
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 137634,
                "output_tokens": 789,
                "total_tokens": 138423
              },
              "turn_count": 6
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 111542,
                "output_tokens": 734,
                "total_tokens": 112276
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 208258,
                "output_tokens": 1014,
                "total_tokens": 209272
              },
              "turn_count": 9
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 87902,
                "output_tokens": 558,
                "total_tokens": 88460
              },
              "turn_count": 4
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 132458,
                "output_tokens": 783,
                "total_tokens": 133241
              },
              "turn_count": 7
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 149814,
                "output_tokens": 871,
                "total_tokens": 150685
              },
              "turn_count": 7
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 135760,
                "output_tokens": 831,
                "total_tokens": 136591
              },
              "turn_count": 7
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 172816,
                "output_tokens": 1141,
                "total_tokens": 173957
              },
              "turn_count": 8
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 88626,
                "output_tokens": 1116,
                "total_tokens": 89742
              },
              "turn_count": 7
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 88835,
                "output_tokens": 1809,
                "total_tokens": 90644
              },
              "turn_count": 7
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 126330,
                "output_tokens": 2271,
                "total_tokens": 128601
              },
              "turn_count": 9
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 41920,
                "output_tokens": 985,
                "total_tokens": 42905
              },
              "turn_count": 4
            }
          }
        }
      }
    },
    "label_color_standardization": {
      "task_name": "claude-code/label_color_standardization",
      "category": "claude-code",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_branch: {\"body\":\"## Problem\\nThe repository lacks clear documentation for a consistent label organization system. This makes it difficult for contributors to choose the right labels and for maintainers to triage issues effectively.\\n\\n## Proposed Solution\\nCreate a comprehensive label guide in `docs/LABEL_COLORS.md` that documents all available labels, their categories, and usage guidelines. This guide will serve as a single source of truth for label management.\\n\\n## Benefits\\n- Improved visual organization of issues and pull requests.\\n- Easier and faster issue triage for maintainers.\\n- Clearer guidelines for contributors on how to label their contributions.\\n\\nKeywords: label documentation, visual organization, label guide, organization\",\"labels\":[\"enhancement\",\"documentation\"],\"owner\":\"mcpmark-eval\",\"repo\":\"claude-code\",\"title\":\"Document label organization for better visual organization and create a label guide\"}{\"branch\":\"feat/label-color-guide\",\"from_branch\":\"main\",\"owner\":\"mcpmark-eval\",\"repo\":\"claude-code\"}",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": 0
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 0,
                "output_tokens": 0,
                "total_tokens": 0
              },
              "turn_count": 1
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 96928,
                "output_tokens": 5913,
                "total_tokens": 102841
              },
              "turn_count": 7
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 96638,
                "output_tokens": 6185,
                "total_tokens": 102823
              },
              "turn_count": 7
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1149321,
                "output_tokens": 4229,
                "total_tokens": 1153550
              },
              "turn_count": 11
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1446182,
                "output_tokens": 4858,
                "total_tokens": 1451040
              },
              "turn_count": 12
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 706686,
                "output_tokens": 4892,
                "total_tokens": 711578
              },
              "turn_count": 13
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 946927,
                "output_tokens": 4557,
                "total_tokens": 951484
              },
              "turn_count": 13
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 555486,
                "output_tokens": 16129,
                "total_tokens": 571615
              },
              "turn_count": 12
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 532668,
                "output_tokens": 14746,
                "total_tokens": 547414
              },
              "turn_count": 11
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 718485,
                "output_tokens": 11618,
                "total_tokens": 730103
              },
              "turn_count": 10
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 586665,
                "output_tokens": 14159,
                "total_tokens": 600824
              },
              "turn_count": 14
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 744059,
                "output_tokens": 3360,
                "total_tokens": 747419
              },
              "turn_count": 12
            },
            "run-2": {
              "success": false,
              "error_message": "Github Authentication failed",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 160756,
                "output_tokens": 679,
                "total_tokens": 161435
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": "Github Authentication failed",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 143408,
                "output_tokens": 486,
                "total_tokens": 143894
              },
              "turn_count": 4
            },
            "run-4": {
              "success": false,
              "error_message": "Github Authentication failed",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 254682,
                "output_tokens": 761,
                "total_tokens": 255443
              },
              "turn_count": 6
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 364970,
                "output_tokens": 516,
                "total_tokens": 365486
              },
              "turn_count": 9
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2428536,
                "output_tokens": 2653,
                "total_tokens": 2431189
              },
              "turn_count": 22
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 856452,
                "output_tokens": 2566,
                "total_tokens": 859018
              },
              "turn_count": 12
            },
            "run-4": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 88525,
                "output_tokens": 203,
                "total_tokens": 88728
              },
              "turn_count": 3
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 158065 tokens (158065 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082018330957938513818738086)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 151503,
                "output_tokens": 277,
                "total_tokens": 151780
              },
              "turn_count": 5
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 880728,
                "output_tokens": 3332,
                "total_tokens": 884060
              },
              "turn_count": 14
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 600859,
                "output_tokens": 2959,
                "total_tokens": 603818
              },
              "turn_count": 10
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Internal Server Error (tid: 2025082111481343018823177642968)', 'type': 'internal_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 639949,
                "output_tokens": 2167,
                "total_tokens": 642116
              },
              "turn_count": 8
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 140437,
                "output_tokens": 2877,
                "total_tokens": 143314
              },
              "turn_count": 10
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 550161,
                "output_tokens": 4769,
                "total_tokens": 554930
              },
              "turn_count": 11
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 101529,
                "output_tokens": 2696,
                "total_tokens": 104225
              },
              "turn_count": 8
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 91611,
                "output_tokens": 2836,
                "total_tokens": 94447
              },
              "turn_count": 7
            }
          }
        }
      }
    },
    "find_salient_file": {
      "task_name": "missing-semester/find_salient_file",
      "category": "missing-semester",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 11738,
                "output_tokens": 2801,
                "total_tokens": 14539
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 15196,
                "output_tokens": 2274,
                "total_tokens": 17470
              },
              "turn_count": 2
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4093,
                "output_tokens": 0,
                "total_tokens": 4093
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 11736,
                "output_tokens": 3304,
                "total_tokens": 15040
              },
              "turn_count": 2
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 200988 tokens > 200000 maximum (tid: 2025082010270850923237544191991)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1693482,
                "output_tokens": 1824,
                "total_tokens": 1695306
              },
              "turn_count": 10
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 202111 tokens > 200000 maximum (tid: 2025082014333354393323322708172)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1701480,
                "output_tokens": 1648,
                "total_tokens": 1703128
              },
              "turn_count": 10
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 202741 tokens > 200000 maximum (tid: 2025082101331176243756355780353)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1714994,
                "output_tokens": 1513,
                "total_tokens": 1716507
              },
              "turn_count": 10
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 200759 tokens > 200000 maximum (tid: 2025082111154415085495739621755)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2100167,
                "output_tokens": 2378,
                "total_tokens": 2102545
              },
              "turn_count": 12
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 270732,
                "output_tokens": 7178,
                "total_tokens": 277910
              },
              "turn_count": 4
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 8824,
                "output_tokens": 1960,
                "total_tokens": 10784
              },
              "turn_count": 2
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 8824,
                "output_tokens": 2600,
                "total_tokens": 11424
              },
              "turn_count": 2
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 277063 tokens. Please reduce the length of the messages. (tid: 2025082109531925225566262317727)', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 905027,
                "output_tokens": 9021,
                "total_tokens": 914048
              },
              "turn_count": 5
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082012385352329195632915968)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 144760,
                "output_tokens": 184,
                "total_tokens": 144944
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082019211831958921803853219)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 141209,
                "output_tokens": 173,
                "total_tokens": 141382
              },
              "turn_count": 2
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082200295719195643575833427)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 141309,
                "output_tokens": 173,
                "total_tokens": 141482
              },
              "turn_count": 2
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082206070934384912448794622)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 141208,
                "output_tokens": 175,
                "total_tokens": 141383
              },
              "turn_count": 2
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2250276,
                "output_tokens": 1193,
                "total_tokens": 2251469
              },
              "turn_count": 11
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2612154,
                "output_tokens": 1633,
                "total_tokens": 2613787
              },
              "turn_count": 19
            },
            "run-3": {
              "success": false,
              "error_message": "Invalid JSON input for tool get_file_contents: {\"owner\": \"mcpmark-eval\", \"repo\": \"missing-semester\", \"path\": \"}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 19498,
                "output_tokens": 61,
                "total_tokens": 19559
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2569926,
                "output_tokens": 1488,
                "total_tokens": 2571414
              },
              "turn_count": 13
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 158580 tokens (158580 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082011192149317994418873636)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 69809,
                "output_tokens": 221,
                "total_tokens": 70030
              },
              "turn_count": 4
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 158577 tokens (158577 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082015405140704051868304746)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 69791,
                "output_tokens": 222,
                "total_tokens": 70013
              },
              "turn_count": 4
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 152057 tokens (152057 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082103085826642658731901460)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 14193,
                "output_tokens": 68,
                "total_tokens": 14261
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 152160 tokens (152160 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082116304641903088158202486)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 14193,
                "output_tokens": 71,
                "total_tokens": 14264
              },
              "turn_count": 1
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 401784,
                "output_tokens": 900,
                "total_tokens": 402684
              },
              "turn_count": 4
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 401018,
                "output_tokens": 936,
                "total_tokens": 401954
              },
              "turn_count": 4
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 269865,
                "output_tokens": 859,
                "total_tokens": 270724
              },
              "turn_count": 3
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 280806,
                "output_tokens": 1163,
                "total_tokens": 281969
              },
              "turn_count": 4
            }
          }
        }
      }
    },
    "pr_automation_workflow": {
      "task_name": "mcpmark-cicd/pr_automation_workflow",
      "category": "mcpmark-cicd",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 61435,
                "output_tokens": 10626,
                "total_tokens": 72061
              },
              "turn_count": 5
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 64006,
                "output_tokens": 3327,
                "total_tokens": 67333
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 304520,
                "output_tokens": 11600,
                "total_tokens": 316120
              },
              "turn_count": 17
            },
            "run-4": {
              "success": false,
              "error_message": "Command '['/home/zijian/miniconda3/envs/benchmark/bin/python', '/home/zijian/mcpmark/tasks/github/mcpmark-cicd/pr_automation_workflow/verify.py']' timed out after 299.9999879200004 seconds",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 61844,
                "output_tokens": 9007,
                "total_tokens": 70851
              },
              "turn_count": 5
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 71754,
                "output_tokens": 513,
                "total_tokens": 72267
              },
              "turn_count": 4
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 145649,
                "output_tokens": 5742,
                "total_tokens": 151391
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 147724,
                "output_tokens": 6263,
                "total_tokens": 153987
              },
              "turn_count": 7
            },
            "run-4": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_or_update_file: {\"owner\": \"mcpmark-eval\", \"repo\": \"mcpmark-cicd\", \"path\": \".github/workflows/pr-automation.yml\", \"branch\": \"pr-automation-workflow\", \"message\": \"Add comprehensive PR automation workflow with parallel jobs\"",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 71749,
                "output_tokens": 508,
                "total_tokens": 72257
              },
              "turn_count": 4
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 69639,
                "output_tokens": 13403,
                "total_tokens": 83042
              },
              "turn_count": 5
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 93934,
                "output_tokens": 19519,
                "total_tokens": 113453
              },
              "turn_count": 6
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 73745,
                "output_tokens": 14289,
                "total_tokens": 88034
              },
              "turn_count": 5
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 88214,
                "output_tokens": 15956,
                "total_tokens": 104170
              },
              "turn_count": 6
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_or_update_file: {\"owner\": \"mcpmark-eval\", \"repo\": \"mcpmark-cicd\", \"path\": \".github/workflows/pr-automation.yml\", \"content\": \"name: Pull Request Automation\\n\\non:\\n  pull_request:\\n    types: [opened, synchronize, reopened]\\n    branches: [main, develop]\\n\\nenv:\\n  NODE_VERSION: '18'\\n\\njobs:\\n  code-quality:\\n    name: code-quality\\n    runs-on: ubuntu-latest\\n    permissions:\\n      contents: read\\n      pull-requests: write\\n    \\n    steps:\\n      - name: Checkout code\\n        uses: actions/checkout@v4\\n        with:\\n          fetch-depth: 0\\n\\n      - name: Setup Node.js\\n        uses: actions/setup-node@v4\\n        with:\\n          node-version: ${{ env.NODE_VERSION }}\\n          cache: 'npm'\\n\\n      - name: Install dependencies\\n        run: npm ci\\n\\n      - name: Run ESLint\\n        id: eslint\\n        run: |\\n          echo \\\"## ESLint Results\\\" > eslint-report.txt\\n          npm run lint 2>&1 | tee -a eslint-report.txt || echo \\\"ESLint completed with issues\\\"\\n          echo \\\"ESLINT_EXIT_CODE=${PIPESTATUS[0]}\\\" >> $GITHUB_OUTPUT\\n\\n      - name: Run Prettier check\\n        id: prettier\\n        run: |\\n          echo \\\"## Prettier Results\\\" > prettier-report.txt\\n          npm run format:check 2>&1 | tee -a prettier-report.txt || echo \\\"Prettier completed with issues\\\"\\n          echo \\\"PRETTIER_EXIT_CODE=${PIPESTATUS[0]}\\\" >> $GITHUB_OUTPUT\\n\\n      - name: Post code quality results\\n        uses: actions/github-script@v7\\n        if: always()\\n        with:\\n          script: |\\n            const fs = require('fs');\\n            \\n            let eslintContent = '';\\n            let prettierContent = '';\\n            \\n            try {\\n              eslintContent = fs.readFileSync('eslint-report.txt', 'utf8');\\n            } catch (e) {\\n              eslintContent = 'ESLint report not available';\\n            }\\n            \\n            try {\\n              prettierContent = fs.readFileSync('prettier-report.txt', 'utf8');\\n            } catch (e) {\\n              prettierContent = 'Prettier report not available';\\n            }\\n            \\n            const body = `## Code Quality Report\\n            \\n            ### ESLint Status: ${{ steps.eslint.outputs.ESLINT_EXIT_CODE == '0' && ' PASSED' || ' FAILED' }}\\n            \\\\`\\\\`\\\\`\\n            ${eslintContent}\\n            \\\\`\\\\`\\\\`\\n            \\n            ### Prettier Status: ${{ steps.prettier.outputs.PRETTIER_EXIT_CODE == '0' && ' PASSED' || ' FAILED' }}\\n            \\\\`\\\\`\\\\`\\n            ${prettierContent}\\n            \\\\`\\\\`\\\\`\\n            \\n            *Generated by PR Automation Workflow*`;\\n            \\n            github.rest.issues.createComment({\\n              issue_number: context.issue.number,\\n              owner: context.repo.owner,\\n              repo: context.repo.repo,\\n              body: body\\n            });\\n\\n  testing-suite:\\n    name: testing-suite\\n    runs-on: ubuntu-latest\\n    permissions:\\n      contents: read\\n      pull-requests: write\\n    \\n    steps:\\n      - name: Checkout code\\n        uses: actions/checkout@v4\\n\\n      - name: Setup Node.js\\n        uses: actions/setup-node@v4\\n        with:\\n          node-version: ${{ env.NODE_VERSION }}\\n          cache: 'npm'\\n\\n      - name: Install dependencies\\n        run: npm ci\\n\\n      - name: Run tests with coverage\\n        id: test\\n        run: |\\n          npm test -- --coverage --coverageReporters=text --coverageReporters=lcov > test-output.txt 2>&1 || echo \\\"Tests completed\\\"\\n          echo \\\"TEST_EXIT_CODE=${PIPESTATUS[0]}\\\" >> $GITHUB_OUTPUT\\n\\n      - name: Generate coverage summary\\n        id: coverage\\n        run: |\\n          if [ -f coverage/lcov.info ]; then\\n            COVERAGE_PCT=$(grep -o 'Lines.*[0-9]*\\\\.[0-9]*%' coverage/lcov.info | head -1 | grep -o '[0-9]*\\\\.[0-9]*%')\\n            echo \\\"COVERAGE_PCT=$COVERAGE_PCT\\\" >> $GITHUB_OUTPUT\\n",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 12167,
                "output_tokens": 76,
                "total_tokens": 12243
              },
              "turn_count": 1
            },
            "run-2": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 15607,
                "output_tokens": 76,
                "total_tokens": 15683
              },
              "turn_count": 1
            },
            "run-3": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_or_update_file: {\"owner\": \"mcpmark-eval\", \"repo\": \"mcpmark-cicd\", \"path\": \".github/workflows/pr-automation.yml\", \"content\": \"name: Pull Request Automation\\n\\non:\\n  pull_request:\\n    types: [opened, synchronize, reopened]\\n    branches: [main, develop]\\n\\nenv:\\n  NODE_VERSION: '18'\\n\\njobs:\\n  code-quality:\\n    name: code-quality\\n    runs-on: ubuntu-latest\\n    permissions:\\n      contents: read\\n      pull-requests: write\\n    \\n    steps:\\n      - name: Checkout code\\n        uses: actions/checkout@v4\\n        with:\\n          fetch-depth: 0\\n\\n      - name: Setup Node.js\\n        uses: actions/setup-node@v4\\n        with:\\n          node-version: ${{ env.NODE_VERSION }}\\n          cache: 'npm'\\n\\n      - name: Install dependencies\\n        run: npm ci\\n\\n      - name: Run ESLint\\n        id: eslint\\n        run: |\\n          echo \\\"## ESLint Results\\\" > eslint-report.txt\\n          npm run lint 2>&1 | tee -a eslint-report.txt || echo \\\"ESLint completed with issues\\\"\\n          echo \\\"ESLINT_EXIT_CODE=${PIPESTATUS[0]}\\\" >> $GITHUB_OUTPUT\\n\\n      - name: Run Prettier check\\n        id: prettier\\n        run: |\\n          echo \\\"## Prettier Results\\\" > prettier-report.txt\\n          npm run format:check 2>&1 | tee -a prettier-report.txt || echo \\\"Prettier completed with issues\\\"\\n          echo \\\"PRETTIER_EXIT_CODE=${PIPESTATUS[0]}\\\" >> $GITHUB_OUTPUT\\n\\n      - name: Post code quality results\\n        uses: actions/github-script@v7\\n        if: always()\\n        with:\\n          script: |\\n            const fs = require('fs');\\n            \\n            let eslintContent = '';\\n            let prettierContent = '';\\n            \\n            try {\\n              eslintContent = fs.readFileSync('eslint-report.txt', 'utf8');\\n            } catch (e) {\\n              eslintContent = 'ESLint report not available';\\n            }\\n            \\n            try {\\n              prettierContent = fs.readFileSync('prettier-report.txt', 'utf8');\\n            } catch (e) {\\n              prettierContent = 'Prettier report not available';\\n            }\\n            \\n            const body = `## Code Quality Report\\n            \\n            ### ESLint\\n            \\\\`\\\\`\\\\`\\n            ${eslintContent}\\n            \\\\`\\\\`\\\\`\\n            \\n            ### Prettier\\n            \\\\`\\\\`\\\\`\\n            ${prettierContent}\\n            \\\\`\\\\`\\\\`\\n            \\n            **Status**: ${context.job.status === 'success' ? ' Passed' : ' Failed'}`;\\n            \\n            github.rest.issues.createComment({\\n              issue_number: context.issue.number,\\n              owner: context.repo.owner,\\n              repo: context.repo.repo,\\n              body: body\\n            });\\n\\n  testing-suite:\\n    name: testing-suite\\n    runs-on: ubuntu-latest\\n    permissions:\\n      contents: read\\n      pull-requests: write\\n    \\n    steps:\\n      - name: Checkout code\\n        uses: actions/checkout@v4\\n\\n      - name: Setup Node.js\\n        uses: actions/setup-node@v4\\n        with:\\n          node-version: ${{ env.NODE_VERSION }}\\n          cache: 'npm'\\n\\n      - name: Install dependencies\\n        run: npm ci\\n\\n      - name: Run tests with coverage\\n        id: test\\n        run: |\\n          echo \\\"## Test Coverage Report\\\" > coverage-report.txt\\n          npm test -- --coverage 2>&1 | tee -a coverage-report.txt || echo \\\"Tests completed with issues\\\"\\n          echo \\\"TEST_EXIT_CODE=${PIPESTATUS[0]}\\\" >> $GITHUB_OUTPUT\\n\\n      - name: Generate coverage summary\\n        run: |\\n          if [ -f coverage/coverage-summary.json ]; then\\n            echo \\\"### Coverage Summary\\\" >> coverage-summary.txt\\n            cat coverage/coverage-summary.json | jq -r '.total | \\\"Lines: \\\\(.lines.pct)%, Statements: \\\\(.statements.pct)%, Functions: \\\\(.functions.pct)%, Branches: \\\\(.branches.pct)%\\\"' >> coverage-summary.txt\\n          else\\n            echo \\\"Coverage report not generated\\\" >> coverage-summary.txt\\n          fi\\n\\n      - name: Upload coverage artifacts\\n        uses: actions/upload-artifact@v4\\n        if",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 12167,
                "output_tokens": 76,
                "total_tokens": 12243
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 103565,
                "output_tokens": 4634,
                "total_tokens": 108199
              },
              "turn_count": 5
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 122494,
                "output_tokens": 3353,
                "total_tokens": 125847
              },
              "turn_count": 5
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 121800,
                "output_tokens": 3086,
                "total_tokens": 124886
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 120164,
                "output_tokens": 2971,
                "total_tokens": 123135
              },
              "turn_count": 5
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 227542,
                "output_tokens": 3382,
                "total_tokens": 230924
              },
              "turn_count": 9
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 100655,
                "output_tokens": 4593,
                "total_tokens": 105248
              },
              "turn_count": 5
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 200789,
                "output_tokens": 4891,
                "total_tokens": 205680
              },
              "turn_count": 10
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 204666,
                "output_tokens": 5038,
                "total_tokens": 209704
              },
              "turn_count": 10
            },
            "run-4": {
              "success": false,
              "error_message": "Command '['/home/zijian/miniconda3/envs/benchmark/bin/python', '/home/zijian/mcpmark/tasks/github/mcpmark-cicd/pr_automation_workflow/verify.py']' timed out after 299.9999883799974 seconds",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 97432,
                "output_tokens": 3610,
                "total_tokens": 101042
              },
              "turn_count": 5
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 65126,
                "output_tokens": 4116,
                "total_tokens": 69242
              },
              "turn_count": 5
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 65670,
                "output_tokens": 5140,
                "total_tokens": 70810
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 67263,
                "output_tokens": 3476,
                "total_tokens": 70739
              },
              "turn_count": 5
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 82221,
                "output_tokens": 5825,
                "total_tokens": 88046
              },
              "turn_count": 6
            }
          }
        }
      }
    },
    "assign_contributor_labels": {
      "task_name": "missing-semester/assign_contributor_labels",
      "category": "missing-semester",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4186,
                "output_tokens": 3925,
                "total_tokens": 8111
              },
              "turn_count": 1
            },
            "run-2": {
              "success": false,
              "error_message": "Invalid JSON input for tool search_issues: {\"q\":\"repo:mcpmark-eval/missing-semester is:pr is:merged sort:updated-desc\"}{\"q\":\"repo:mcpmark-eval/missing-semester is:open\"}",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": 0
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4186,
                "output_tokens": 3794,
                "total_tokens": 7980
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 12031,
                "output_tokens": 4110,
                "total_tokens": 16141
              },
              "turn_count": 2
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 201730 tokens > 200000 maximum (tid: 2025082010165982105402229676165)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 367304,
                "output_tokens": 364,
                "total_tokens": 367668
              },
              "turn_count": 3
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 202147 tokens > 200000 maximum (tid: 2025082014261457861052300248246)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 367403,
                "output_tokens": 866,
                "total_tokens": 368269
              },
              "turn_count": 3
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 201752 tokens > 200000 maximum (tid: 2025082101195113059056469823943)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 367092,
                "output_tokens": 354,
                "total_tokens": 367446
              },
              "turn_count": 3
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 201920 tokens > 200000 maximum (tid: 2025082111090063724414762951399)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 367325,
                "output_tokens": 522,
                "total_tokens": 367847
              },
              "turn_count": 3
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1330982,
                "output_tokens": 22418,
                "total_tokens": 1353400
              },
              "turn_count": 9
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 910515,
                "output_tokens": 11572,
                "total_tokens": 922087
              },
              "turn_count": 7
            },
            "run-3": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 316198,
                "output_tokens": 5517,
                "total_tokens": 321715
              },
              "turn_count": 3
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1084483,
                "output_tokens": 16152,
                "total_tokens": 1100635
              },
              "turn_count": 8
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082012323162768624421098353)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 271855,
                "output_tokens": 150,
                "total_tokens": 272005
              },
              "turn_count": 3
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082017412773717415323119598)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 275518,
                "output_tokens": 166,
                "total_tokens": 275684
              },
              "turn_count": 3
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082200234670672940302674094)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 271853,
                "output_tokens": 151,
                "total_tokens": 272004
              },
              "turn_count": 3
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082205581056143409407636080)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 275290,
                "output_tokens": 150,
                "total_tokens": 275440
              },
              "turn_count": 3
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2836448,
                "output_tokens": 2285,
                "total_tokens": 2838733
              },
              "turn_count": 16
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2742557,
                "output_tokens": 3714,
                "total_tokens": 2746271
              },
              "turn_count": 16
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 3225390,
                "output_tokens": 1593,
                "total_tokens": 3226983
              },
              "turn_count": 19
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2710370,
                "output_tokens": 2046,
                "total_tokens": 2712416
              },
              "turn_count": 16
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 152210 tokens (152210 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082011150165103233084231781)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 14284,
                "output_tokens": 130,
                "total_tokens": 14414
              },
              "turn_count": 1
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 152308 tokens (152308 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082015352926145690402270125)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 14284,
                "output_tokens": 128,
                "total_tokens": 14412
              },
              "turn_count": 1
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 152110 tokens (152110 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082103055184148903277213149)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 14284,
                "output_tokens": 130,
                "total_tokens": 14414
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 152149 tokens (152149 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082116272746574893558813468)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 14284,
                "output_tokens": 69,
                "total_tokens": 14353
              },
              "turn_count": 1
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1490960,
                "output_tokens": 1553,
                "total_tokens": 1492513
              },
              "turn_count": 11
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1493646,
                "output_tokens": 1583,
                "total_tokens": 1495229
              },
              "turn_count": 11
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 139522,
                "output_tokens": 1191,
                "total_tokens": 140713
              },
              "turn_count": 2
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 723234,
                "output_tokens": 1904,
                "total_tokens": 725138
              },
              "turn_count": 6
            }
          }
        }
      }
    },
    "issue_tagging_pr_closure": {
      "task_name": "harmony/issue_tagging_pr_closure",
      "category": "harmony",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 14693,
                "output_tokens": 2366,
                "total_tokens": 17059
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 14813,
                "output_tokens": 1848,
                "total_tokens": 16661
              },
              "turn_count": 2
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4952,
                "output_tokens": 0,
                "total_tokens": 4952
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": "Invalid JSON input for tool add_issue_comment: {\"body\":\"After a thorough investigation, it's clear that this migration is not feasible due to several technical barriers. The harmony core's dependency on a specific `require` pattern for its tokenization logic means that a pure ESM environment is **breaking compatibility**. Our analysis shows that the **core tokenization** process is tightly coupled with a dynamic `require` that cannot be replicated with static ESM imports.\\n\\nFurthermore, we've encountered significant **build system constraints**. The webpack configuration required for ESM bundling conflicts with the assumptions made by the harmony core's build process. The core library is fundamentally **CommonJS required**, and any attempt to shim or polyfill this behavior introduces unacceptable complexity and performance overhead.\\n\\nGiven these findings, this **approach is not viable** without a major refactoring of the harmony core itself. The existing CommonJS dependencies are too deeply integrated to be easily replaced. We recommend pausing this effort until the core library is updated to support modern module systems.\",\"issue_number\":28,\"owner\":\"mcpmark-eval\",\"repo\":\"harmony\"}{\"body\":\"The attempt to migrate the JavaScript demo to ESM has been blocked by significant **technical constraints**. Our investigation, detailed in PR #28, revealed that the harmony core has a hard **CommonJS dependency** that is incompatible with an ESM-only environment.\\n\\nThe primary issue stems from **harmony core limitations**, where the tokenization logic relies on a dynamic `require` pattern that cannot be replicated with static ESM imports. This creates a fundamental conflict in **build system compatibility** between the modern ESM approach and the legacy CommonJS requirements of the core library.\\n\\nUnfortunately, this means the proposed ESM migration is **not viable at this time**. We will be closing the associated pull request and this issue until the core library is updated to support modern module systems.\",\"issue_number\":27,\"owner\":\"mcpmark-eval\",\"repo\":\"harmony\"}{\"owner\":\"mcpmark-eval\",\"pullNumber\":28,\"repo\":\"harmony\",\"state\":\"closed\"}{\"issue_number\":28,\"labels\":[\"enhancement\",\"needs-investigation\",\"wontfix\"],\"owner\":\"mcpmark-eval\",\"repo\":\"harmony\"}{\"body\":\"Due to the **architectural limitations** discovered during the implementation, this pull request **cannot be merged**. The core issue is that a significant **core refactoring required** to make the harmony library compatible with ESM modules.\\n\\nThis migration will be revisited for **future consideration** once the underlying dependencies are updated. For now, we are closing this PR and marking it as 'wontfix'. All findings have been documented in issue #27.\",\"issue_number\":28,\"owner\":\"mcpmark-eval\",\"repo\":\"harmony\"}{\"issue_number\":27,\"owner\":\"mcpmark-eval\",\"repo\":\"harmony\",\"state\":\"closed\"}{\"body\":\"After a thorough investigation, we are **closing as not planned** due to insurmountable **architectural constraints**. The **future implementation blocked** until the harmony core library is redesigned to support modern module systems. This **requires core redesign** to address the hard dependency on CommonJS. All findings are documented in PR #28.\",\"issue_number\":27,\"owner\":\"mcpmark-eval\",\"repo\":\"harmony\"}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 35663,
                "output_tokens": 3774,
                "total_tokens": 39437
              },
              "turn_count": 4
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 475887,
                "output_tokens": 4301,
                "total_tokens": 480188
              },
              "turn_count": 14
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 354936,
                "output_tokens": 4106,
                "total_tokens": 359042
              },
              "turn_count": 13
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 492474,
                "output_tokens": 4657,
                "total_tokens": 497131
              },
              "turn_count": 15
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 537480,
                "output_tokens": 4696,
                "total_tokens": 542176
              },
              "turn_count": 16
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 319499,
                "output_tokens": 11608,
                "total_tokens": 331107
              },
              "turn_count": 16
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 289965,
                "output_tokens": 15590,
                "total_tokens": 305555
              },
              "turn_count": 15
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 281442,
                "output_tokens": 12655,
                "total_tokens": 294097
              },
              "turn_count": 15
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 357447,
                "output_tokens": 16674,
                "total_tokens": 374121
              },
              "turn_count": 16
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": 0
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 290461,
                "output_tokens": 2779,
                "total_tokens": 293240
              },
              "turn_count": 12
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 315096,
                "output_tokens": 2657,
                "total_tokens": 317753
              },
              "turn_count": 13
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 295636,
                "output_tokens": 3014,
                "total_tokens": 298650
              },
              "turn_count": 12
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 393661,
                "output_tokens": 3047,
                "total_tokens": 396708
              },
              "turn_count": 13
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 819279,
                "output_tokens": 3177,
                "total_tokens": 822456
              },
              "turn_count": 20
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 626481,
                "output_tokens": 3456,
                "total_tokens": 629937
              },
              "turn_count": 20
            },
            "run-4": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_issue: {\"owner\": \"mcpmark-eval\", \"repo\": \"harmony\", \"title\": \"Upgrade JavaScript demo to use ESM imports and modern module system\", \"body\": \"## Problem\\nThe current JavaScript demo uses CommonJS require statements which are outdated and limit compatibility with modern tools and environments. This approach also creates issues with module bundling and tree-shaking optimizations.\\n\\n## Proposed Solution\\nMigrate the JavaScript demo from CommonJS to ESM imports to leverage the modern JavaScript module system. This would involve updating package.json and refactoring import statements throughout the demo code.\\n\\n## Benefits\\n- Better compatibility with modern build tools\\n- Improved module bundling through tree-shaking\\n- Alignment with current JavaScript standards\\n- Enhanced developer experience\\n\\nThis enhancement addresses issue #26 and aims to modernize our JavaScript demo.\\n\\nKeywords: CommonJS, ESM imports, module bundling, modern JavaScript\", \"labels\": enhancement}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 20311,
                "output_tokens": 239,
                "total_tokens": 20550
              },
              "turn_count": 1
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": 0
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 296557,
                "output_tokens": 2808,
                "total_tokens": 299365
              },
              "turn_count": 12
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 297355,
                "output_tokens": 2867,
                "total_tokens": 300222
              },
              "turn_count": 12
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 297304,
                "output_tokens": 2824,
                "total_tokens": 300128
              },
              "turn_count": 12
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 252081,
                "output_tokens": 2618,
                "total_tokens": 254699
              },
              "turn_count": 14
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 258294,
                "output_tokens": 3865,
                "total_tokens": 262159
              },
              "turn_count": 14
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 215964,
                "output_tokens": 3282,
                "total_tokens": 219246
              },
              "turn_count": 12
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 228586,
                "output_tokens": 2710,
                "total_tokens": 231296
              },
              "turn_count": 13
            }
          }
        }
      }
    },
    "linting_ci_workflow": {
      "task_name": "mcpmark-cicd/linting_ci_workflow",
      "category": "mcpmark-cicd",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 56590,
                "output_tokens": 4747,
                "total_tokens": 61337
              },
              "turn_count": 5
            },
            "run-2": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": 0
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 74836,
                "output_tokens": 5019,
                "total_tokens": 79855
              },
              "turn_count": 6
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 39133,
                "output_tokens": 3813,
                "total_tokens": 42946
              },
              "turn_count": 4
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 148950,
                "output_tokens": 3113,
                "total_tokens": 152063
              },
              "turn_count": 7
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 210332,
                "output_tokens": 2827,
                "total_tokens": 213159
              },
              "turn_count": 8
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 148656,
                "output_tokens": 2988,
                "total_tokens": 151644
              },
              "turn_count": 7
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 139868,
                "output_tokens": 2379,
                "total_tokens": 142247
              },
              "turn_count": 7
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 72028,
                "output_tokens": 5751,
                "total_tokens": 77779
              },
              "turn_count": 6
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 60384,
                "output_tokens": 6464,
                "total_tokens": 66848
              },
              "turn_count": 5
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 60546,
                "output_tokens": 6817,
                "total_tokens": 67363
              },
              "turn_count": 5
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 71857,
                "output_tokens": 7453,
                "total_tokens": 79310
              },
              "turn_count": 6
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 109181,
                "output_tokens": 1871,
                "total_tokens": 111052
              },
              "turn_count": 7
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 132748,
                "output_tokens": 1758,
                "total_tokens": 134506
              },
              "turn_count": 7
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 108671,
                "output_tokens": 1789,
                "total_tokens": 110460
              },
              "turn_count": 7
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 133456,
                "output_tokens": 1876,
                "total_tokens": 135332
              },
              "turn_count": 7
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 225088,
                "output_tokens": 1780,
                "total_tokens": 226868
              },
              "turn_count": 9
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 247313,
                "output_tokens": 1669,
                "total_tokens": 248982
              },
              "turn_count": 10
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 247682,
                "output_tokens": 2364,
                "total_tokens": 250046
              },
              "turn_count": 10
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 248045,
                "output_tokens": 2079,
                "total_tokens": 250124
              },
              "turn_count": 10
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 178519,
                "output_tokens": 1917,
                "total_tokens": 180436
              },
              "turn_count": 9
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 179163,
                "output_tokens": 2121,
                "total_tokens": 181284
              },
              "turn_count": 9
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 177738,
                "output_tokens": 1853,
                "total_tokens": 179591
              },
              "turn_count": 9
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 178105,
                "output_tokens": 1936,
                "total_tokens": 180041
              },
              "turn_count": 9
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 60148,
                "output_tokens": 1630,
                "total_tokens": 61778
              },
              "turn_count": 5
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 59817,
                "output_tokens": 2322,
                "total_tokens": 62139
              },
              "turn_count": 5
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 59876,
                "output_tokens": 2661,
                "total_tokens": 62537
              },
              "turn_count": 5
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 70886,
                "output_tokens": 2038,
                "total_tokens": 72924
              },
              "turn_count": 6
            }
          }
        }
      }
    },
    "find_rag_commit": {
      "task_name": "build_your_own_x/find_rag_commit",
      "category": "build_your_own_x",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 11845,
                "output_tokens": 2502,
                "total_tokens": 14347
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 52763,
                "output_tokens": 10880,
                "total_tokens": 63643
              },
              "turn_count": 3
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 11835,
                "output_tokens": 4147,
                "total_tokens": 15982
              },
              "turn_count": 2
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 11894,
                "output_tokens": 1847,
                "total_tokens": 13741
              },
              "turn_count": 2
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 215839 tokens > 200000 maximum (tid: 2025082009475222675694558504112)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 80927,
                "output_tokens": 427,
                "total_tokens": 81354
              },
              "turn_count": 3
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 215791 tokens > 200000 maximum (tid: 2025082014051065832068524126944)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 95316,
                "output_tokens": 603,
                "total_tokens": 95919
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 215810 tokens > 200000 maximum (tid: 2025082023575117237458908220869)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 95334,
                "output_tokens": 599,
                "total_tokens": 95933
              },
              "turn_count": 5
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 215663 tokens > 200000 maximum (tid: 2025082109564159329074322312247)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 97828,
                "output_tokens": 498,
                "total_tokens": 98326
              },
              "turn_count": 4
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2081944,
                "output_tokens": 7781,
                "total_tokens": 2089725
              },
              "turn_count": 15
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 342515 tokens. Please reduce the length of the messages. (tid: 2025082013271577934755599535091)', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1419710,
                "output_tokens": 7385,
                "total_tokens": 1427095
              },
              "turn_count": 10
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1370826,
                "output_tokens": 7839,
                "total_tokens": 1378665
              },
              "turn_count": 16
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 366675 tokens. Please reduce the length of the messages. (tid: 2025082106113432066620878461145)', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1418462,
                "output_tokens": 6470,
                "total_tokens": 1424932
              },
              "turn_count": 12
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082011334944210665875888911)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 101925,
                "output_tokens": 355,
                "total_tokens": 102280
              },
              "turn_count": 6
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082016141786681642072298624)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 146108,
                "output_tokens": 426,
                "total_tokens": 146534
              },
              "turn_count": 7
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082123161496796357898512183)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 132268,
                "output_tokens": 393,
                "total_tokens": 132661
              },
              "turn_count": 7
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082204355356572068168677562)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 101951,
                "output_tokens": 364,
                "total_tokens": 102315
              },
              "turn_count": 6
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 96423,
                "output_tokens": 183,
                "total_tokens": 96606
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 19538,
                "output_tokens": 95,
                "total_tokens": 19633
              },
              "turn_count": 1
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 3141248,
                "output_tokens": 1991,
                "total_tokens": 3143239
              },
              "turn_count": 24
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2921051,
                "output_tokens": 1386,
                "total_tokens": 2922437
              },
              "turn_count": 16
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 198592 tokens (198592 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082010325110172152377208812)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 269103,
                "output_tokens": 519,
                "total_tokens": 269622
              },
              "turn_count": 8
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 162314 tokens (162314 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082014380039711773812980390)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 211771,
                "output_tokens": 363,
                "total_tokens": 212134
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 284597 tokens (284597 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082101401164278384589600368)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 353910,
                "output_tokens": 477,
                "total_tokens": 354387
              },
              "turn_count": 7
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 163853 tokens (163853 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082111204895098397482619534)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 338288,
                "output_tokens": 542,
                "total_tokens": 338830
              },
              "turn_count": 9
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1637352,
                "output_tokens": 2347,
                "total_tokens": 1639699
              },
              "turn_count": 13
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 200000 tokens. However, your messages resulted in 201592 tokens (192948 in the messages, 8644 in the functions). Please reduce the length of the messages or functions. (tid: 2025082013123795344195530866861)\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 770351,
                "output_tokens": 820,
                "total_tokens": 771171
              },
              "turn_count": 8
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2851700,
                "output_tokens": 5660,
                "total_tokens": 2857360
              },
              "turn_count": 24
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 236805,
                "output_tokens": 1314,
                "total_tokens": 238119
              },
              "turn_count": 10
            }
          }
        }
      }
    },
    "multi_branch_commit_aggregation": {
      "task_name": "harmony/multi_branch_commit_aggregation",
      "category": "harmony",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 13004,
                "output_tokens": 3659,
                "total_tokens": 16663
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 16779,
                "output_tokens": 9670,
                "total_tokens": 26449
              },
              "turn_count": 2
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 13462,
                "output_tokens": 8840,
                "total_tokens": 22302
              },
              "turn_count": 2
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 13004,
                "output_tokens": 1955,
                "total_tokens": 14959
              },
              "turn_count": 2
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1255150,
                "output_tokens": 4423,
                "total_tokens": 1259573
              },
              "turn_count": 15
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1395488,
                "output_tokens": 6056,
                "total_tokens": 1401544
              },
              "turn_count": 20
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 661868,
                "output_tokens": 4248,
                "total_tokens": 666116
              },
              "turn_count": 13
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1077617,
                "output_tokens": 4252,
                "total_tokens": 1081869
              },
              "turn_count": 15
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 481679 tokens. Please reduce the length of the messages. (tid: 202508200938531144764717470063)', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 43793,
                "output_tokens": 6167,
                "total_tokens": 49960
              },
              "turn_count": 3
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 429538 tokens. Please reduce the length of the messages. (tid: 2025082013555486560590624927198)', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 29556,
                "output_tokens": 5013,
                "total_tokens": 34569
              },
              "turn_count": 3
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 481937 tokens. Please reduce the length of the messages. (tid: 2025082023060458932814475876707)', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 54430,
                "output_tokens": 6817,
                "total_tokens": 61247
              },
              "turn_count": 4
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 443916 tokens. Please reduce the length of the messages. (tid: 2025082107260324315248991683131)', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 29169,
                "output_tokens": 6941,
                "total_tokens": 36110
              },
              "turn_count": 3
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_or_update_file: {\"owner\": \"mcpmark-eval\", \"repo\": \"harmony\", \"path\": \"MERGE_TIMELINE.txt\", \"content\": \"2025-08-06T23:21:08Z | Merge pull request #29 from axion66/improve-readme-and-checks | 3efbf742533a375fc148d75513597e139329578b\\n2025-08-06T23:20:40Z | Merge pull request #30 from Yuan-ManX/harmony-format | 9d653a4c7382abc42d115014d195d9354e7ad357\\n2025-08-06T00:12:50Z | Merge pull request #26 from jordan-wu-97/jordan/fix-function-call-atomic-bool | 82b3afb9eb043343f322c937262cc50405e892c3\\n2025-08-05T20:44:48Z | Merge pull request #18 from openai/dev/scl/better-ci | b255cbeb6274adbea774f26fd9590922ce8874ed\\n2025-08-05T20:40:06Z | Merge pull request #21 from Tialo/main | 058ef3257c24fb099aac7960c10ce51c8e55d9fe\\n2025-08-05T18:42:30Z | Merge pull request",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 310210,
                "output_tokens": 2009,
                "total_tokens": 312219
              },
              "turn_count": 6
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 568415,
                "output_tokens": 2784,
                "total_tokens": 571199
              },
              "turn_count": 8
            },
            "run-3": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_or_update_file: {\"owner\": \"mcpmark-eval\", \"repo\": \"harmony\", \"path\": \"MERGE_TIMELINE.txt\", \"content\": \"2025-08-07T03:32:52Z | Update format.md | 9fa3f54cf2a2501c7dcbf554d5fbdd0de619fdda\\n2025-08-06T23:21:08Z | Merge pull request #29 from axion66/improve-readme-and-checks | 3efbf742533a375fc148d75513597e139329578b\\n2025-08-06T23:20:40Z | Merge pull request #30 from Yuan-ManX/harmony-format | 9d653a4c7382abc42d115014d195d9354e7ad357\\n2025-08-06T16:58:46Z | fix race conditions and add offline tokenizer loading api | 1dca6392934bf4e3c403b2ecc2104e8ff3f67f45\\n2025-08-06T00:13:26Z | version bump | 9528c7b4a00a3307fd9685fc1328aee11c3d9c90\\n2025-08-06T00:12:50Z | Merge pull request #26 from jordan-wu-97/jordan/fix-function-call-atomic-bool | 82b3afb9eb043343f322c937262cc50405e892c3\\n2025-08-05T20:44:48Z | Merge pull request #18 from openai/dev/scl/better-ci | b255cbeb6274adbea774f26fd9590922ce8874ed\\n2025",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 327404,
                "output_tokens": 1951,
                "total_tokens": 329355
              },
              "turn_count": 6
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 637823,
                "output_tokens": 3023,
                "total_tokens": 640846
              },
              "turn_count": 9
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 62648,
                "output_tokens": 233,
                "total_tokens": 62881
              },
              "turn_count": 3
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1739356,
                "output_tokens": 4245,
                "total_tokens": 1743601
              },
              "turn_count": 15
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1888518,
                "output_tokens": 3559,
                "total_tokens": 1892077
              },
              "turn_count": 20
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 927875,
                "output_tokens": 3581,
                "total_tokens": 931456
              },
              "turn_count": 16
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 144101 tokens (144101 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082011072292606347052678568)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 540388,
                "output_tokens": 978,
                "total_tokens": 541366
              },
              "turn_count": 15
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Internal Server Error (tid: 202508201515336719239821497581)', 'type': 'internal_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 328869,
                "output_tokens": 1019,
                "total_tokens": 329888
              },
              "turn_count": 7
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 183840 tokens (183840 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082102382398217526030398699)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 697420,
                "output_tokens": 1015,
                "total_tokens": 698435
              },
              "turn_count": 16
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 153886 tokens (153886 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082112281065991499182847931)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 886898,
                "output_tokens": 1085,
                "total_tokens": 887983
              },
              "turn_count": 18
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 843975,
                "output_tokens": 9379,
                "total_tokens": 853354
              },
              "turn_count": 14
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 567249,
                "output_tokens": 8309,
                "total_tokens": 575558
              },
              "turn_count": 13
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 950277,
                "output_tokens": 11622,
                "total_tokens": 961899
              },
              "turn_count": 13
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1478766,
                "output_tokens": 17726,
                "total_tokens": 1496492
              },
              "turn_count": 18
            }
          }
        }
      }
    },
    "advanced_branch_strategy": {
      "task_name": "easyr1/advanced_branch_strategy",
      "category": "easyr1",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 12790,
                "output_tokens": 6688,
                "total_tokens": 19478
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_or_update_file: {\"branch\":\"develop\",\"from_branch\":\"main\",\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}{\"branch\":\"release/v1.0.0\",\"from_branch\":\"develop\",\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}{\"branch\":\"feature/protocol-serialization-fix\",\"from_branch\":\"develop\",\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}{\"branch\":\"feature/protocol-serialization-fix\",\"content\":\"# Protocol Serialization Fixes\\n\\n## Critical Fix for Data Proto Issue\\n- Enhanced serialization safety check implemented\\n- Addresses issue from commit 098931530606d22f867fd121b1dcb3225a43661f\\n- Status: Ready for integration testing\",\"message\":\"feat: Add protocol serialization fix documentation\",\"owner\":\"mcpmark-eval\",\"path\":\"PROTOCOL_FIXES.md\",\"repo\":\"EasyR1\"}",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": 0
            },
            "run-3": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_branch: {\"branch\":\"release/v1.0.0\",\"from_branch\":\"develop\",\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}{\"branch\":\"feature/protocol-serialization-fix\",\"from_branch\":\"develop\",\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 13411,
                "output_tokens": 5700,
                "total_tokens": 19111
              },
              "turn_count": 2
            },
            "run-4": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_branch: {\"branch\":\"release/v1.0.0\",\"from_branch\":\"develop\",\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}{\"branch\":\"feature/protocol-serialization-fix\",\"from_branch\":\"develop\",\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4544,
                "output_tokens": 1290,
                "total_tokens": 5834
              },
              "turn_count": 1
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 207756,
                "output_tokens": 2408,
                "total_tokens": 210164
              },
              "turn_count": 10
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 261716,
                "output_tokens": 2556,
                "total_tokens": 264272
              },
              "turn_count": 11
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 208171,
                "output_tokens": 2408,
                "total_tokens": 210579
              },
              "turn_count": 10
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 207623,
                "output_tokens": 2271,
                "total_tokens": 209894
              },
              "turn_count": 10
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 214537,
                "output_tokens": 4881,
                "total_tokens": 219418
              },
              "turn_count": 12
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 159168,
                "output_tokens": 8522,
                "total_tokens": 167690
              },
              "turn_count": 11
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 288465,
                "output_tokens": 6339,
                "total_tokens": 294804
              },
              "turn_count": 14
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 253101,
                "output_tokens": 5686,
                "total_tokens": 258787
              },
              "turn_count": 14
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 211952,
                "output_tokens": 1561,
                "total_tokens": 213513
              },
              "turn_count": 11
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 148188,
                "output_tokens": 1321,
                "total_tokens": 149509
              },
              "turn_count": 8
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 177273,
                "output_tokens": 1324,
                "total_tokens": 178597
              },
              "turn_count": 10
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 170515,
                "output_tokens": 1275,
                "total_tokens": 171790
              },
              "turn_count": 10
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2907524,
                "output_tokens": 2711,
                "total_tokens": 2910235
              },
              "turn_count": 24
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1168300,
                "output_tokens": 1845,
                "total_tokens": 1170145
              },
              "turn_count": 15
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 398753,
                "output_tokens": 1767,
                "total_tokens": 400520
              },
              "turn_count": 14
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 270952,
                "output_tokens": 1482,
                "total_tokens": 272434
              },
              "turn_count": 11
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 189316,
                "output_tokens": 1296,
                "total_tokens": 190612
              },
              "turn_count": 10
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 297631,
                "output_tokens": 1479,
                "total_tokens": 299110
              },
              "turn_count": 14
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 200041,
                "output_tokens": 1271,
                "total_tokens": 201312
              },
              "turn_count": 11
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 399439,
                "output_tokens": 1657,
                "total_tokens": 401096
              },
              "turn_count": 17
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 80135,
                "output_tokens": 1151,
                "total_tokens": 81286
              },
              "turn_count": 7
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 157269,
                "output_tokens": 3098,
                "total_tokens": 160367
              },
              "turn_count": 11
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 212753,
                "output_tokens": 1844,
                "total_tokens": 214597
              },
              "turn_count": 12
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 152859,
                "output_tokens": 3234,
                "total_tokens": 156093
              },
              "turn_count": 11
            }
          }
        }
      }
    },
    "issue_pr_commit_workflow": {
      "task_name": "harmony/issue_pr_commit_workflow",
      "category": "harmony",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 153091,
                "output_tokens": 4296,
                "total_tokens": 157387
              },
              "turn_count": 10
            },
            "run-2": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_branch: {\"body\":\"## Problem\\nThis issue addresses a race condition that occurs when multiple threads attempt to download and load HarmonyEncoding tokenizers simultaneously. The current implementation is not thread-safe, leading to corrupted tokenizer file downloads and unpredictable behavior under concurrent access.\\n\\n## Root Cause\\nThe root cause of this bug is the lack of a file locking mechanism to protect the tokenizer download and initialization process. When multiple threads execute this critical section at the same time, they can interfere with each other's file operations.\\n\\n## Expected Solution\\n- Implement a mutex-based file locking mechanism to ensure that only one thread can download and load a tokenizer at a time.\\n- Use a thread-safe initialization pattern to prevent redundant downloads and ensure the tokenizer is loaded only once.\\n- Provide an offline loading API to allow users to load tokenizers from a local file path, as requested in issue #1.\\n\\nThis bug is related to the feature request in #1 and the original race condition report in #6.\\nKeywords: \\\"multiple threads\\\", \\\"tokenizer file downloads\\\", \\\"mutex-based file locking\\\"\",\"owner\":\"mcpmark-eval\",\"repo\":\"harmony\",\"title\":\"Bug: race condition in HarmonyEncoding during concurrent access\"}{\"branch\":\"fix/race-condition-tokenizer-loading\",\"from_branch\":\"main\",\"owner\":\"mcpmark-eval\",\"repo\":\"harmony\"}",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": 0
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4811,
                "output_tokens": 0,
                "total_tokens": 4811
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4811,
                "output_tokens": 0,
                "total_tokens": 4811
              },
              "turn_count": 1
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 238786,
                "output_tokens": 3887,
                "total_tokens": 242673
              },
              "turn_count": 10
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 231195,
                "output_tokens": 3550,
                "total_tokens": 234745
              },
              "turn_count": 10
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 231844,
                "output_tokens": 3560,
                "total_tokens": 235404
              },
              "turn_count": 10
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 232384,
                "output_tokens": 3571,
                "total_tokens": 235955
              },
              "turn_count": 10
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 149028,
                "output_tokens": 5070,
                "total_tokens": 154098
              },
              "turn_count": 10
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 147852,
                "output_tokens": 4545,
                "total_tokens": 152397
              },
              "turn_count": 10
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 147885,
                "output_tokens": 5889,
                "total_tokens": 153774
              },
              "turn_count": 10
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 146019,
                "output_tokens": 4392,
                "total_tokens": 150411
              },
              "turn_count": 10
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 203805,
                "output_tokens": 3075,
                "total_tokens": 206880
              },
              "turn_count": 11
            },
            "run-2": {
              "success": false,
              "error_message": "Invalid JSON input for tool submit_pending_pull_request_review: {\"owner\": \"mcpmark-eval\", \"repo\": \"pullNumber\": 28, \"event\": \"COMMENT\", \"body\": \"Technical review completed. The implementation successfully addresses the race condition through proper use of OnceLock and Mutex patterns. The solution provides thread-safe tokenizer loading while maintaining backward compatibility and adding the requested offline loading API from issue #1.\"}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 115873,
                "output_tokens": 2178,
                "total_tokens": 118051
              },
              "turn_count": 6
            },
            "run-3": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 29168,
                "output_tokens": 301,
                "total_tokens": 29469
              },
              "turn_count": 2
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 214995,
                "output_tokens": 2964,
                "total_tokens": 217959
              },
              "turn_count": 10
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 313648,
                "output_tokens": 1945,
                "total_tokens": 315593
              },
              "turn_count": 12
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 439570,
                "output_tokens": 2801,
                "total_tokens": 442371
              },
              "turn_count": 16
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 342443,
                "output_tokens": 2508,
                "total_tokens": 344951
              },
              "turn_count": 13
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 287035,
                "output_tokens": 2130,
                "total_tokens": 289165
              },
              "turn_count": 11
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 213758,
                "output_tokens": 2612,
                "total_tokens": 216370
              },
              "turn_count": 10
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 212920,
                "output_tokens": 2353,
                "total_tokens": 215273
              },
              "turn_count": 10
            },
            "run-3": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": 0
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 213362,
                "output_tokens": 2236,
                "total_tokens": 215598
              },
              "turn_count": 10
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 144490,
                "output_tokens": 2266,
                "total_tokens": 146756
              },
              "turn_count": 10
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 145059,
                "output_tokens": 2182,
                "total_tokens": 147241
              },
              "turn_count": 10
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 146166,
                "output_tokens": 2699,
                "total_tokens": 148865
              },
              "turn_count": 10
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 179909,
                "output_tokens": 3090,
                "total_tokens": 182999
              },
              "turn_count": 12
            }
          }
        }
      }
    },
    "find_commit_date": {
      "task_name": "build_your_own_x/find_commit_date",
      "category": "build_your_own_x",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 41202,
                "output_tokens": 11934,
                "total_tokens": 53136
              },
              "turn_count": 5
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 11805,
                "output_tokens": 2510,
                "total_tokens": 14315
              },
              "turn_count": 2
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 11800,
                "output_tokens": 5607,
                "total_tokens": 17407
              },
              "turn_count": 2
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 38883,
                "output_tokens": 8891,
                "total_tokens": 47774
              },
              "turn_count": 3
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 215840 tokens > 200000 maximum (tid: 2025082009452725077726475847467)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 97775,
                "output_tokens": 526,
                "total_tokens": 98301
              },
              "turn_count": 4
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 215826 tokens > 200000 maximum (tid: 2025082014032189542049059325647)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 131607,
                "output_tokens": 807,
                "total_tokens": 132414
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 212282 tokens > 200000 maximum (tid: 202508202356012553466117293057)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 64464,
                "output_tokens": 458,
                "total_tokens": 64922
              },
              "turn_count": 3
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 212333 tokens > 200000 maximum (tid: 2025082109550287360751529272183)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 64473,
                "output_tokens": 464,
                "total_tokens": 64937
              },
              "turn_count": 3
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 8835,
                "output_tokens": 1320,
                "total_tokens": 10155
              },
              "turn_count": 1
            },
            "run-2": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": 0
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 386946 tokens. Please reduce the length of the messages. (tid: 2025082021555088405190346688167)', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1096555,
                "output_tokens": 11088,
                "total_tokens": 1107643
              },
              "turn_count": 8
            },
            "run-4": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 336392,
                "output_tokens": 3366,
                "total_tokens": 339758
              },
              "turn_count": 4
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 202508201128234410445687312463)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 51032,
                "output_tokens": 336,
                "total_tokens": 51368
              },
              "turn_count": 3
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082016110735671209293887580)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 51034,
                "output_tokens": 335,
                "total_tokens": 51369
              },
              "turn_count": 3
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082123131355275885629112674)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 57923,
                "output_tokens": 241,
                "total_tokens": 58164
              },
              "turn_count": 3
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082204330364590399910956928)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 51026,
                "output_tokens": 342,
                "total_tokens": 51368
              },
              "turn_count": 3
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2001661,
                "output_tokens": 1760,
                "total_tokens": 2003421
              },
              "turn_count": 19
            },
            "run-2": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 93334,
                "output_tokens": 319,
                "total_tokens": 93653
              },
              "turn_count": 4
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 572324,
                "output_tokens": 670,
                "total_tokens": 572994
              },
              "turn_count": 7
            },
            "run-4": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 53753,
                "output_tokens": 179,
                "total_tokens": 53932
              },
              "turn_count": 2
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 198128 tokens (198128 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082010291044839770221863381)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 59788,
                "output_tokens": 334,
                "total_tokens": 60122
              },
              "turn_count": 3
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 198007 tokens (198007 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082014353019718146435296460)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 59788,
                "output_tokens": 206,
                "total_tokens": 59994
              },
              "turn_count": 3
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 285049 tokens (285049 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082101364810736487298748521)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 324517,
                "output_tokens": 416,
                "total_tokens": 324933
              },
              "turn_count": 6
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 198013 tokens (198013 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082111174030206430011135550)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 59794,
                "output_tokens": 209,
                "total_tokens": 60003
              },
              "turn_count": 3
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 751398,
                "output_tokens": 1933,
                "total_tokens": 753331
              },
              "turn_count": 7
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 167883,
                "output_tokens": 1215,
                "total_tokens": 169098
              },
              "turn_count": 2
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Your organization must be verified to stream this model. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate. (tid: 2025082020323920723030317255502)', 'type': 'invalid_request_error', 'param': 'stream', 'code': 'unsupported_value'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 388243,
                "output_tokens": 2457,
                "total_tokens": 390700
              },
              "turn_count": 5
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 727129,
                "output_tokens": 2922,
                "total_tokens": 730051
              },
              "turn_count": 6
            }
          }
        }
      }
    },
    "feature_commit_tracking": {
      "task_name": "claude-code/feature_commit_tracking",
      "category": "claude-code",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 12905,
                "output_tokens": 5074,
                "total_tokens": 17979
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 12634,
                "output_tokens": 6434,
                "total_tokens": 19068
              },
              "turn_count": 2
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 12875,
                "output_tokens": 4183,
                "total_tokens": 17058
              },
              "turn_count": 2
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 13760,
                "output_tokens": 6879,
                "total_tokens": 20639
              },
              "turn_count": 2
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 219669 tokens > 200000 maximum (tid: 2025082009561489358192359602650)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 979080,
                "output_tokens": 1108,
                "total_tokens": 980188
              },
              "turn_count": 8
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 206974 tokens > 200000 maximum (tid: 2025082014110990910916886534893)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 371512,
                "output_tokens": 632,
                "total_tokens": 372144
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 209003 tokens > 200000 maximum (tid: 2025082100142684833502167886696)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 730857,
                "output_tokens": 852,
                "total_tokens": 731709
              },
              "turn_count": 7
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 203482 tokens > 200000 maximum (tid: 2025082110151691319777066964786)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1158677,
                "output_tokens": 1324,
                "total_tokens": 1160001
              },
              "turn_count": 11
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 556434 tokens. Please reduce the length of the messages. (tid: 2025082009234566358155209761139)', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 42881,
                "output_tokens": 3669,
                "total_tokens": 46550
              },
              "turn_count": 4
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 554853 tokens. Please reduce the length of the messages. (tid: 2025082013365019872316942252162)', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 18860,
                "output_tokens": 1972,
                "total_tokens": 20832
              },
              "turn_count": 2
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 281648 tokens. Please reduce the length of the messages. (tid: 2025082022214631833837514034548)', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 161908,
                "output_tokens": 3785,
                "total_tokens": 165693
              },
              "turn_count": 3
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 416571 tokens. Please reduce the length of the messages. (tid: 2025082106434465839038498582533)', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 315304,
                "output_tokens": 4863,
                "total_tokens": 320167
              },
              "turn_count": 5
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082011480780890221214434271)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 230787,
                "output_tokens": 359,
                "total_tokens": 231146
              },
              "turn_count": 6
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082016331135957320656371839)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 361175,
                "output_tokens": 470,
                "total_tokens": 361645
              },
              "turn_count": 8
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082123340730301715515419446)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 85584,
                "output_tokens": 289,
                "total_tokens": 85873
              },
              "turn_count": 6
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082204541520911366666660742)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 370592,
                "output_tokens": 435,
                "total_tokens": 371027
              },
              "turn_count": 8
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 479980,
                "output_tokens": 445,
                "total_tokens": 480425
              },
              "turn_count": 6
            },
            "run-2": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 260499,
                "output_tokens": 476,
                "total_tokens": 260975
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1250582,
                "output_tokens": 677,
                "total_tokens": 1251259
              },
              "turn_count": 10
            },
            "run-4": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 239469,
                "output_tokens": 300,
                "total_tokens": 239769
              },
              "turn_count": 4
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 170714 tokens (170714 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082010482240403859654785255)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 804658,
                "output_tokens": 867,
                "total_tokens": 805525
              },
              "turn_count": 15
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 192326 tokens (192326 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082014494395357762115163720)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1548554,
                "output_tokens": 935,
                "total_tokens": 1549489
              },
              "turn_count": 20
            },
            "run-3": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 107789,
                "output_tokens": 347,
                "total_tokens": 108136
              },
              "turn_count": 5
            },
            "run-4": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 252075,
                "output_tokens": 387,
                "total_tokens": 252462
              },
              "turn_count": 6
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 314706,
                "output_tokens": 1109,
                "total_tokens": 315815
              },
              "turn_count": 4
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 475365,
                "output_tokens": 1007,
                "total_tokens": 476372
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Your organization must be verified to stream this model. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate. (tid: 2025082020552156600393124874791)', 'type': 'invalid_request_error', 'param': 'stream', 'code': 'unsupported_value'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1561074,
                "output_tokens": 4229,
                "total_tokens": 1565303
              },
              "turn_count": 17
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 171338,
                "output_tokens": 916,
                "total_tokens": 172254
              },
              "turn_count": 4
            }
          }
        }
      }
    },
    "issue_management_workflow": {
      "task_name": "mcpmark-cicd/issue_management_workflow",
      "category": "mcpmark-cicd",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 134874,
                "output_tokens": 12717,
                "total_tokens": 147591
              },
              "turn_count": 8
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 135483,
                "output_tokens": 12333,
                "total_tokens": 147816
              },
              "turn_count": 8
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 146525,
                "output_tokens": 8853,
                "total_tokens": 155378
              },
              "turn_count": 9
            },
            "run-4": {
              "success": false,
              "error_message": "Command '['/home/zijian/miniconda3/envs/benchmark/bin/python', '/home/zijian/mcpmark/tasks/github/mcpmark-cicd/issue_management_workflow/verify.py']' timed out after 299.9999880889991 seconds",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 132268,
                "output_tokens": 11568,
                "total_tokens": 143836
              },
              "turn_count": 8
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Command '['/home/zijian/miniconda3/envs/benchmark/bin/python', '/home/zijian/mcpmark/tasks/github/mcpmark-cicd/issue_management_workflow/verify.py']' timed out after 299.9999880050309 seconds",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": null
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1766128,
                "output_tokens": 15010,
                "total_tokens": 1781138
              },
              "turn_count": 22
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 292415,
                "output_tokens": 8545,
                "total_tokens": 300960
              },
              "turn_count": 11
            },
            "run-4": {
              "success": false,
              "error_message": "Command '['/home/zijian/miniconda3/envs/benchmark/bin/python', '/home/zijian/mcpmark/tasks/github/mcpmark-cicd/issue_management_workflow/verify.py']' timed out after 299.99998671400044 seconds",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 766158,
                "output_tokens": 9977,
                "total_tokens": 776135
              },
              "turn_count": 20
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 189144,
                "output_tokens": 24536,
                "total_tokens": 213680
              },
              "turn_count": 10
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 490744,
                "output_tokens": 51906,
                "total_tokens": 542650
              },
              "turn_count": 17
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 382930,
                "output_tokens": 31141,
                "total_tokens": 414071
              },
              "turn_count": 15
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 643026,
                "output_tokens": 53153,
                "total_tokens": 696179
              },
              "turn_count": 20
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 54848,
                "output_tokens": 869,
                "total_tokens": 55717
              },
              "turn_count": 4
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 277872,
                "output_tokens": 6976,
                "total_tokens": 284848
              },
              "turn_count": 11
            },
            "run-3": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 54929,
                "output_tokens": 893,
                "total_tokens": 55822
              },
              "turn_count": 4
            },
            "run-4": {
              "success": false,
              "error_message": "Command '['/home/zijian/miniconda3/envs/benchmark/bin/python', '/home/zijian/mcpmark/tasks/github/mcpmark-cicd/issue_management_workflow/verify.py']' timed out after 299.9999870640022 seconds",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 288450,
                "output_tokens": 6766,
                "total_tokens": 295216
              },
              "turn_count": 12
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1071363,
                "output_tokens": 10707,
                "total_tokens": 1082070
              },
              "turn_count": 28
            },
            "run-2": {
              "success": false,
              "error_message": "Command '['/home/zijian/miniconda3/envs/benchmark/bin/python', '/home/zijian/mcpmark/tasks/github/mcpmark-cicd/issue_management_workflow/verify.py']' timed out after 299.9999877969967 seconds",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 242050,
                "output_tokens": 4085,
                "total_tokens": 246135
              },
              "turn_count": 9
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 591962,
                "output_tokens": 4369,
                "total_tokens": 596331
              },
              "turn_count": 19
            },
            "run-4": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 135056,
                "output_tokens": 3576,
                "total_tokens": 138632
              },
              "turn_count": 6
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Command '['/home/zijian/miniconda3/envs/benchmark/bin/python', '/home/zijian/mcpmark/tasks/github/mcpmark-cicd/issue_management_workflow/verify.py']' timed out after 299.9999882999982 seconds",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 262706,
                "output_tokens": 6120,
                "total_tokens": 268826
              },
              "turn_count": 11
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 326253,
                "output_tokens": 6532,
                "total_tokens": 332785
              },
              "turn_count": 13
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 292398,
                "output_tokens": 6805,
                "total_tokens": 299203
              },
              "turn_count": 12
            },
            "run-4": {
              "success": false,
              "error_message": "Command '['/home/zijian/miniconda3/envs/benchmark/bin/python', '/home/zijian/mcpmark/tasks/github/mcpmark-cicd/issue_management_workflow/verify.py']' timed out after 299.9999869330004 seconds",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 262311,
                "output_tokens": 5832,
                "total_tokens": 268143
              },
              "turn_count": 11
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Command '['/home/zijian/miniconda3/envs/benchmark/bin/python', '/home/zijian/mcpmark/tasks/github/mcpmark-cicd/issue_management_workflow/verify.py']' timed out after 299.999988976866 seconds",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": null
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 105328,
                "output_tokens": 6523,
                "total_tokens": 111851
              },
              "turn_count": 8
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 70776,
                "output_tokens": 6438,
                "total_tokens": 77214
              },
              "turn_count": 5
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 132366,
                "output_tokens": 7460,
                "total_tokens": 139826
              },
              "turn_count": 8
            }
          }
        }
      }
    },
    "automated_changelog_generation": {
      "task_name": "claude-code/automated_changelog_generation",
      "category": "claude-code",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 13180,
                "output_tokens": 1808,
                "total_tokens": 14988
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4708,
                "output_tokens": 2449,
                "total_tokens": 7157
              },
              "turn_count": 1
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 13624,
                "output_tokens": 3896,
                "total_tokens": 17520
              },
              "turn_count": 2
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 13646,
                "output_tokens": 5670,
                "total_tokens": 19316
              },
              "turn_count": 2
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 451427,
                "output_tokens": 8377,
                "total_tokens": 459804
              },
              "turn_count": 10
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 481726,
                "output_tokens": 8053,
                "total_tokens": 489779
              },
              "turn_count": 10
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 458500,
                "output_tokens": 10490,
                "total_tokens": 468990
              },
              "turn_count": 10
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 489538,
                "output_tokens": 10068,
                "total_tokens": 499606
              },
              "turn_count": 10
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 200087,
                "output_tokens": 13351,
                "total_tokens": 213438
              },
              "turn_count": 6
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 183962,
                "output_tokens": 10106,
                "total_tokens": 194068
              },
              "turn_count": 6
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 211667,
                "output_tokens": 13628,
                "total_tokens": 225295
              },
              "turn_count": 7
            },
            "run-4": {
              "success": false,
              "error_message": "Invalid JSON input for tool merge_pull_request: {\"owner\":\"mcpmark-eval\",\"repo\":\"claude-code\",\"pullNumber\":48,\"merge_method\":\"squash\",\"commit_title\":\"docs: Generated changelog and migration documentation\",\"commit_message\":\"Automated documentation updates: changelog, migration guide, issue analysis, and PR integration plan.\"",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 105539,
                "output_tokens": 8536,
                "total_tokens": 114075
              },
              "turn_count": 4
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 504328,
                "output_tokens": 1766,
                "total_tokens": 506094
              },
              "turn_count": 11
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082016183319473951608011164)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 166775,
                "output_tokens": 383,
                "total_tokens": 167158
              },
              "turn_count": 6
            },
            "run-3": {
              "success": false,
              "error_message": "Github Operation timed out",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 127291,
                "output_tokens": 2064,
                "total_tokens": 129355
              },
              "turn_count": 5
            },
            "run-4": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_or_update_file: {\"owner\": \"mcpmark-eval\", \"repo\": \"claude-code\", \"path\": \"docs/MIGRATION_GUIDE.md\", \"content\": \"# Migration Guide for Pending Features\\n\\nThis guide provides comprehensive information about all open pull requests and their impact on the codebase. Use this document to understand upcoming changes and prepare for integration.\\n\\n## PR #53: Automate Docker Image Builds with GitHub Actions\\n\\n**Title**: FeatAutomate Docker Image Builds with GitHub Actions\\n**Status**: Open\\n**Author**: zjwu0522\\n\\n### Summary of Changes\\nThis PR introduces automated Docker image building through GitHub Actions. The workflow will trigger automatically on branch updates, ensuring fresh Docker images are built for the latest code changes.\\n\\n### Key Features\\n- **Automated CI/CD**: New GitHub Actions workflow for Docker builds\\n- **Continuous Integration**: Automatic builds on push events\\n- **Best Practices**: Leverages GitHub Action templates for reliability\\n- **Early Issue Detection**: Catches build issues early in development cycle\\n\\n### New Configuration\\n- **File**: `.github/workflows/docker-build.yml` (or similar)\\n- **Trigger**: `push` events to branches\\n- **Action**: Automated Docker image building\\n\\n### Environment Variables\\n- No new environment variables required\\n- Uses standard GitHub Actions context variables\\n\\n### Installation/Usage Instructions\\n1. **Testing**: Push a new commit to any branch\\n2. **Monitoring**: Check GitHub Actions tab for workflow execution\\n3. **Validation**: Confirm workflow completes successfully\\n4. **Integration**: Docker images will be available for deployment\\n\\n### Migration Impact\\n- **Risk Level**: Low - additive feature\\n- **Dependencies**: GitHub Actions infrastructure\\n- **Breaking Changes**: None\\n\\n---\\n\\n## PR #52: Add Shell Completions (bash, zsh, fish)\\n\\n**Title**: feat: add shell completions (bash, zsh, fish)\\n**Status**: Open\\n**Author**: zjwu0522\\n\\n### Summary of Changes\\nThis PR adds static shell completion scripts for improved command-line experience across different shells. Provides tab autocompletion for Claude Code commands.\\n\\n### Key Features\\n- **Multi-shell Support**: bash, zsh, and fish completions\\n- **Static Scripts**: Pre-built completion files\\n- **Enhanced UX**: Tab autocompletion for commands\\n- **No Binary Changes**: Works with existing Claude Code binary\\n\\n### New Files Added\\n- `shell-completions/claude-completions.bash`\\n- `shell-completions/claude-completions.zsh`\\n- `shell-completions/claude-completions.fish`\\n\\n### Installation Instructions\\n1. **Manual Installation**: Copy appropriate completion file to shell configuration\\n2. **Bash**: `source shell-completions/claude-completions.bash`\\n3. **Zsh**: `source shell-completions/claude-completions.zsh`\\n4. **Fish**: `source shell-completions/claude-completions.fish`\\n\\n### Alternative Usage (if supported)\\n```bash\\nsource <(claude completion $SHELL)\\n```\\n*Note: This requires upstream integration which is not currently possible*\\n\\n### Migration Impact\\n- **Risk Level**: Very Low - additive feature\\n- **Dependencies**: None\\n- **Breaking Changes**: None\\n- **User Benefit**: Improved CLI experience\\n\\n---\\n\\n## PR #51: Enhance Statsig Event Logging in GitHub Workflows\\n\\n**Title**: Enhance Statsig event logging in GitHub workflows\\n**Status**: Open\\n**Author**: zjwu0522\\n\\n### Summary of Changes\\nThis PR enhances the existing Statsig event logging system by adding additional events for issue management workflows. Specifically tracks duplicate issue handling and comment additions.\\n\\n### Key Features\\n- **Enhanced Logging**: Additional Statsig events for issue management\\n- **Duplicate Tracking**: Logs when issues are closed as duplicates\\n- **Comment Monitoring**: Tracks duplicate comment additions\\n- **Consistency**: Maintains existing logging patterns\\n\\n### New Events Logged\\n- Issue closed as duplicate events\\n- Duplicate comment addition events\\n- Enhanced issue lifecycle tracking\\n\\n### Configuration Changes\\n- **Location**: GitHub workflow files\\n- **Integration**: Statsig event logging system\\n- **Pattern**: Consistent with existing logging approaches\\n\\n### Migration Impact\\n- **Risk Level**: Low - logging enhancement\\n- **Dependencies**: Statsig integration\\n- **Breaking Changes**: None\\n- **Performance**: Minimal impact on workflow execution\\n\\n### Testing Requirements\\n1. **Event Validation**: Verify new events are properly logged\\n2. **Statsig Integration**: Confirm events appear in Statsig dashboard\\n3. **Workflow Testing**: Ensure GitHub Actions complete successfully\\n4. **Data Consistency**: Validate event data accuracy",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 119167,
                "output_tokens": 1057,
                "total_tokens": 120224
              },
              "turn_count": 5
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 306956,
                "output_tokens": 683,
                "total_tokens": 307639
              },
              "turn_count": 11
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1119270,
                "output_tokens": 4667,
                "total_tokens": 1123937
              },
              "turn_count": 18
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 469901,
                "output_tokens": 3622,
                "total_tokens": 473523
              },
              "turn_count": 11
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 544850,
                "output_tokens": 4355,
                "total_tokens": 549205
              },
              "turn_count": 12
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 406789,
                "output_tokens": 6514,
                "total_tokens": 413303
              },
              "turn_count": 9
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 558170,
                "output_tokens": 5891,
                "total_tokens": 564061
              },
              "turn_count": 17
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 510385,
                "output_tokens": 6217,
                "total_tokens": 516602
              },
              "turn_count": 13
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 446320,
                "output_tokens": 6713,
                "total_tokens": 453033
              },
              "turn_count": 11
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 227234,
                "output_tokens": 5894,
                "total_tokens": 233128
              },
              "turn_count": 8
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 217622,
                "output_tokens": 5967,
                "total_tokens": 223589
              },
              "turn_count": 7
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 217388,
                "output_tokens": 5503,
                "total_tokens": 222891
              },
              "turn_count": 7
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 218743,
                "output_tokens": 5738,
                "total_tokens": 224481
              },
              "turn_count": 7
            }
          }
        }
      }
    },
    "performance_regression_investigation": {
      "task_name": "easyr1/performance_regression_investigation",
      "category": "easyr1",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 19691,
                "output_tokens": 6764,
                "total_tokens": 26455
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_issue: {\"labels\":[\"bug\",\"performance\",\"investigation\"],\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\",\"title\":\"Performance Regression Analysis: Data Protocol Changes\"}{\"branch\":\"investigate-protocol-changes\",\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}{\"branch\":\"investigate-batch-processing\",\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}{\"branch\":\"investigate-memory-usage\",\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}{\"base\":\"main\",\"head\":\"investigate-protocol-changes\",\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\",\"title\":\"Performance Analysis: Protocol Changes Investigation\"}{\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\",\"title\":\"Test Performance Impact: fix multi modal data oom\"}{\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\",\"title\":\"Test Performance Impact: upgrade vllm to 0.10\"}{\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\",\"title\":\"Test Performance Impact: non blocking false by default\"}",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": 0
            },
            "run-3": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_or_update_file: {\"issue_number\":54,\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\",\"sub_issue_id\":3340158886}{\"issue_number\":54,\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\",\"sub_issue_id\":3340158885}{\"issue_number\":54,\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\",\"sub_issue_id\":3340158887}{\"body\":\"File `verl/protocol.py` was modified in commit `098931530606d22f867fd121b1dcb3225a43661f`.\",\"issue_number\":54,\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}{\"body\":\"File `examples/config.yaml` was modified in commit `098931530606d22f867fd121b1dcb3225a43661f`.\",\"issue_number\":54,\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}{\"branch\":\"investigate-protocol-changes\",\"content\":\"This PR is for investigating performance regressions related to protocol changes.\",\"message\":\"Add analysis placeholder\",\"owner\":\"mcpmark-eval\",\"path\":\"analysis.md\",\"repo\":\"EasyR1\"}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4461,
                "output_tokens": 4248,
                "total_tokens": 8709
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_pull_request: {\"branch\":\"investigate-protocol-changes\",\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}{\"branch\":\"investigate-batch-processing\",\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}{\"branch\":\"investigate-memory-usage\",\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}{\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\",\"title\":\"Test Performance Impact: fix multi modal data oom\"}{\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\",\"title\":\"Test Performance Impact: upgrade vllm to 0.10\"}{\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\",\"title\":\"Test Performance Impact: non blocking false by default\"}{\"body\":\"File change from commit `098931530606d22f867fd121b1dcb3225a43661f`: `verl/protocol.py`\",\"issue_number\":51,\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}{\"body\":\"File change from commit `098931530606d22f867fd121b1dcb3225a43661f`: `examples/config.yaml`\",\"issue_number\":51,\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\"}{\"base\":\"main\",\"head\":\"investigate-protocol-changes\",\"owner\":\"mcpmark-eval\",\"repo\":\"EasyR1\",\"title\":\"Performance Analysis: Protocol Changes Investigation\"}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4461,
                "output_tokens": 2367,
                "total_tokens": 6828
              },
              "turn_count": 1
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 483882,
                "output_tokens": 5707,
                "total_tokens": 489589
              },
              "turn_count": 17
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 630215,
                "output_tokens": 7101,
                "total_tokens": 637316
              },
              "turn_count": 19
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 435531,
                "output_tokens": 5924,
                "total_tokens": 441455
              },
              "turn_count": 16
            },
            "run-4": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 15230,
                "output_tokens": 150,
                "total_tokens": 15380
              },
              "turn_count": 1
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 97937,
                "output_tokens": 7987,
                "total_tokens": 105924
              },
              "turn_count": 5
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 284444,
                "output_tokens": 9451,
                "total_tokens": 293895
              },
              "turn_count": 17
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 248889,
                "output_tokens": 6860,
                "total_tokens": 255749
              },
              "turn_count": 11
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 189215,
                "output_tokens": 7571,
                "total_tokens": 196786
              },
              "turn_count": 11
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 239122,
                "output_tokens": 3606,
                "total_tokens": 242728
              },
              "turn_count": 10
            },
            "run-2": {
              "success": false,
              "error_message": "Invalid JSON input for tool add_issue_comment: {\"owner\": \"mcpmark-eval\", \"repo\": \"EasyR1\", \"issue_number\": 51, \"body\": \"## Performance Regression Hypotheses\\n\\nBased on the detailed file analysis, here are the key areas where performance regressions might occur:\\n\\n### 1. **Batch Size Impact (HIGH PRIORITY)**\\nThe dramatic reduction in micro-batch sizes (75-87.5% reduction) in `examples/config.yaml` is the most likely source of performance regression:\\n- **Throughput Impact**: Smaller batches = more iterations = potentially slower training\\n- **GPU Utilization**: Risk of GPU underutilization with very small batch sizes\\n- **Memory vs Speed Trade-off**: While memory usage decreases, training speed may suffer significantly\\n\\n### 2. **Protocol Serialization Changes (MEDIUM PRIORITY)**\\nChanges in `verl/protocol.py` to handle `None` batch cases:\\n- **Memory Efficiency**: Should improve memory usage for edge cases\\n- **Serialization Overhead**: New TensorDict operations might add slight overhead\\n- **Edge Case Performance**: Better handling of None batches but potential overhead for normal cases\\n\\n### 3. **Configuration Consistency (MEDIUM PRIORITY)**\\nRemoval of explicit parameters in shell scripts means all configurations now flow through the reduced batch sizes in config.yaml, amplifying the batch size impact across all training runs.\\n\\n### Testing Recommendations\\n1. **Baseline Comparison**: Compare training speed before/after commit with identical hardware\\n2. **Batch Size Ablation**: Test intermediate batch sizes (e.g., 2, 4, 8) to find optimal balance\\n",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 95076,
                "output_tokens": 1482,
                "total_tokens": 96558
              },
              "turn_count": 5
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 466799,
                "output_tokens": 4298,
                "total_tokens": 471097
              },
              "turn_count": 17
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 281644,
                "output_tokens": 3846,
                "total_tokens": 285490
              },
              "turn_count": 10
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 277869,
                "output_tokens": 1025,
                "total_tokens": 278894
              },
              "turn_count": 12
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 614166,
                "output_tokens": 2979,
                "total_tokens": 617145
              },
              "turn_count": 21
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 874692,
                "output_tokens": 3050,
                "total_tokens": 877742
              },
              "turn_count": 26
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 797415,
                "output_tokens": 3131,
                "total_tokens": 800546
              },
              "turn_count": 27
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 225019,
                "output_tokens": 2644,
                "total_tokens": 227663
              },
              "turn_count": 9
            },
            "run-2": {
              "success": false,
              "error_message": "Github Resource already exists",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 186406,
                "output_tokens": 1969,
                "total_tokens": 188375
              },
              "turn_count": 8
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 312627,
                "output_tokens": 3331,
                "total_tokens": 315958
              },
              "turn_count": 11
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 424953,
                "output_tokens": 2952,
                "total_tokens": 427905
              },
              "turn_count": 19
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 286630,
                "output_tokens": 3876,
                "total_tokens": 290506
              },
              "turn_count": 19
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 260369,
                "output_tokens": 2010,
                "total_tokens": 262379
              },
              "turn_count": 16
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 325383,
                "output_tokens": 4541,
                "total_tokens": 329924
              },
              "turn_count": 20
            },
            "run-4": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 39836,
                "output_tokens": 868,
                "total_tokens": 40704
              },
              "turn_count": 4
            }
          }
        }
      }
    },
    "claude_collaboration_analysis": {
      "task_name": "claude-code/claude_collaboration_analysis",
      "category": "claude-code",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 13460,
                "output_tokens": 6179,
                "total_tokens": 19639
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4391,
                "output_tokens": 1296,
                "total_tokens": 5687
              },
              "turn_count": 1
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4381,
                "output_tokens": 1777,
                "total_tokens": 6158
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4381,
                "output_tokens": 1678,
                "total_tokens": 6059
              },
              "turn_count": 1
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 207894 tokens > 200000 maximum (tid: 202508200950595808975830175792)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 408756,
                "output_tokens": 311,
                "total_tokens": 409067
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 207909 tokens > 200000 maximum (tid: 202508201408036089558129493552)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 211964,
                "output_tokens": 283,
                "total_tokens": 212247
              },
              "turn_count": 2
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 207662 tokens > 200000 maximum (tid: 2025082100070928157466987211112)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 233688,
                "output_tokens": 383,
                "total_tokens": 234071
              },
              "turn_count": 3
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 209668 tokens > 200000 maximum (tid: 2025082110062144543998028049171)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 406727,
                "output_tokens": 464,
                "total_tokens": 407191
              },
              "turn_count": 3
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 554566 tokens. Please reduce the length of the messages. (tid: 2025082009201237467935610116790)', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 18650,
                "output_tokens": 2872,
                "total_tokens": 21522
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 315794,
                "output_tokens": 10850,
                "total_tokens": 326644
              },
              "turn_count": 4
            },
            "run-3": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_or_update_file: {\"owner\":\"mcpmark-eval",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 379139,
                "output_tokens": 7186,
                "total_tokens": 386325
              },
              "turn_count": 3
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 613793,
                "output_tokens": 13286,
                "total_tokens": 627079
              },
              "turn_count": 5
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082011385269960414391292122)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 106274,
                "output_tokens": 333,
                "total_tokens": 106607
              },
              "turn_count": 5
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 429701,
                "output_tokens": 1896,
                "total_tokens": 431597
              },
              "turn_count": 9
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082123244213619162357174019)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 11972,
                "output_tokens": 64,
                "total_tokens": 12036
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082204434232332527905780847)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 11972,
                "output_tokens": 64,
                "total_tokens": 12036
              },
              "turn_count": 1
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 965265,
                "output_tokens": 1145,
                "total_tokens": 966410
              },
              "turn_count": 8
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 711359,
                "output_tokens": 1205,
                "total_tokens": 712564
              },
              "turn_count": 10
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'This endpoint\\'s maximum context length is 262144 tokens. However, you requested about 360127 tokens (346655 of text input, 13472 of tool input). Please reduce the length of either one, or use the \"middle-out\" transform to compress your prompt automatically.', 'code': 400, 'metadata': {'provider_name': None}}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1187092,
                "output_tokens": 829,
                "total_tokens": 1187921
              },
              "turn_count": 6
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 696037,
                "output_tokens": 906,
                "total_tokens": 696943
              },
              "turn_count": 8
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 228258 tokens (228258 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082010360879506461770084771)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 143981,
                "output_tokens": 355,
                "total_tokens": 144336
              },
              "turn_count": 6
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 172877 tokens (172877 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082014405254048688213611643)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 76236,
                "output_tokens": 275,
                "total_tokens": 76511
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 169698 tokens (169698 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082101502469540171978225911)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 14476,
                "output_tokens": 73,
                "total_tokens": 14549
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 169590 tokens (169590 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082111314152491353800788328)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 14476,
                "output_tokens": 65,
                "total_tokens": 14541
              },
              "turn_count": 1
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 304969,
                "output_tokens": 1193,
                "total_tokens": 306162
              },
              "turn_count": 3
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 9031,
                "output_tokens": 284,
                "total_tokens": 9315
              },
              "turn_count": 1
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 304297,
                "output_tokens": 1112,
                "total_tokens": 305409
              },
              "turn_count": 3
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 315239,
                "output_tokens": 1743,
                "total_tokens": 316982
              },
              "turn_count": 4
            }
          }
        }
      }
    },
    "find_legacy_name": {
      "task_name": "missing-semester/find_legacy_name",
      "category": "missing-semester",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 37118,
                "output_tokens": 830,
                "total_tokens": 37948
              },
              "turn_count": 3
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 74314,
                "output_tokens": 10066,
                "total_tokens": 84380
              },
              "turn_count": 6
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 54913,
                "output_tokens": 1915,
                "total_tokens": 56828
              },
              "turn_count": 4
            },
            "run-4": {
              "success": false,
              "error_message": "Invalid JSON input for tool get_file_contents: {\"owner\":\"mcpmark-eval\",\"path\":\"_config.yml\",\"repo\":\"missing-semester\"}{\"owner\":\"mcpmark-eval\",\"path\":\"CNAME\",\"repo\":\"missing-semester\"}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4110,
                "output_tokens": 183,
                "total_tokens": 4293
              },
              "turn_count": 1
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 202037 tokens > 200000 maximum (tid: 2025082010220983770349767918446)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2012748,
                "output_tokens": 1492,
                "total_tokens": 2014240
              },
              "turn_count": 12
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 204340 tokens > 200000 maximum (tid: 2025082014283192489947462305076)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 190461,
                "output_tokens": 294,
                "total_tokens": 190755
              },
              "turn_count": 2
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2506061,
                "output_tokens": 2255,
                "total_tokens": 2508316
              },
              "turn_count": 16
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 204332 tokens > 200000 maximum (tid: 2025082111110164045357102331141)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 190463,
                "output_tokens": 288,
                "total_tokens": 190751
              },
              "turn_count": 2
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1097037,
                "output_tokens": 6958,
                "total_tokens": 1103995
              },
              "turn_count": 15
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1498290,
                "output_tokens": 5189,
                "total_tokens": 1503479
              },
              "turn_count": 12
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Input tokens exceed the configured limit of 272000 tokens. Your messages resulted in 289556 tokens. Please reduce the length of the messages. (tid: 2025082023493686002407993060317)', 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1573881,
                "output_tokens": 7383,
                "total_tokens": 1581264
              },
              "turn_count": 14
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1294957,
                "output_tokens": 5519,
                "total_tokens": 1300476
              },
              "turn_count": 8
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082012351112641434742371309)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 29156,
                "output_tokens": 150,
                "total_tokens": 29306
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082017470149992240955503763)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 698483,
                "output_tokens": 513,
                "total_tokens": 698996
              },
              "turn_count": 8
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082200263868507777856411330)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 141266,
                "output_tokens": 167,
                "total_tokens": 141433
              },
              "turn_count": 2
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082206034154490535952303802)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 662155,
                "output_tokens": 532,
                "total_tokens": 662687
              },
              "turn_count": 6
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1396980,
                "output_tokens": 718,
                "total_tokens": 1397698
              },
              "turn_count": 11
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2968158,
                "output_tokens": 1435,
                "total_tokens": 2969593
              },
              "turn_count": 17
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1783022,
                "output_tokens": 1170,
                "total_tokens": 1784192
              },
              "turn_count": 12
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 5398338,
                "output_tokens": 1986,
                "total_tokens": 5400324
              },
              "turn_count": 23
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 198607 tokens (198607 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082011171382590776579428786)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 88294,
                "output_tokens": 182,
                "total_tokens": 88476
              },
              "turn_count": 3
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 147460 tokens (147460 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082015384691445910234724620)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 248857,
                "output_tokens": 360,
                "total_tokens": 249217
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 152075 tokens (152075 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082103072436369392970480357)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 14212,
                "output_tokens": 67,
                "total_tokens": 14279
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 152180 tokens (152180 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082116290733514483409228615)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 14212,
                "output_tokens": 72,
                "total_tokens": 14284
              },
              "turn_count": 1
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1875401,
                "output_tokens": 3440,
                "total_tokens": 1878841
              },
              "turn_count": 14
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1739114,
                "output_tokens": 3713,
                "total_tokens": 1742827
              },
              "turn_count": 15
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1512453,
                "output_tokens": 2777,
                "total_tokens": 1515230
              },
              "turn_count": 12
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 139280,
                "output_tokens": 935,
                "total_tokens": 140215
              },
              "turn_count": 2
            }
          }
        }
      }
    },
    "fix_conflict": {
      "task_name": "harmony/fix_conflict",
      "category": "harmony",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 12273,
                "output_tokens": 3547,
                "total_tokens": 15820
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 44863,
                "output_tokens": 22179,
                "total_tokens": 67042
              },
              "turn_count": 6
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 20269,
                "output_tokens": 2977,
                "total_tokens": 23246
              },
              "turn_count": 3
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 12278,
                "output_tokens": 3485,
                "total_tokens": 15763
              },
              "turn_count": 2
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1893841,
                "output_tokens": 8624,
                "total_tokens": 1902465
              },
              "turn_count": 24
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1700333,
                "output_tokens": 6550,
                "total_tokens": 1706883
              },
              "turn_count": 21
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2075631,
                "output_tokens": 9115,
                "total_tokens": 2084746
              },
              "turn_count": 26
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2230652,
                "output_tokens": 8780,
                "total_tokens": 2239432
              },
              "turn_count": 28
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1102088,
                "output_tokens": 11055,
                "total_tokens": 1113143
              },
              "turn_count": 17
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1114145,
                "output_tokens": 9878,
                "total_tokens": 1124023
              },
              "turn_count": 17
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1410317,
                "output_tokens": 16042,
                "total_tokens": 1426359
              },
              "turn_count": 21
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 949377,
                "output_tokens": 12661,
                "total_tokens": 962038
              },
              "turn_count": 15
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1056656,
                "output_tokens": 2631,
                "total_tokens": 1059287
              },
              "turn_count": 19
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1446202,
                "output_tokens": 3132,
                "total_tokens": 1449334
              },
              "turn_count": 25
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082200004520291855866503547)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1684367,
                "output_tokens": 3276,
                "total_tokens": 1687643
              },
              "turn_count": 25
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 835049,
                "output_tokens": 2202,
                "total_tokens": 837251
              },
              "turn_count": 16
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1653121,
                "output_tokens": 3956,
                "total_tokens": 1657077
              },
              "turn_count": 24
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1089070,
                "output_tokens": 1091,
                "total_tokens": 1090161
              },
              "turn_count": 17
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1748512,
                "output_tokens": 2305,
                "total_tokens": 1750817
              },
              "turn_count": 25
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1636520,
                "output_tokens": 4551,
                "total_tokens": 1641071
              },
              "turn_count": 25
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1298589,
                "output_tokens": 3883,
                "total_tokens": 1302472
              },
              "turn_count": 20
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1151176,
                "output_tokens": 2365,
                "total_tokens": 1153541
              },
              "turn_count": 15
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1582371,
                "output_tokens": 4165,
                "total_tokens": 1586536
              },
              "turn_count": 24
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1375486,
                "output_tokens": 3426,
                "total_tokens": 1378912
              },
              "turn_count": 20
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 962424,
                "output_tokens": 5150,
                "total_tokens": 967574
              },
              "turn_count": 19
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 686522,
                "output_tokens": 4114,
                "total_tokens": 690636
              },
              "turn_count": 15
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 786319,
                "output_tokens": 3952,
                "total_tokens": 790271
              },
              "turn_count": 16
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 851498,
                "output_tokens": 3727,
                "total_tokens": 855225
              },
              "turn_count": 17
            }
          }
        }
      }
    },
    "deployment_status_workflow": {
      "task_name": "mcpmark-cicd/deployment_status_workflow",
      "category": "mcpmark-cicd",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 62766,
                "output_tokens": 6368,
                "total_tokens": 69134
              },
              "turn_count": 5
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 66739,
                "output_tokens": 6774,
                "total_tokens": 73513
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 63661,
                "output_tokens": 6519,
                "total_tokens": 70180
              },
              "turn_count": 5
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 63874,
                "output_tokens": 11182,
                "total_tokens": 75056
              },
              "turn_count": 5
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 109705,
                "output_tokens": 5627,
                "total_tokens": 115332
              },
              "turn_count": 5
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 112320,
                "output_tokens": 6324,
                "total_tokens": 118644
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 15653,
                "output_tokens": 162,
                "total_tokens": 15815
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 15653,
                "output_tokens": 163,
                "total_tokens": 15816
              },
              "turn_count": 1
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 98557,
                "output_tokens": 19948,
                "total_tokens": 118505
              },
              "turn_count": 6
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 72570,
                "output_tokens": 16225,
                "total_tokens": 88795
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 296123,
                "output_tokens": 30782,
                "total_tokens": 326905
              },
              "turn_count": 12
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 120993,
                "output_tokens": 26172,
                "total_tokens": 147165
              },
              "turn_count": 7
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Github MCP service error",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 12343,
                "output_tokens": 65,
                "total_tokens": 12408
              },
              "turn_count": 1
            },
            "run-2": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 51712,
                "output_tokens": 192,
                "total_tokens": 51904
              },
              "turn_count": 3
            },
            "run-3": {
              "success": false,
              "error_message": "Invalid JSON input for tool create_or_update_file: {\"owner\": \"mcpmark-eval\", \"repo\": \"mcpmark-cicd\", \"path\": \".github/workflows/deployment-status.yml\", \"content\": \"name: Deployment Status Workflow\\n\\non:\\n  push:\\n    branches:\\n      - main\\n\\nenv:\\n  NODE_VERSION: '18'\\n\\njobs:\\n  pre-deployment:\\n    name: pre-deployment\\n    runs-on: ubuntu-latest\\n    outputs:\\n      issue-number: ${{ steps.create-issue.outputs.issue-number }}\\n      previous-sha: ${{ steps.get-info.outputs.previous-sha }}\\n      current-sha: ${{ steps.get-info.outputs.current-sha }}\\n      package-version: ${{ steps.get-info.outputs.package-version }}\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v4\\n        with:\\n          fetch-depth: 2\\n\\n      - name: Setup Node.js\\n        uses: actions/setup-node@v4\\n        with:\\n          node-version: ${{ env.NODE_VERSION }}\\n          cache: 'npm'\\n\\n      - name: Install dependencies\\n        run: npm ci\\n\\n      - name: Run lint\\n        run: npm run lint --if-present\\n\\n      - name: Run tests\\n        run: npm test --if-present\\n\\n      - name: Get commit and package info\\n        id: get-info\\n        run: |\\n          PREVIOUS_SHA=$(git rev-parse HEAD^)\\n          CURRENT_SHA=$(git rev-parse HEAD)\\n          PACKAGE_VERSION=$(node -p \\\"require('./package.json').version\\\" || echo \\\"1.0.0\\\")\\n          \\n          echo \\\"previous-sha=$PREVIOUS_SHA\\\" >> $GITHUB_OUTPUT\\n          echo \\\"current-sha=$CURRENT_SHA\\\" >> $GITHUB_OUTPUT\\n          echo \\\"package-version=$PACKAGE_VERSION\\\" >> $GITHUB_OUTPUT\\n          \\n          echo \\\"Previous SHA: $PREVIOUS_SHA\\\"\\n          echo \\\"Current SHA: $CURRENT_SHA\\\"\\n          echo \\\"Package Version: $PACKAGE_VERSION\\\"\\n\\n      - name: Create deployment tracking issue\\n        id: create-issue\\n        uses: actions/github-script@v7\\n        with:\\n          script: |\\n            const shortSha = '${{ steps.get-info.outputs.current-sha }}'.substring(0, 7);\\n            const issue = await github.rest.issues.create({\\n              owner: context.repo.owner,\\n              repo: context.repo.repo,\\n              title: `Deployment Tracking - ${shortSha}`,\\n              body: `## Deployment Tracking\\n              \\n              **Commit:** ${{ steps.get-info.outputs.current-sha }}\\n              **Previous Commit:** ${{ steps.get-info.outputs.previous-sha }}\\n              **Package Version:** ${{ steps.get-info.outputs.package-version }}\\n              \\n              This issue tracks the deployment process for commit ${{ steps.get-info.outputs.current-sha }}.\\n              \\n              ### Status\\n              - [x] Pre-deployment checks initiated\\n              - [ ] Rollback preparation\\n              - [ ] Deployment completion`,\\n              labels: ['deployment', 'in-progress']\\n            });\\n            \\n            core.setOutput('issue-number', issue.data.number);\\n            return issue.data.number;\\n\\n      - name: Post pre-deployment comment\\n        uses: actions/github-script@v7\\n        with:\\n          script: |\\n            const issueNumber = ${{ steps.create-issue.outputs.issue-number }};\\n            \\n            await github.rest.issues.createComment({\\n              owner: context.repo.owner,\\n              repo: context.repo.repo,\\n              issue_number: issueNumber,\\n              body: '##  Pre-deployment checks completed\\\\n\\\\nAll basic quality checks have passed successfully. Ready to proceed with rollback preparation.'\\n            });\\n\\n  rollback-preparation:\\n    name: rollback-preparation\\n    runs-on: ubuntu-latest\\n    needs: pre-deployment\\n    outputs:\\n      artifact-name: ${{ steps.create-artifacts.outputs.artifact-name }}\\n      checksum: ${{ steps.create-artifacts.outputs.checksum }}\\n    steps:\\n      - name: Checkout repository\\n        uses: actions/checkout@v4\\n        with:\\n          fetch-depth: 2\\n\\n      - name: Setup Node.js\\n        uses: actions/setup-node@v4\\n        with:\\n          node-version: ${{ env.NODE_VERSION }}\\n\\n      - name: Create rollback artifacts\\n        id: create-artifacts\\n        run: |\\n          # Create rollback directory structure\\n          mkdir -p rollback-artifacts/{scripts,backups,docs}\\n          \\n          # Get commit information\\n          PREVIOUS_SHA=\\\"${{ needs.pre-deployment.outputs.previous-sha }}\\\"\\n          CURRENT_SHA=\\\"${{ needs.pre-deployment.outputs.current-sha }}\\\"\\n          PACKAGE_VERSION=\\\"${{ needs.pre-deployment.outputs",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 12343,
                "output_tokens": 65,
                "total_tokens": 12408
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 106916,
                "output_tokens": 5542,
                "total_tokens": 112458
              },
              "turn_count": 5
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 213806,
                "output_tokens": 10536,
                "total_tokens": 224342
              },
              "turn_count": 8
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 218898,
                "output_tokens": 3562,
                "total_tokens": 222460
              },
              "turn_count": 9
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 156523,
                "output_tokens": 4190,
                "total_tokens": 160713
              },
              "turn_count": 6
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 167836,
                "output_tokens": 4057,
                "total_tokens": 171893
              },
              "turn_count": 7
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 201055,
                "output_tokens": 5330,
                "total_tokens": 206385
              },
              "turn_count": 10
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 102064,
                "output_tokens": 4785,
                "total_tokens": 106849
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 100909,
                "output_tokens": 4386,
                "total_tokens": 105295
              },
              "turn_count": 5
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 103556,
                "output_tokens": 5308,
                "total_tokens": 108864
              },
              "turn_count": 5
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 65281,
                "output_tokens": 4341,
                "total_tokens": 69622
              },
              "turn_count": 5
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 67234,
                "output_tokens": 5350,
                "total_tokens": 72584
              },
              "turn_count": 5
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 65785,
                "output_tokens": 4702,
                "total_tokens": 70487
              },
              "turn_count": 5
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 65029,
                "output_tokens": 5892,
                "total_tokens": 70921
              },
              "turn_count": 5
            }
          }
        }
      }
    },
    "critical_issue_hotfix_workflow": {
      "task_name": "claude-code/critical_issue_hotfix_workflow",
      "category": "claude-code",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 261845,
                "output_tokens": 8271,
                "total_tokens": 270116
              },
              "turn_count": 11
            },
            "run-2": {
              "success": false,
              "error_message": "Invalid JSON input for tool get_pull_request: {\"body\":\"## Critical Issues\\n- Issue #49\\n- Issue #46\\n\\n## Impact Assessment\\nThe identified memory exhaustion and context management failures are causing significant user impact, including application crashes and unresponsiveness, particularly on macOS platforms. These issues make the tool unreliable for large-scale operations.\\n\\n## Resolution Strategy\\nThe planned approach involves implementing a hotfix (v1.0.72) that introduces progressive context cleanup, streaming data processing for large operations, and optimized garbage collection. This will be tracked in a dedicated hotfix branch and merged into main upon successful validation.\\n\\n### Related Issues\\n- #49\\n- #46\\n- #47\\n\\n### Keywords\\nmemory exhaustion, context auto-compact, JavaScript heap, hotfix priority\",\"owner\":\"mcpmark-eval\",\"repo\":\"claude-code\",\"title\":\"CRITICAL: Memory and Context Management Issues - Hotfix Tracking\"}{\"branch\":\"hotfix/memory-optimization-v1.0.72\",\"from_branch\":\"main\",\"owner\":\"mcpmark-eval\",\"repo\":\"claude-code\"}{\"owner\":\"mcpmark-eval\",\"pullNumber\":51,\"repo\":\"claude-code\"}",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": 0
            },
            "run-3": {
              "success": false,
              "error_message": "Invalid JSON input for tool get_pull_request: {\"branch\":\"hotfix/memory-optimization-v1.0.72\",\"from_branch\":\"main\",\"owner\":\"mcpmark-eval\",\"repo\":\"claude-code\"}{\"owner\":\"mcpmark-eval\",\"pullNumber\":51,\"repo\":\"claude-code\"}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 5035,
                "output_tokens": 2170,
                "total_tokens": 7205
              },
              "turn_count": 1
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 0,
                "output_tokens": 0,
                "total_tokens": 0
              },
              "turn_count": 1
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 296041,
                "output_tokens": 4560,
                "total_tokens": 300601
              },
              "turn_count": 10
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 335900,
                "output_tokens": 3865,
                "total_tokens": 339765
              },
              "turn_count": 11
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 337697,
                "output_tokens": 4124,
                "total_tokens": 341821
              },
              "turn_count": 11
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 294818,
                "output_tokens": 4296,
                "total_tokens": 299114
              },
              "turn_count": 10
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 228701,
                "output_tokens": 9027,
                "total_tokens": 237728
              },
              "turn_count": 11
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 224377,
                "output_tokens": 9797,
                "total_tokens": 234174
              },
              "turn_count": 11
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 176319,
                "output_tokens": 6794,
                "total_tokens": 183113
              },
              "turn_count": 10
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 223089,
                "output_tokens": 7238,
                "total_tokens": 230327
              },
              "turn_count": 11
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": 0
            },
            "run-2": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": 0
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 249537,
                "output_tokens": 2647,
                "total_tokens": 252184
              },
              "turn_count": 11
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 251261,
                "output_tokens": 2652,
                "total_tokens": 253913
              },
              "turn_count": 11
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 552831,
                "output_tokens": 3748,
                "total_tokens": 556579
              },
              "turn_count": 16
            },
            "run-2": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 690420,
                "output_tokens": 4006,
                "total_tokens": 694426
              },
              "turn_count": 20
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 301381,
                "output_tokens": 2271,
                "total_tokens": 303652
              },
              "turn_count": 10
            },
            "run-4": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 429813,
                "output_tokens": 3162,
                "total_tokens": 432975
              },
              "turn_count": 13
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 744516,
                "output_tokens": 3230,
                "total_tokens": 747746
              },
              "turn_count": 17
            },
            "run-2": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {},
              "turn_count": 0
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 264150,
                "output_tokens": 2592,
                "total_tokens": 266742
              },
              "turn_count": 10
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 267645,
                "output_tokens": 3158,
                "total_tokens": 270803
              },
              "turn_count": 10
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 148900,
                "output_tokens": 2826,
                "total_tokens": 151726
              },
              "turn_count": 9
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 150303,
                "output_tokens": 2460,
                "total_tokens": 152763
              },
              "turn_count": 9
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 149513,
                "output_tokens": 2643,
                "total_tokens": 152156
              },
              "turn_count": 9
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 149382,
                "output_tokens": 2299,
                "total_tokens": 151681
              },
              "turn_count": 9
            }
          }
        }
      }
    },
    "config_parameter_audit": {
      "task_name": "easyr1/config_parameter_audit",
      "category": "easyr1",
      "service": "github",
      "models": {
        "gemini-2-5-pro": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 13402,
                "output_tokens": 3931,
                "total_tokens": 17333
              },
              "turn_count": 2
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4631,
                "output_tokens": 1133,
                "total_tokens": 5764
              },
              "turn_count": 1
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 13962,
                "output_tokens": 4762,
                "total_tokens": 18724
              },
              "turn_count": 2
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 13398,
                "output_tokens": 10263,
                "total_tokens": 23661
              },
              "turn_count": 2
            }
          }
        },
        "claude-4-sonnet": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 210361 tokens > 200000 maximum (tid: 202508200958364415508727564138)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 15443,
                "output_tokens": 141,
                "total_tokens": 15584
              },
              "turn_count": 1
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 202376 tokens > 200000 maximum (tid: 2025082014154246977579610901934)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1274252,
                "output_tokens": 1522,
                "total_tokens": 1275774
              },
              "turn_count": 10
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 201641 tokens > 200000 maximum (tid: 2025082100305264049589929500382)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 2354837,
                "output_tokens": 3358,
                "total_tokens": 2358195
              },
              "turn_count": 17
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'prompt is too long: 209409 tokens > 200000 maximum (tid: 2025082110265752917821156287538)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 662469,
                "output_tokens": 898,
                "total_tokens": 663367
              },
              "turn_count": 6
            }
          }
        },
        "gpt-5": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 3906187,
                "output_tokens": 16928,
                "total_tokens": 3923115
              },
              "turn_count": 20
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 3086418,
                "output_tokens": 13254,
                "total_tokens": 3099672
              },
              "turn_count": 15
            },
            "run-3": {
              "success": true,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4153014,
                "output_tokens": 18104,
                "total_tokens": 4171118
              },
              "turn_count": 21
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 3040138,
                "output_tokens": 9741,
                "total_tokens": 3049879
              },
              "turn_count": 14
            }
          }
        },
        "k2": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082011595250913802879895424)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 914650,
                "output_tokens": 1140,
                "total_tokens": 915790
              },
              "turn_count": 10
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082016572552567409139530634)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1447889,
                "output_tokens": 1005,
                "total_tokens": 1448894
              },
              "turn_count": 14
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082123473946867681520283774)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1058213,
                "output_tokens": 804,
                "total_tokens": 1059017
              },
              "turn_count": 11
            },
            "run-4": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Invalid request: Your request exceeded model token limit: 131072 (tid: 2025082205131394679586559655464)', 'type': 'invalid_request_error', 'param': '', 'code': None}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 3038500,
                "output_tokens": 2914,
                "total_tokens": 3041414
              },
              "turn_count": 29
            }
          }
        },
        "qwen-3-coder": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1470902,
                "output_tokens": 628,
                "total_tokens": 1471530
              },
              "turn_count": 7
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 404 - {'error': {'message': 'No endpoints found that support tool use. To learn more about provider routing, visit: https://openrouter.ai/docs/provider-routing', 'code': 404}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 3578740,
                "output_tokens": 1733,
                "total_tokens": 3580473
              },
              "turn_count": 16
            },
            "run-3": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 4947528,
                "output_tokens": 2322,
                "total_tokens": 4949850
              },
              "turn_count": 17
            },
            "run-4": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 254841,
                "output_tokens": 294,
                "total_tokens": 255135
              },
              "turn_count": 3
            }
          }
        },
        "deepseek-chat": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 141057 tokens (141057 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082011031847217132869987402)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 139356,
                "output_tokens": 220,
                "total_tokens": 139576
              },
              "turn_count": 3
            },
            "run-2": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 202584 tokens (202584 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082014552226405209130008672)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 62659,
                "output_tokens": 241,
                "total_tokens": 62900
              },
              "turn_count": 4
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 131072 tokens. However, you requested 138583 tokens (138583 in the messages, 0 in the completion). Please reduce the length of the messages or completion. (tid: 2025082102142921095385951963385)\", 'type': 'invalid_request_error', 'param': '', 'code': 'invalid_request_error'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 768102,
                "output_tokens": 821,
                "total_tokens": 768923
              },
              "turn_count": 9
            },
            "run-4": {
              "success": false,
              "error_message": "Github Resource not found",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 30546,
                "output_tokens": 133,
                "total_tokens": 30679
              },
              "turn_count": 2
            }
          }
        },
        "o3": {
          "runs": {
            "run-1": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 370924,
                "output_tokens": 708,
                "total_tokens": 371632
              },
              "turn_count": 5
            },
            "run-2": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1925803,
                "output_tokens": 9678,
                "total_tokens": 1935481
              },
              "turn_count": 21
            },
            "run-3": {
              "success": false,
              "error_message": "Error code: 400 - {'error': {'message': 'Your organization must be verified to stream this model. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate. (tid: 2025082021062313171384763760270)', 'type': 'invalid_request_error', 'param': 'stream', 'code': 'unsupported_value'}}",
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 1471901,
                "output_tokens": 3004,
                "total_tokens": 1474905
              },
              "turn_count": 9
            },
            "run-4": {
              "success": false,
              "error_message": null,
              "execution_time": 0,
              "token_usage": {
                "input_tokens": 446315,
                "output_tokens": 1956,
                "total_tokens": 448271
              },
              "turn_count": 9
            }
          }
        }
      }
    }
  }
}