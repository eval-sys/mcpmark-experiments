{
  "task_id": "ai_data_analyst",
  "task_name": "AI Data Analyst",
  "category_id": "reddit",
  "category_name": "Reddit",
  "description": "Create account on forum platform, collect AI/ML discussion metrics including post counts, vote data, and analyze community engagement patterns through systematic data extraction.",
  "author": "Fanqing Meng",
  "created_at": "2025-08-12",
  "difficulty": "L3",
  "tags": [
    "user interaction",
    "data extraction",
    "comparative analysis"
  ],
  "mcp": [
    "playwright"
  ],
  "metadata": {},
  "instruction": "I'm conducting an AI research analysis and need specific data from this forum platform. Please help me gather information about AI discussions and community engagement.\n\n**Task Requirements:**\n\n1. Create a new account with the exact 'AIDataAnalyst2025' and password 'SecurePass123!'\n\n2. After logging in, navigate to the deeplearning forum and count the exact number of posts visible on the first page\n\n3. Go to the MachineLearning forum and find the post titled '[P] I made a command-line tool that explains your errors using ChatGPT (link in comments)' - record its exact vote count\n\n4. Use the search bar to search for 'transformer' and record the exact title of the third search result\n\n5. In the MachineLearning forum, click on the 'Sort by: Hot' button and change it to 'New'. Navigate to page 2 of the results and find the post with the highest upvote count on that page:\n   - Record the exact post title\n   - Record the exact vote count\n   - Click through to the post and find the last comment\n   - Record the username of the last commenter\n   - Record the exact text of the last comment\n\n6. After collecting all the data, go to the MachineLearning forum and submit a new post with:\n   - Title: \"MachineLearning_Extraction\"\n   - Body text must be EXACTLY these lines without anything (keep the keys as-is, only replace the values after the pipe, follow the markdown format):\n\n```\n- Deeplearning_Post_Count|FILL_VALUE\n- ChatGPT_Tool_Vote_Count|FILL_VALUE\n- Transformer_Third_Result|FILL_VALUE\n- Page2_Top_Post_Title|FILL_VALUE\n- Page2_Top_Post_Votes|FILL_VALUE\n- Page2_Last_Comment_Username|FILL_VALUE\n- Page2_Last_Comment_Text|FILL_VALUE\n```",
  "verify": "import asyncio\nimport sys\nimport re\nimport os\nfrom pathlib import Path\nfrom playwright.async_api import (\n    async_playwright,\n    TimeoutError as PlaywrightTimeoutError,\n)\n\n# 从环境变量读取 base_url，默认回退到本地\nBASE_URL = os.getenv(\"WEBARENA_BASE_URL\", \"http://localhost:9999\").rstrip(\"/\")\n\n\ndef parse_key_value_format(text):\n    \"\"\"\n    Parse the Key|Value format from the submission body using regex.\n    Works with markdown format using pipe separators, with or without list markers.\n    \"\"\"\n    data = {}\n\n    # Define patterns for each field using pipe separator\n    # Optional list markers (-, •, *) at the beginning\n    patterns = {\n        \"Deeplearning_Post_Count\": r\"(?:[-•*]\\s*)?Deeplearning_Post_Count\\s*\\|\\s*(\\d+)\",\n        \"ChatGPT_Tool_Vote_Count\": r\"(?:[-•*]\\s*)?ChatGPT_Tool_Vote_Count\\s*\\|\\s*(\\d+)\",\n        \"Transformer_Third_Result\": r\"(?:[-•*]\\s*)?Transformer_Third_Result\\s*\\|\\s*(.+?)(?=\\n|$)\",\n        \"Page2_Top_Post_Title\": r\"(?:[-•*]\\s*)?Page2_Top_Post_Title\\s*\\|\\s*(.+?)(?=\\n|$)\",\n        \"Page2_Top_Post_Votes\": r\"(?:[-•*]\\s*)?Page2_Top_Post_Votes\\s*\\|\\s*(\\d+)\",\n        \"Page2_Last_Comment_Username\": r\"(?:[-•*]\\s*)?Page2_Last_Comment_Username\\s*\\|\\s*(.+?)(?=\\n|$)\",\n        \"Page2_Last_Comment_Text\": r\"(?:[-•*]\\s*)?Page2_Last_Comment_Text\\s*\\|\\s*(.+?)(?=\\n|$)\",\n    }\n\n    # Extract each field using regex\n    for key, pattern in patterns.items():\n        match = re.search(pattern, text, re.MULTILINE)\n        if match:\n            # For text fields, clean up newlines and extra spaces\n            value = match.group(1).strip()\n            if key not in [\n                \"Deeplearning_Post_Count\",\n                \"ChatGPT_Tool_Vote_Count\",\n                \"Page2_Top_Post_Votes\",\n            ]:\n                # Replace newlines with spaces and normalize whitespace\n                value = \" \".join(value.split())\n            data[key] = value\n\n    return data\n\n\ndef normalize_text(text):\n    \"\"\"\n    Normalize text for comparison by handling different quote styles and whitespace.\n    \"\"\"\n    if not isinstance(text, str):\n        return str(text)\n\n    # Replace various quote styles with standard quotes\n    text = text.replace(\"\"\", \"'\").replace(\"\"\", \"'\")\n    text = text.replace('\"', '\"').replace('\"', '\"')\n\n    # Normalize whitespace\n    text = \" \".join(text.split())\n\n    return text.strip()\n\n\nasync def verify() -> bool:\n    \"\"\"\n    Verifies that the ML extraction task has been completed correctly by checking the forum post.\n    \"\"\"\n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=True)\n        context = await browser.new_context()\n        page = await context.new_page()\n\n        try:\n            # Navigate to the main page\n            print(\"Navigating to forum...\", file=sys.stderr)\n            await page.goto(f\"{BASE_URL}/\", wait_until=\"networkidle\")\n\n            # Step 1: Check if account was created correctly by trying to login\n            print(\"Step 1: Verifying account creation...\", file=sys.stderr)\n            user_button = page.locator('button:has-text(\"AIDataAnalyst2025\")')\n            if not await user_button.count():\n                # Try to login\n                print(\"Attempting to login with AIDataAnalyst2025...\", file=sys.stderr)\n\n                # Click login link\n                await page.click('a:has-text(\"Log in\")')\n                await page.wait_for_load_state(\"networkidle\")\n\n                # Fill login form\n                await page.fill('input[name=\"_username\"]', \"AIDataAnalyst2025\")\n                await page.fill('input[name=\"_password\"]', \"SecurePass123!\")\n\n                # Submit login form\n                await page.click('button:has-text(\"Log in\")')\n                await page.wait_for_load_state(\"networkidle\")\n\n                # Check if login successful\n                user_button = page.locator('button:has-text(\"AIDataAnalyst2025\")')\n                if not await user_button.count():\n                    print(\"FAILED: Account AIDataAnalyst2025 with password SecurePass123! cannot be logged in\", file=sys.stderr)\n                    print(\"This means the account was not created correctly\", file=sys.stderr)\n                    return False\n\n                print(\"PASSED: Successfully logged in as AIDataAnalyst2025\", file=sys.stderr)\n            else:\n                print(\"PASSED: Already logged in as AIDataAnalyst2025\", file=sys.stderr)\n\n            # Step 2: Check if submission was created correctly in MachineLearning forum\n            print(\"\\nStep 2: Verifying submission creation...\", file=sys.stderr)\n            print(\"Navigating to MachineLearning forum...\", file=sys.stderr)\n            await page.goto(\n                f\"{BASE_URL}/f/MachineLearning\", wait_until=\"networkidle\"\n            )\n\n            # Look for the post with title \"MachineLearning_Extraction\"\n            print(\n                \"Looking for submission with title 'MachineLearning_Extraction'...\",\n                file=sys.stderr,\n            )\n            post_link = page.locator('a:has-text(\"MachineLearning_Extraction\")')\n\n            if not await post_link.count():\n                print(\n                    \"FAILED: Could not find submission with title 'MachineLearning_Extraction' in MachineLearning forum\",\n                    file=sys.stderr,\n                )\n                return False\n            \n            print(\"PASSED: Found submission 'MachineLearning_Extraction' in MachineLearning forum\", file=sys.stderr)\n\n            # Step 3: Check submission content matches expected values\n            print(\"\\nStep 3: Verifying submission content...\", file=sys.stderr)\n            \n            # Click on the submission to view its content\n            await post_link.first.click()\n            await page.wait_for_load_state(\"networkidle\")\n\n            # Extract the submission body content\n            # Try multiple possible selectors for the post body\n            post_content = None\n            selectors = [\n                \".submission__body\",\n                \".post-body\",\n                \".RichText\",\n                '[class*=\"RichText\"]',\n                'div:has(> p:has-text(\"Deeplearning_Post_Count\"))',\n                'div:has-text(\"Deeplearning_Post_Count\"):has-text(\"Page2_Last_Comment_Text\")',\n            ]\n\n            for selector in selectors:\n                content_element = page.locator(selector)\n                if await content_element.count():\n                    post_content = await content_element.first.inner_text()\n                    if \"Deeplearning_Post_Count\" in post_content:\n                        print(\n                            f\"Found submission content using selector: {selector}\",\n                            file=sys.stderr,\n                        )\n                        break\n\n            if not post_content or \"Deeplearning_Post_Count\" not in post_content:\n                print(\n                    \"FAILED: Could not find submission body with required format\",\n                    file=sys.stderr,\n                )\n                print(\n                    \"Expected body to contain 'Deeplearning_Post_Count' in pipe-separated format\",\n                    file=sys.stderr,\n                )\n                return False\n\n            print(\"Found submission body content\", file=sys.stderr)\n            print(f\"Raw content preview: {post_content[:200]}...\", file=sys.stderr)\n\n            # Parse the Key: Value format\n            extracted_data = parse_key_value_format(post_content)\n            print(f\"Extracted data: {extracted_data}\", file=sys.stderr)\n\n            # Load expected values from label.txt\n            label_path = Path(__file__).parent / \"label.txt\"\n            if label_path.exists():\n                with open(label_path, \"r\") as f:\n                    expected_text = f.read().strip()\n                expected_data = parse_key_value_format(expected_text)\n                print(\"Loaded expected values from label.txt\", file=sys.stderr)\n\n            # Verify all required keys are present\n            required_keys = [\n                \"Deeplearning_Post_Count\",\n                \"ChatGPT_Tool_Vote_Count\",\n                \"Transformer_Third_Result\",\n                \"Page2_Top_Post_Title\",\n                \"Page2_Top_Post_Votes\",\n                \"Page2_Last_Comment_Username\",\n                \"Page2_Last_Comment_Text\",\n            ]\n\n            missing_keys = []\n            for key in required_keys:\n                if key not in extracted_data:\n                    missing_keys.append(key)\n\n            if missing_keys:\n                print(\n                    \"FAILED: Missing required keys in submission: {', '.join(missing_keys)}\",\n                    file=sys.stderr,\n                )\n                print(\n                    \"Expected all 7 fields to be present in pipe-separated format\",\n                    file=sys.stderr,\n                )\n                return False\n\n            # Validate data format and content\n            errors = []\n\n            # Check numeric fields\n            try:\n                post_count = int(extracted_data[\"Deeplearning_Post_Count\"])\n                if (\n                    \"expected_data\" in locals()\n                    and \"Deeplearning_Post_Count\" in expected_data\n                ):\n                    expected_count = int(expected_data[\"Deeplearning_Post_Count\"])\n                    if post_count != expected_count:\n                        errors.append(\n                            f\"Deeplearning_Post_Count mismatch: got {post_count}, expected {expected_count}\"\n                        )\n            except ValueError:\n                errors.append(\n                    f\"Deeplearning_Post_Count must be a number, got: {extracted_data['Deeplearning_Post_Count']}\"\n                )\n\n            # If we have expected data, compare against it\n            if \"expected_data\" in locals():\n                # Compare each field\n                for key in required_keys:\n                    if key in expected_data and key in extracted_data:\n                        expected_val = normalize_text(expected_data[key])\n                        actual_val = normalize_text(extracted_data[key])\n\n                        # For numeric fields, compare as integers\n                        if key in [\n                            \"Deeplearning_Post_Count\",\n                            \"ChatGPT_Tool_Vote_Count\",\n                            \"Page2_Top_Post_Votes\",\n                        ]:\n                            try:\n                                expected_int = int(expected_val)\n                                actual_int = int(actual_val)\n                                if expected_int != actual_int:\n                                    errors.append(\n                                        f\"{key} mismatch: got {actual_int}, expected {expected_int}\"\n                                    )\n                            except ValueError:\n                                errors.append(\n                                    f\"{key} should be numeric: got '{actual_val}'\"\n                                )\n                        else:\n                            # For text fields, compare normalized text\n                            if expected_val != actual_val:\n                                errors.append(\n                                    f\"{key} mismatch: got '{actual_val}', expected '{expected_val}'\"\n                                )\n\n            else:\n                # If no expected data, just do basic validation\n                for key in required_keys:\n                    if key not in extracted_data:\n                        errors.append(f\"Missing required key: {key}\")\n                    elif (\n                        not extracted_data[key] or extracted_data[key] == \"[FILL_VALUE]\"\n                    ):\n                        errors.append(f\"{key} was not filled in\")\n\n            if errors:\n                print(\n                    \"FAILED: Content validation failed with the following issues:\",\n                    file=sys.stderr,\n                )\n                for error in errors:\n                    print(f\"  - {error}\", file=sys.stderr)\n                print(\"\\nExpected values from label.txt:\", file=sys.stderr)\n                if \"expected_data\" in locals():\n                    for key in required_keys:\n                        if key in expected_data:\n                            print(f\"  {key}: {expected_data[key]}\", file=sys.stderr)\n                return False\n\n            # All checks passed\n            print(\"\\n=== VERIFICATION SUCCESSFUL ===\")\n            print(\"✓ Step 1: Account AIDataAnalyst2025 can login with password SecurePass123!\")\n            print(\"✓ Step 2: Submission 'MachineLearning_Extraction' found in MachineLearning forum\")\n            print(\"✓ Step 3: All submission content matches expected values:\")\n            print(f\"  - Deeplearning_Post_Count: {extracted_data['Deeplearning_Post_Count']}\")\n            print(f\"  - ChatGPT_Tool_Vote_Count: {extracted_data['ChatGPT_Tool_Vote_Count']}\")\n            print(f\"  - Transformer_Third_Result: {extracted_data['Transformer_Third_Result']}\")\n            print(f\"  - Page2_Top_Post_Title: {extracted_data['Page2_Top_Post_Title']}\")\n            print(f\"  - Page2_Top_Post_Votes: {extracted_data['Page2_Top_Post_Votes']}\")\n            print(f\"  - Page2_Last_Comment_Username: {extracted_data['Page2_Last_Comment_Username']}\")\n            print(f\"  - Page2_Last_Comment_Text: {extracted_data['Page2_Last_Comment_Text']}\")\n            print(\"✓ All data in correct pipe-separated markdown format\")\n            return True\n\n        except PlaywrightTimeoutError as e:\n            print(f\"Error: Timeout occurred - {str(e)}\", file=sys.stderr)\n            return False\n        except Exception as e:\n            print(f\"Error: Unexpected error - {str(e)}\", file=sys.stderr)\n            return False\n        finally:\n            await browser.close()\n\n\ndef main():\n    \"\"\"\n    Executes the verification process and exits with a status code.\n    \"\"\"\n    result = asyncio.run(verify())\n    sys.exit(0 if result else 1)\n\n\nif __name__ == \"__main__\":\n    main()\n"
}