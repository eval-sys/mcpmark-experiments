{
  "task_id": "config_parameter_audit",
  "task_name": "Config Parameter Audit",
  "category_id": "easyr1",
  "category_name": "EasyR1",
  "description": "Investigate configuration changes causing training instability by analyzing commits and identifying related memory issues.",
  "author": "Xiangyan Liu",
  "created_at": "2025-08-15",
  "difficulty": "L3",
  "tags": [
    "repository analysis",
    "issue management"
  ],
  "mcp": [
    "github"
  ],
  "metadata": {},
  "instruction": "I need you to perform a deep investigation into recent configuration changes in our EasyR1 repository that may be causing training instability issues.\n\n## Task Requirements\n\n### 1. Deep Commit Analysis\nFind the exact commit SHA where the `micro_batch_size_per_device_for_update` parameter was changed from `4` to `1` in the `examples/config.yaml` file. Use GitHub API to:\n- Examine recent commits that modified `examples/config.yaml` \n- Get the specific commit diff showing this parameter change\n- Identify the commit author and timestamp\n\n### 2. Related Parameter Investigation  \nIn the same commit you found above, identify what value the `micro_batch_size_per_device_for_experience` parameter was changed to. Document:\n- The before value for this parameter\n- The after value for this parameter  \n- The specific line numbers in the diff where these changes occurred\n\n### 3. Issue Search and Verification\nSearch through all GitHub issues (both open and closed) to find issues that contain specific keywords. Identify all issue numbers where the issue title or body text contains any of these exact terms:\n- \"OOM\" (case insensitive)\n- \"memory\" (case insensitive) \n- \"batch\" (case insensitive)\n- \"显存\" (GPU memory in Chinese)\n\nYou must find and list ALL issues that contain any of these keywords in their titles or bodies, regardless of whether you think they're related to the parameter changes.\n\n### 4. File Creation and Results\nCreate a file named exactly `ANALYSIS_RESULTS.json` in the repository root with this exact structure:\n\n```json\n{\n  \"target_commit_sha\": \"full-40-character-commit-sha\",\n  \"commit_author\": \"author-username\", \n  \"commit_date\": \"YYYY-MM-DD\",\n  \"parameter_changes\": {\n    \"micro_batch_size_per_device_for_update\": {\n      \"before\": 4,\n      \"after\": 1,\n      \"line_number\": 123\n    },\n    \"micro_batch_size_per_device_for_experience\": {\n      \"before\": 16,\n      \"after\": 2, \n      \"line_number\": 124\n    }\n  },\n  \"related_issue_number_list\": [9, 46]\n}\n```\n\n### 5. Verification Requirements\n- The commit SHA must be exactly 40 hexadecimal characters\n- The parameter values must match the actual repository changes  \n- The issue number must reference a real issue in the repository\n- All data must be obtained through GitHub API analysis, not guesswork",
  "verify": "import sys\nimport os\nimport json\nimport requests\nimport re\nfrom typing import Dict, Optional, Tuple\nfrom dotenv import load_dotenv\n\nload_dotenv(\".mcp_env\")\n\n\ndef _get_github_api(\n    endpoint: str, headers: Dict[str, str]\n) -> Tuple[bool, Optional[Dict]]:\n    \"\"\"Make a GET request to GitHub API and return (success, response).\"\"\"\n    github_org = os.environ.get(\"GITHUB_EVAL_ORG\")\n    url = f\"https://api.github.com/repos/{github_org}/EasyR1/{endpoint}\"\n    try:\n        response = requests.get(url, headers=headers)\n        if response.status_code == 200:\n            return True, response.json()\n        elif response.status_code == 404:\n            return False, None\n        else:\n            print(f\"API error for {endpoint}: {response.status_code}\", file=sys.stderr)\n            return False, None\n    except Exception as e:\n        print(f\"Exception for {endpoint}: {e}\", file=sys.stderr)\n        return False, None\n\n\ndef _get_analysis_results(headers: Dict[str, str]) -> Optional[Dict]:\n    \"\"\"Get ANALYSIS_RESULTS.json file content.\"\"\"\n    success, file_data = _get_github_api(\"contents/ANALYSIS_RESULTS.json\", headers)\n    if not success:\n        return None\n\n    # Decode base64 content\n    import base64\n\n    content = file_data.get(\"content\", \"\")\n    if content:\n        try:\n            decoded_content = base64.b64decode(content).decode(\"utf-8\")\n            return json.loads(decoded_content)\n        except Exception as e:\n            print(f\"Error parsing JSON: {e}\", file=sys.stderr)\n            return None\n    return None\n\n\ndef _verify_commit_data(results: Dict, headers: Dict[str, str]) -> bool:\n    \"\"\"Verify the commit data is accurate.\"\"\"\n    commit_sha = results.get(\"target_commit_sha\")\n\n    # Validate SHA format\n    if not re.match(r\"^[a-f0-9]{40}$\", commit_sha, re.IGNORECASE):\n        print(f\"Error: Invalid commit SHA format: {commit_sha}\", file=sys.stderr)\n        return False\n\n    # Get commit details\n    success, commit_data = _get_github_api(f\"commits/{commit_sha}\", headers)\n    if not success:\n        print(f\"Error: Commit {commit_sha} not found in repository\", file=sys.stderr)\n        return False\n\n    # Verify author\n    expected_author = results.get(\"commit_author\")\n    actual_author = commit_data.get(\"author\", {}).get(\"login\")\n    if expected_author != actual_author:\n        print(\n            f\"Error: Commit author mismatch. Expected: {expected_author}, Actual: {actual_author}\",\n            file=sys.stderr,\n        )\n        return False\n\n    # Verify date format\n    commit_date = results.get(\"commit_date\")\n    if not re.match(r\"^\\d{4}-\\d{2}-\\d{2}$\", commit_date):\n        print(\n            f\"Error: Invalid date format: {commit_date}. Expected YYYY-MM-DD\",\n            file=sys.stderr,\n        )\n        return False\n\n    return True\n\n\ndef _verify_parameter_changes(results: Dict, headers: Dict[str, str]) -> bool:\n    \"\"\"Verify the parameter changes are accurate.\"\"\"\n    param_changes = results.get(\"parameter_changes\", {})\n\n    # Check required parameters exist\n    required_params = [\n        \"micro_batch_size_per_device_for_update\",\n        \"micro_batch_size_per_device_for_experience\",\n    ]\n    for param in required_params:\n        if param not in param_changes:\n            print(f\"Error: Missing parameter change data for: {param}\", file=sys.stderr)\n            return False\n\n        change_data = param_changes[param]\n        if not all(key in change_data for key in [\"before\", \"after\", \"line_number\"]):\n            print(\n                f\"Error: Incomplete change data for parameter: {param}\", file=sys.stderr\n            )\n            return False\n\n    # Verify specific expected values based on known repository state\n    update_param = param_changes.get(\"micro_batch_size_per_device_for_update\", {})\n    if update_param.get(\"before\") != 4 or update_param.get(\"after\") != 1:\n        print(\n            \"Error: Incorrect values for micro_batch_size_per_device_for_update\",\n            file=sys.stderr,\n        )\n        return False\n\n    experience_param = param_changes.get(\n        \"micro_batch_size_per_device_for_experience\", {}\n    )\n    if experience_param.get(\"before\") != 16 or experience_param.get(\"after\") != 2:\n        print(\n            \"Error: Incorrect values for micro_batch_size_per_device_for_experience\",\n            file=sys.stderr,\n        )\n        return False\n\n    return True\n\n\ndef _get_all_issues_with_keywords(headers: Dict[str, str]) -> set:\n    \"\"\"Find all issues in repository that contain the required keywords.\"\"\"\n    required_keywords = [\"oom\", \"memory\", \"batch\", \"显存\"]\n    keyword_issues = set()\n\n    # Get all issues from repository (both open and closed)\n    page = 1\n    while True:\n        success, issues = _get_github_api(\n            f\"issues?state=all&per_page=100&page={page}\", headers\n        )\n        if not success or not issues:\n            break\n\n        for issue in issues:\n            issue_number = issue.get(\"number\")\n            title = issue.get(\"title\", \"\").lower()\n            body = issue.get(\"body\", \"\").lower() if issue.get(\"body\") else \"\"\n            issue_text = title + \" \" + body\n\n            # Check if any keyword appears in title or body\n            for keyword in required_keywords:\n                if keyword.lower() in issue_text:\n                    keyword_issues.add(issue_number)\n                    break\n\n        # If we got less than 100 issues, we're done\n        if len(issues) < 100:\n            break\n        page += 1\n\n    return keyword_issues\n\n\ndef _verify_issue_references(results: Dict, headers: Dict[str, str]) -> bool:\n    \"\"\"Verify the issue references contain the required keywords.\"\"\"\n    issue_number_list = results.get(\"related_issue_number_list\")\n\n    if not isinstance(issue_number_list, list) or len(issue_number_list) == 0:\n        print(\n            \"Error: related_issue_number_list must be a non-empty list\",\n            file=sys.stderr,\n        )\n        return False\n\n    # Required keywords to search for (case insensitive)\n    required_keywords = [\"oom\", \"memory\", \"batch\", \"显存\"]\n\n    # First, dynamically find all issues that contain the required keywords\n    expected_issues = _get_all_issues_with_keywords(headers)\n    print(expected_issues)\n    provided_issues = set(issue_number_list)\n\n    # Verify each provided issue contains at least one of the required keywords\n    for issue_number in issue_number_list:\n        if not isinstance(issue_number, int) or issue_number <= 0:\n            print(\n                f\"Error: Invalid issue number format: {issue_number}\", file=sys.stderr\n            )\n            return False\n\n        # Get issue details\n        success, issue_data = _get_github_api(f\"issues/{issue_number}\", headers)\n        if not success:\n            print(\n                f\"Error: Issue #{issue_number} not found in repository\", file=sys.stderr\n            )\n            return False\n\n        # Check if issue title or body contains any required keywords\n        title = issue_data.get(\"title\", \"\").lower()\n        body = issue_data.get(\"body\", \"\").lower() if issue_data.get(\"body\") else \"\"\n        issue_text = title + \" \" + body\n\n        issue_has_keyword = False\n        for keyword in required_keywords:\n            if keyword.lower() in issue_text:\n                issue_has_keyword = True\n                break\n\n        if not issue_has_keyword:\n            print(\n                f\"Error: Issue #{issue_number} does not contain any required keywords: {required_keywords}\",\n                file=sys.stderr,\n            )\n            return False\n\n    # Verify agent found exactly the same issues as our dynamic search\n    if provided_issues != expected_issues:\n        missing = expected_issues - provided_issues\n        extra = provided_issues - expected_issues\n        if missing:\n            print(\n                f\"Error: Missing issues that contain required keywords: {missing}\",\n                file=sys.stderr,\n            )\n        if extra:\n            print(\n                f\"Error: Extra issues that don't contain required keywords: {extra}\",\n                file=sys.stderr,\n            )\n        return False\n\n    print(\n        f\"✓ Found all {len(issue_number_list)} issues containing required keywords: {issue_number_list}\"\n    )\n    return True\n\n\ndef verify() -> bool:\n    \"\"\"\n    Programmatically verify that the deep commit analysis meets the requirements.\n    \"\"\"\n    # Get GitHub token\n    github_token = os.environ.get(\"MCP_GITHUB_TOKEN\")\n    if not github_token:\n        print(\"Error: MCP_GITHUB_TOKEN environment variable not set\", file=sys.stderr)\n        return False\n\n    headers = {\n        \"Authorization\": f\"token {github_token}\",\n        \"Accept\": \"application/vnd.github.v3+json\",\n    }\n\n    print(\"Verifying deep commit analysis completion...\")\n\n    # 1. Check ANALYSIS_RESULTS.json exists and is valid JSON\n    print(\"1. Checking ANALYSIS_RESULTS.json exists and is valid...\")\n    results = _get_analysis_results(headers)\n    if not results:\n        print(\"Error: ANALYSIS_RESULTS.json not found or invalid JSON\", file=sys.stderr)\n        return False\n\n    print(\"✓ Found valid ANALYSIS_RESULTS.json\")\n\n    # 2. Verify commit data accuracy\n    print(\"2. Verifying commit data accuracy...\")\n    if not _verify_commit_data(results, headers):\n        return False\n\n    print(\"✓ Commit SHA, author, and date verified\")\n\n    # 3. Verify parameter changes accuracy\n    print(\"3. Verifying parameter changes accuracy...\")\n    if not _verify_parameter_changes(results, headers):\n        return False\n\n    print(\"✓ Parameter changes verified with correct before/after values\")\n\n    # 4. Verify issue references\n    print(\"4. Verifying issue references...\")\n    if not _verify_issue_references(results, headers):\n        return False\n\n    print(\"\\n✓ Task completed successfully!\")\n    print(\"Deep commit analysis results verified:\")\n    print(f\"- Found target commit: {results.get('target_commit_sha')}\")\n    print(\n        \"- Verified parameter changes: micro_batch_size_per_device_for_update (4→1), micro_batch_size_per_device_for_experience (16→2)\"\n    )\n    print(\n        f\"- Verified memory/performance issue correlations: {results.get('related_issue_number_list')}\"\n    )\n    print(\"- All data obtained through accurate GitHub API analysis\")\n\n    return True\n\n\nif __name__ == \"__main__\":\n    success = verify()\n    sys.exit(0 if success else 1)\n"
}