{
  "task_id": "gaming_accessories_analysis",
  "task_name": "Gaming Accessories Analysis",
  "category_id": "shopping",
  "category_name": "Shopping",
  "description": "Research gaming peripherals by analyzing technical specifications, comparing performance metrics, evaluating user reviews, tracking price trends, and creating detailed gaming accessory recommendations.",
  "author": "Yaoqi Ye",
  "created_at": "2025-08-17",
  "difficulty": "L3",
  "tags": [
    "search aggregation",
    "comparative analysis",
    "data extraction"
  ],
  "mcp": [
    "playwright"
  ],
  "meta_data": {
    "stateType": "video",
    "stateContent": null,
    "stateUrl": "",
    "stateOriginalUrl": "https://github.com/web-arena-x/webarena/tree/main/environment_docker"
  },
  "instruction": "\n\n**Task Requirements:**\n\n1. In Video Games category, count products with customer rating 70% or higher in the first 2 pages\n\n2. Sort products by price (ascending) and identify the cheapest product that has customer reviews\n\n3. Find product with SKU 'B07D6LSCXZ' (N64 Controller), add to cart with quantity 3\n\n4. Add products with SKU 'B071DR5V1K' and 'B082LZ4451' to comparison list, then count total products on comparison page\n\n5. In cart, update N64 Controller quantity to 5 and record the subtotal for this item\n\n6. Proceed to checkout and fill shipping form:\n   - Email: test.buyer@example.com\n   - First Name: Alice\n   - Last Name: Johnson\n   - Street Address: 456 Oak Avenue\n   - Country: United States\n   - State/Province: California\n   - City: San Francisco\n   - Zip Code: 94102\n   - Phone: 415-555-0123\n   Then count available shipping methods\n\n7. Output your findings in this format:\n\n```\n<answer>\nProducts70Plus|count\nCheapestReviewedSKU|sku\nCheapestReviewedPrice|price\nComparisonCount|count\nN64Subtotal|amount\nCheckoutEmail|test.buyer@example.com\nShippingState|California\nShippingMethods|count\n</answer>\n```\n\n",
  "verify": "import asyncio\nimport sys\nimport re\nimport os\nimport json\nfrom pathlib import Path\n\n\ndef get_model_response():\n    \"\"\"\n    Get the model's response from the MCP_MESSAGES environment variable.\n    Returns the last assistant message text.\n    \"\"\"\n    messages_path = os.getenv(\"MCP_MESSAGES\")\n    print(f\"MCP_MESSAGES: {messages_path}\")\n    if not messages_path:\n        print(\"Warning: MCP_MESSAGES environment variable not set\", file=sys.stderr)\n        return None\n\n    try:\n        with open(messages_path, \"r\") as f:\n            messages = json.load(f)\n\n        # Find the last assistant message\n        for message in reversed(messages):\n            if (\n                message.get(\"role\") == \"assistant\"\n                and message.get(\"status\") == \"completed\"\n                and message.get(\"type\") == \"message\"\n            ):\n                content = message.get(\"content\", [])\n                for item in content:\n                    if item.get(\"type\") == \"output_text\":\n                        return item.get(\"text\", \"\")\n\n        print(\"Warning: No assistant response found in messages\", file=sys.stderr)\n        return None\n    except Exception as e:\n        print(f\"Error reading messages file: {str(e)}\", file=sys.stderr)\n        return None\n\n\ndef parse_answer_format(text):\n    \"\"\"\n    Parse the <answer>...</answer> format from the agent's output.\n    Returns a dictionary with the parsed values.\n    \"\"\"\n    if not text:\n        return None\n\n    # Look for <answer>...</answer> pattern\n    match = re.search(r\"<answer>(.*?)</answer>\", text, re.IGNORECASE | re.DOTALL)\n    if not match:\n        return None\n\n    answer_content = match.group(1).strip()\n\n    # Parse each line\n    result = {}\n    lines = answer_content.split(\"\\n\")\n\n    if len(lines) != 8:\n        print(f\"Error: Expected 8 lines in answer, got {len(lines)}\", file=sys.stderr)\n        return None\n\n    for line in lines:\n        if \"|\" in line:\n            key, value = line.split(\"|\", 1)\n            result[key.strip()] = value.strip()\n\n    return result\n\n\ndef load_expected_answer(label_path):\n    \"\"\"\n    Load the expected answer from label.txt file.\n    Returns a dictionary with the expected values.\n    \"\"\"\n    try:\n        with open(label_path, \"r\") as f:\n            lines = f.read().strip().split(\"\\n\")\n\n        expected = {}\n        for line in lines:\n            if \"|\" in line:\n                key, value = line.split(\"|\", 1)\n                expected[key.strip()] = value.strip()\n\n        return expected\n    except Exception as e:\n        print(f\"Error reading label file: {str(e)}\", file=sys.stderr)\n        return None\n\n\ndef compare_answers(model_answer, expected_answer):\n    \"\"\"\n    Compare the model's answer with the expected answer.\n    Returns True if all key information matches, False otherwise.\n    \"\"\"\n    if not model_answer or not expected_answer:\n        return False\n\n    # Check each expected key\n    mismatches = []\n    for key, expected_value in expected_answer.items():\n        model_value = model_answer.get(key, \"\")\n\n        # Special handling for different types of values\n        if key in [\"CheapestReviewedPrice\", \"N64Subtotal\"]:\n            # For price fields, only support $XX.XX format\n            # Check if model value has correct format\n            if not model_value.startswith(\"$\"):\n                mismatches.append(\n                    f\"{key}: incorrect format - expected '$XX.XX' format, got '{model_value}'\"\n                )\n            else:\n                # Normalize and compare values\n                expected_clean = expected_value.replace(\"$\", \"\").replace(\",\", \"\")\n                model_clean = model_value.replace(\"$\", \"\").replace(\",\", \"\")\n                if expected_clean != model_clean:\n                    mismatches.append(\n                        f\"{key}: expected '{expected_value}', got '{model_value}'\"\n                    )\n\n        elif key == \"CheckoutEmail\":\n            # Email should match exactly (case-insensitive)\n            if model_value.lower() != expected_value.lower():\n                mismatches.append(\n                    f\"{key}: expected '{expected_value}', got '{model_value}'\"\n                )\n\n        elif key == \"Products70Plus\":\n            # For count fields, allow some flexibility (products might change)\n            # But still check if it's a reasonable number\n            try:\n                model_count = int(model_value)\n                expected_count = int(expected_value)\n                # Allow up to 2 products difference (in case of dynamic content)\n                if abs(model_count - expected_count) > 2:\n                    mismatches.append(\n                        f\"{key}: expected around '{expected_value}', got '{model_value}'\"\n                    )\n            except ValueError:\n                mismatches.append(\n                    f\"{key}: expected '{expected_value}', got '{model_value}'\"\n                )\n\n        else:\n            # Exact match for other fields\n            if model_value != expected_value:\n                mismatches.append(\n                    f\"{key}: expected '{expected_value}', got '{model_value}'\"\n                )\n\n    if mismatches:\n        print(\"\\n=== Answer Comparison Mismatches ===\", file=sys.stderr)\n        for mismatch in mismatches:\n            print(f\"✗ {mismatch}\", file=sys.stderr)\n        return False\n\n    print(\"\\n=== Answer Comparison ===\", file=sys.stderr)\n    print(\"✓ All key information matches the expected answer\", file=sys.stderr)\n    return True\n\n\nasync def verify() -> bool:\n    \"\"\"\n    Verifies that the gaming accessories analysis task has been completed correctly.\n    Checks the model's answer against the expected label.\n    \"\"\"\n    # Get the label file path\n    label_path = Path(__file__).parent / \"label.txt\"\n\n    # Load expected answer\n    expected_answer = load_expected_answer(label_path)\n    if not expected_answer:\n        print(\"Error: Could not load expected answer from label.txt\", file=sys.stderr)\n        return False\n\n    # Get model's response from MCP_MESSAGES\n    model_response = get_model_response()\n    if model_response:\n        print(\"Found model response, parsing answer format...\", file=sys.stderr)\n        model_answer = parse_answer_format(model_response)\n\n        if model_answer:\n            print(\"\\n=== Model Answer Parsed ===\", file=sys.stderr)\n            for key, value in model_answer.items():\n                print(f\"{key}: {value}\", file=sys.stderr)\n\n            # Compare answers\n            answer_match = compare_answers(model_answer, expected_answer)\n            if not answer_match:\n                print(\"\\nModel answer does not match expected answer\", file=sys.stderr)\n                return False\n            print(\"\\n✓ Model answer matches expected answer\", file=sys.stderr)\n            return True\n        else:\n            print(\n                \"Warning: Could not parse answer format from model response\",\n                file=sys.stderr,\n            )\n            return False\n    else:\n        print(\"No model response found\", file=sys.stderr)\n        return False\n\n\ndef main():\n    \"\"\"\n    Executes the verification process and exits with a status code.\n    \"\"\"\n    result = asyncio.run(verify())\n    sys.exit(0 if result else 1)\n\n\nif __name__ == \"__main__\":\n    main()"
}