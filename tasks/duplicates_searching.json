{
  "task_id": "duplicates_searching",
  "task_name": "Duplicates Searching",
  "category_id": "file_context",
  "category_name": "File Context",
  "description": "Scan directory to identify files with identical content, then organize all duplicate files into a separate dedicated directory for cleanup.",
  "author": "Lingjun Chen",
  "created_at": "2025-08-06",
  "difficulty": "L3",
  "tags": [
    "pattern analysis",
    "file organization"
  ],
  "mcp": [
    "filesystem"
  ],
  "metadata": {},
  "instruction": "# File Duplicates Detection and Organization Task\n\n## üìã Task Description\n\nYou are given a directory containing multiple text files. Some files have identical content and need to be organized. Your task is to identify all files with duplicate content and move them to a newly created 'duplicates' directory.\n\n## üéØ Task Objectives\n\n1. **Scan all text files** in the test directory to identify groups with identical content\n2. **Create a 'duplicates' directory** in the test directory root\n3. **Move all duplicate files** into the 'duplicates' directory\n4. **Leave unique files** in their original location\n\n## üìù Expected Output\n\nAfter completing the task, the directory structure should be:\n\n- `duplicates/` directory containing all files with duplicate content\n- Original directory containing only files with unique content\n",
  "verify": "#!/usr/bin/env python3\n\"\"\"\nVerification script for File Duplicates Detection and Organization Task\n\"\"\"\n\nimport sys\nfrom pathlib import Path\nimport os\nimport hashlib\n\ndef get_test_directory() -> Path:\n    \"\"\"Get the test directory from FILESYSTEM_TEST_DIR env var.\"\"\"\n    test_root = os.environ.get(\"FILESYSTEM_TEST_DIR\")\n    if not test_root:\n        raise ValueError(\"FILESYSTEM_TEST_DIR environment variable is required\")\n    return Path(test_root)\n\ndef calculate_file_hash(file_path: Path) -> str:\n    \"\"\"Calculate MD5 hash of file content.\"\"\"\n    try:\n        with open(file_path, 'rb') as f:\n            return hashlib.md5(f.read()).hexdigest()\n    except Exception as e:\n        print(f\"‚ùå Error reading file {file_path}: {e}\")\n        return None\n\ndef verify_duplicates_directory_exists(test_dir: Path) -> bool:\n    \"\"\"Verify that the duplicates directory exists.\"\"\"\n    duplicates_dir = test_dir / \"duplicates\"\n    \n    if not duplicates_dir.exists():\n        print(\"‚ùå 'duplicates' directory not found\")\n        return False\n    \n    if not duplicates_dir.is_dir():\n        print(\"‚ùå 'duplicates' exists but is not a directory\")\n        return False\n    \n    print(\"‚úÖ 'duplicates' directory exists\")\n    return True\n\ndef get_expected_duplicate_groups():\n    \"\"\"Return the expected duplicate file groups based on content analysis.\"\"\"\n    # Based on the answer.md and content analysis\n    return {\n        # Group 1: file_01.txt, file_02.txt (identical content)\n        \"group1\": [\"file_01.txt\", \"file_02.txt\"],\n        # Group 2: file_03.txt, file_04.txt (identical content)\n        \"group2\": [\"file_03.txt\", \"file_04.txt\"],\n        # Group 3: file_07.txt, file_08.txt (identical content)\n        \"group3\": [\"file_07.txt\", \"file_08.txt\"],\n        # Group 4: file_10.txt, file_11.txt (identical content)\n        \"group4\": [\"file_10.txt\", \"file_11.txt\"],\n        # Group 5: file_13.txt, file_14.txt (identical content)\n        \"group5\": [\"file_13.txt\", \"file_14.txt\"],\n        # Group 6: file_15.txt, file_16.txt (identical content)\n        \"group6\": [\"file_15.txt\", \"file_16.txt\"],\n        # Group 7: file_18.txt, file_19.txt (identical content)\n        \"group7\": [\"file_18.txt\", \"file_19.txt\"]\n    }\n\ndef get_expected_unique_files():\n    \"\"\"Return the expected unique files that should remain in original location.\"\"\"\n    return [\n        \"file_05.txt\", \"file_06.txt\", \"file_09.txt\", \n        \"file_12.txt\", \"file_17.txt\", \"file_20.txt\"\n    ]\n\ndef verify_duplicate_files_moved(test_dir: Path) -> bool:\n    \"\"\"Verify that all duplicate files have been moved to the duplicates directory.\"\"\"\n    duplicates_dir = test_dir / \"duplicates\"\n    expected_duplicate_groups = get_expected_duplicate_groups()\n    \n    # Check that all expected duplicate files are in the duplicates directory\n    missing_files = []\n    for group_name, files in expected_duplicate_groups.items():\n        for filename in files:\n            file_path = duplicates_dir / filename\n            if not file_path.exists():\n                missing_files.append(filename)\n    \n    if missing_files:\n        print(f\"‚ùå Missing duplicate files in 'duplicates' directory: {missing_files}\")\n        return False\n    \n    print(\"‚úÖ All expected duplicate files are in the 'duplicates' directory\")\n    return True\n\ndef verify_unique_files_remain(test_dir: Path) -> bool:\n    \"\"\"Verify that unique files remain in the original location.\"\"\"\n    expected_unique_files = get_expected_unique_files()\n    \n    missing_files = []\n    for filename in expected_unique_files:\n        file_path = test_dir / filename\n        if not file_path.exists():\n            missing_files.append(filename)\n    \n    if missing_files:\n        print(f\"‚ùå Missing unique files in original location: {missing_files}\")\n        return False\n    \n    print(\"‚úÖ All expected unique files remain in the original location\")\n    return True\n\ndef verify_no_duplicate_files_in_original(test_dir: Path) -> bool:\n    \"\"\"Verify that no duplicate files remain in the original location.\"\"\"\n    expected_duplicate_groups = get_expected_duplicate_groups()\n    \n    remaining_duplicates = []\n    for group_name, files in expected_duplicate_groups.items():\n        for filename in files:\n            file_path = test_dir / filename\n            if file_path.exists():\n                remaining_duplicates.append(filename)\n    \n    if remaining_duplicates:\n        print(f\"‚ùå Duplicate files still exist in original location: {remaining_duplicates}\")\n        return False\n    \n    print(\"‚úÖ No duplicate files remain in the original location\")\n    return True\n\ndef verify_content_integrity(test_dir: Path) -> bool:\n    \"\"\"Verify that file content integrity is maintained after moving.\"\"\"\n    duplicates_dir = test_dir / \"duplicates\"\n    expected_duplicate_groups = get_expected_duplicate_groups()\n    \n    # Check that files in each duplicate group have identical content\n    for group_name, files in expected_duplicate_groups.items():\n        if len(files) < 2:\n            continue\n            \n        # Calculate hash of the first file in the group\n        first_file = duplicates_dir / files[0]\n        if not first_file.exists():\n            print(f\"‚ùå First file of group {group_name} not found: {files[0]}\")\n            return False\n        \n        first_hash = calculate_file_hash(first_file)\n        if first_hash is None:\n            return False\n        \n        # Check that all other files in the group have the same hash\n        for filename in files[1:]:\n            file_path = duplicates_dir / filename\n            if not file_path.exists():\n                print(f\"‚ùå File in group {group_name} not found: {filename}\")\n                return False\n            \n            file_hash = calculate_file_hash(file_path)\n            if file_hash is None:\n                return False\n            \n            if file_hash != first_hash:\n                print(f\"‚ùå Files in group {group_name} have different content: {files[0]} vs {filename}\")\n                return False\n    \n    print(\"‚úÖ Content integrity verified - duplicate files have identical content\")\n    return True\n\ndef verify_total_file_count(test_dir: Path) -> bool:\n    \"\"\"Verify that the duplicates directory contains exactly 14 files.\"\"\"\n    duplicates_dir = test_dir / \"duplicates\"\n    \n    # Count files in original location (excluding the duplicates directory itself)\n    original_files = [f for f in test_dir.iterdir() if f.is_file()]\n    \n    # Count files in duplicates directory\n    duplicate_files = [f for f in duplicates_dir.iterdir() if f.is_file()]\n    \n    # Expected: 14 files in duplicates directory\n    expected_duplicates = 14\n    actual_duplicates = len(duplicate_files)\n    \n    if actual_duplicates != expected_duplicates:\n        print(f\"‚ùå Wrong number of files in duplicates directory. Expected: {expected_duplicates}, Actual: {actual_duplicates}\")\n        return False\n    \n    print(f\"‚úÖ Duplicates directory has correct number of files: {actual_duplicates}\")\n    return True\n\n\n\ndef main():\n    \"\"\"Main verification function.\"\"\"\n    test_dir = get_test_directory()\n    print(\"üîç Verifying File Duplicates Detection and Organization Task...\")\n    \n    # Define verification steps\n    verification_steps = [\n        (\"Duplicates Directory Exists\", verify_duplicates_directory_exists),\n        (\"Duplicate Files Moved\", verify_duplicate_files_moved),\n        (\"Unique Files Remain\", verify_unique_files_remain),\n        (\"No Duplicates in Original\", verify_no_duplicate_files_in_original),\n        (\"Content Integrity\", verify_content_integrity),\n        (\"Duplicates Count\", verify_total_file_count),\n    ]\n    \n    # Run all verification steps\n    all_passed = True\n    for step_name, verify_func in verification_steps:\n        print(f\"\\n--- {step_name} ---\")\n        if not verify_func(test_dir):\n            all_passed = False\n    \n    # Final result\n    print(\"\\n\" + \"=\"*50)\n    if all_passed:\n        print(\"‚úÖ File duplicates detection and organization completed correctly!\")\n        print(\"üéâ Task verification: PASS\")\n        sys.exit(0)\n    else:\n        print(\"‚ùå Task verification: FAIL\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()",
  "model_results": {
    "gemini-2-5-pro": 0,
    "deepseek-chat": 0,
    "qwen-3-coder": 0,
    "o3": 1,
    "gpt-5": 0,
    "k2": 0,
    "claude-4-sonnet": 0
  }
}