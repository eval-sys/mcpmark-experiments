{
  "task_id": "llm_research_summary",
  "task_name": "LLM Research Summary",
  "category_id": "reddit",
  "category_name": "Reddit",
  "description": "Aggregate and analyze LLM research discussions across multiple forums, collect trending topics, compile technical insights, and create comprehensive summary post with community engagement.",
  "author": "Fanqing Meng",
  "created_at": "2025-08-12",
  "difficulty": "L3",
  "tags": [
    "data extraction",
    "search aggregation",
    "content submission",
    "user interaction"
  ],
  "mcp": [
    "playwright"
  ],
  "meta_data": {
    "stateType": "video",
    "stateContent": null,
    "stateUrl": "https://storage.mcpmark.ai/tasks_state/playwright_video/postmill.mp4",
    "stateOriginalUrl": "https://github.com/web-arena-x/webarena/tree/main/environment_docker"
  },
  "instruction": "I need you to perform a comprehensive analysis of Large Language Model discussions on this forum platform and create a summary post. This analysis will help me understand the community's engagement with LLM topics.\n\n**Task Requirements:**\n\n1. Register a new account with username 'llm_analyst_2024' and password 'LLMExpert2024!'\n\n2. After successful registration, navigate to the MachineLearning forum and analyze posts about Large Language Models:\n   - Count all posts on the first page that contain 'GPT', 'ChatGPT', or 'LLM' in their titles\n\n3. Identify the top 3 posts with the highest upvote counts from your LLM-related posts list and record their complete details\n\n4. Navigate to the deeplearning forum and go to page 2:\n   - Find the post with the most comments on page 2\n   - Record its exact title and comment count\n\n5. Create a new submission in the MachineLearning forum with:\n   - Title: \"LLM Research Summary: GPT Discussions Analysis [2024]\"\n   - Body text must be EXACTLY these lines without anything (keep the keys as-is, only replace the values after the pipe, follow the markdown format):\n\n```\n- Total_LLM_Posts|FILL_VALUE\n- Top1_Title|FILL_VALUE\n- Top1_Upvotes|FILL_VALUE\n- Top1_Date|FILL_VALUE\n- Top2_Title|FILL_VALUE\n- Top2_Upvotes|FILL_VALUE\n- Top2_Date|FILL_VALUE\n- Top3_Title|FILL_VALUE\n- Top3_Upvotes|FILL_VALUE\n- Top3_Date|FILL_VALUE\n- Deeplearning_MostDiscussed|FILL_VALUE\n- Deeplearning_Comments|FILL_VALUE\n```",
  "verify": "import asyncio\nimport sys\nimport re\nimport os\nfrom pathlib import Path\nfrom playwright.async_api import (\n    async_playwright,\n    TimeoutError as PlaywrightTimeoutError,\n)\n\n# 从环境变量读取 base_url，默认回退到本地\nBASE_URL = os.getenv(\"WEBARENA_BASE_URL\", \"http://localhost:9999\").rstrip(\"/\")\n\n\ndef parse_key_value_format(text):\n    \"\"\"\n    Parse the Key|Value format from the submission body.\n    Handles both pipe (|) and colon (:) separators for compatibility.\n    \"\"\"\n    data = {}\n    \n    # Try to parse with pipe separator first (expected format)\n    lines = text.strip().split('\\n')\n    for line in lines:\n        line = line.strip()\n        if not line:\n            continue\n        \n        # Remove markdown list prefix if present\n        if line.startswith('- '):\n            line = line[2:]\n        elif line.startswith('* '):\n            line = line[2:]\n        \n        # Try pipe separator first\n        if '|' in line:\n            parts = line.split('|', 1)\n            if len(parts) == 2:\n                key = parts[0].strip()\n                value = parts[1].strip()\n                data[key] = value\n        # Fallback to colon separator for label.txt compatibility\n        elif ':' in line:\n            parts = line.split(':', 1)\n            if len(parts) == 2:\n                key = parts[0].strip()\n                value = parts[1].strip()\n                data[key] = value\n    \n    return data\n\n\ndef normalize_text(text):\n    \"\"\"\n    Normalize text for comparison by handling different quote styles and whitespace.\n    \"\"\"\n    if not isinstance(text, str):\n        return str(text)\n\n    # Replace various quote styles with standard quotes\n    text = text.replace(\"\"\", \"'\").replace(\"\"\", \"'\")\n    text = text.replace('\"', '\"').replace('\"', '\"')\n\n    # Normalize whitespace\n    text = \" \".join(text.split())\n\n    return text.strip()\n\n\nasync def verify() -> bool:\n    \"\"\"\n    Verifies that the LLM analysis task has been completed correctly.\n    \"\"\"\n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=True)\n        context = await browser.new_context()\n        page = await context.new_page()\n\n        try:\n            # Navigate to the main page\n            print(\"Navigating to forum...\", file=sys.stderr)\n            await page.goto(f\"{BASE_URL}/\", wait_until=\"networkidle\")\n\n            # Check if logged in as llm_analyst_2024\n            user_button = page.locator('button:has-text(\"llm_analyst_2024\")')\n            if not await user_button.count():\n                # Try to login\n                print(\"Not logged in, attempting to login...\", file=sys.stderr)\n\n                await page.click('a:has-text(\"Log in\")')\n                await page.wait_for_load_state(\"networkidle\")\n\n                await page.fill('input[name=\"_username\"]', \"llm_analyst_2024\")\n                await page.fill('input[name=\"_password\"]', \"LLMExpert2024!\")\n\n                await page.click('button:has-text(\"Log in\")')\n                await page.wait_for_load_state(\"networkidle\")\n\n                user_button = page.locator('button:has-text(\"llm_analyst_2024\")')\n                if not await user_button.count():\n                    print(\"Error: Login failed for llm_analyst_2024\", file=sys.stderr)\n                    return False\n\n                print(\"Successfully logged in as llm_analyst_2024\", file=sys.stderr)\n            else:\n                print(\"Already logged in as llm_analyst_2024\", file=sys.stderr)\n\n            # Navigate to MachineLearning forum\n            print(\"Navigating to MachineLearning forum...\", file=sys.stderr)\n            await page.goto(\n                f\"{BASE_URL}/f/MachineLearning\", wait_until=\"networkidle\"\n            )\n\n            # Look for the submission with our specific title\n            print(\n                \"Looking for submission 'LLM Research Summary: GPT Discussions Analysis [2024]'...\",\n                file=sys.stderr,\n            )\n            post_link = page.locator(\n                'a:has-text(\"LLM Research Summary: GPT Discussions Analysis [2024]\")'\n            )\n\n            if not await post_link.count():\n                print(\n                    \"Error: Could not find submission with required title\",\n                    file=sys.stderr,\n                )\n                return False\n\n            # Click on the submission to view its content\n            await post_link.first.click()\n            await page.wait_for_load_state(\"networkidle\")\n\n            # Extract the submission body content\n            # Try multiple possible selectors for the post body\n            post_content = None\n            selectors = [\n                \".submission__body\",\n                \".post-body\",\n                \".RichText\",\n                '[class*=\"RichText\"]',\n                'div:has(> p:has-text(\"Total_LLM_Posts\"))',\n                'div:has-text(\"Total_LLM_Posts\"):has-text(\"Deeplearning_Comments\")',\n            ]\n\n            for selector in selectors:\n                content_element = page.locator(selector)\n                if await content_element.count():\n                    post_content = await content_element.first.inner_text()\n                    if \"Total_LLM_Posts\" in post_content:\n                        print(\n                            f\"Found submission content using selector: {selector}\",\n                            file=sys.stderr,\n                        )\n                        break\n\n            if not post_content or \"Total_LLM_Posts\" not in post_content:\n                print(\n                    \"Error: Could not find submission body with required format\",\n                    file=sys.stderr,\n                )\n                return False\n\n            print(\"Submission content found, parsing data...\", file=sys.stderr)\n            print(f\"Raw content: {post_content[:200]}...\", file=sys.stderr)\n\n            # Parse the Key: Value format\n            extracted_data = parse_key_value_format(post_content)\n            print(f\"Extracted data: {extracted_data}\", file=sys.stderr)\n\n            # Load expected values from label.txt\n            label_path = Path(__file__).parent / \"label.txt\"\n            if label_path.exists():\n                with open(label_path, \"r\") as f:\n                    expected_text = f.read().strip()\n                expected_data = parse_key_value_format(expected_text)\n                print(\"Loaded expected values from label.txt\", file=sys.stderr)\n\n            # Verify all required keys are present\n            required_keys = [\n                \"Total_LLM_Posts\",\n                \"Top1_Title\",\n                \"Top1_Upvotes\",\n                \"Top1_Date\",\n                \"Top2_Title\",\n                \"Top2_Upvotes\",\n                \"Top2_Date\",\n                \"Top3_Title\",\n                \"Top3_Upvotes\",\n                \"Top3_Date\",\n                \"Deeplearning_MostDiscussed\",\n                \"Deeplearning_Comments\",\n            ]\n\n            missing_keys = []\n            for key in required_keys:\n                if key not in extracted_data:\n                    missing_keys.append(key)\n\n            if missing_keys:\n                print(\n                    f\"Error: Missing required keys: {', '.join(missing_keys)}\",\n                    file=sys.stderr,\n                )\n                return False\n\n            # Validate data format and content\n            errors = []\n\n            # Check Total_LLM_Posts is a number and matches expected\n            try:\n                total_posts = int(extracted_data[\"Total_LLM_Posts\"])\n                if \"expected_data\" in locals() and \"Total_LLM_Posts\" in expected_data:\n                    expected_total = int(expected_data[\"Total_LLM_Posts\"])\n                    if total_posts != expected_total:\n                        errors.append(\n                            f\"Total_LLM_Posts mismatch: got {total_posts}, expected {expected_total}\"\n                        )\n                elif total_posts < 5:  # Based on exploration, should be at least 5\n                    errors.append(f\"Total_LLM_Posts seems too low: {total_posts}\")\n            except ValueError:\n                errors.append(\n                    f\"Total_LLM_Posts must be a number, got: {extracted_data['Total_LLM_Posts']}\"\n                )\n\n            # If we have expected data, compare against it\n            if \"expected_data\" in locals():\n                # Compare each field\n                for key in required_keys:\n                    if key in expected_data and key in extracted_data:\n                        expected_val = normalize_text(expected_data[key])\n                        actual_val = normalize_text(extracted_data[key])\n\n                        # For numeric fields, compare as integers\n                        if (\n                            \"Upvotes\" in key\n                            or \"Comments\" in key\n                            or key == \"Total_LLM_Posts\"\n                        ):\n                            try:\n                                expected_int = int(expected_val)\n                                actual_int = int(actual_val)\n                                if expected_int != actual_int:\n                                    errors.append(\n                                        f\"{key} mismatch: got {actual_int}, expected {expected_int}\"\n                                    )\n                            except ValueError:\n                                errors.append(\n                                    f\"{key} should be numeric: got '{actual_val}'\"\n                                )\n                        else:\n                            # For text fields, compare normalized text\n                            if expected_val != actual_val:\n                                errors.append(\n                                    f\"{key} mismatch: got '{actual_val}', expected '{expected_val}'\"\n                                )\n\n            else:\n                # If no expected data, just do basic validation\n                for key in required_keys:\n                    if key not in extracted_data:\n                        errors.append(f\"Missing required key: {key}\")\n                    elif (\n                        not extracted_data[key] or extracted_data[key] == \"[FILL_VALUE]\"\n                    ):\n                        errors.append(f\"{key} was not filled in\")\n\n            # Verify upvotes are in descending order for top 3\n            try:\n                top1_votes = int(extracted_data[\"Top1_Upvotes\"])\n                top2_votes = int(extracted_data[\"Top2_Upvotes\"])\n                top3_votes = int(extracted_data[\"Top3_Upvotes\"])\n\n                if not (top1_votes >= top2_votes >= top3_votes):\n                    errors.append(\n                        f\"Top posts should be ordered by upvotes: {top1_votes} >= {top2_votes} >= {top3_votes}\"\n                    )\n            except (ValueError, KeyError):\n                pass  # Already reported above\n\n            if errors:\n                print(\n                    \"Error: Validation failed with the following issues:\",\n                    file=sys.stderr,\n                )\n                for error in errors:\n                    print(f\"  - {error}\", file=sys.stderr)\n                return False\n\n            # All checks passed\n            print(\"Success: LLM analysis task completed successfully.\")\n            print(\"- Account llm_analyst_2024 verified\")\n            print(\n                \"- Submission 'LLM Research Summary: GPT Discussions Analysis [2024]' found\"\n            )\n            print(\n                f\"- Total LLM-related posts analyzed: {extracted_data['Total_LLM_Posts']}\"\n            )\n            print(\"- Top 3 posts by upvotes identified and documented\")\n            print(\n                f\"- Deeplearning forum page 2 most discussed post: {extracted_data['Deeplearning_MostDiscussed']}\"\n            )\n            print(\"- All data in correct Key: Value format with 12 lines\")\n            return True\n\n        except PlaywrightTimeoutError as e:\n            print(f\"Error: Timeout occurred - {str(e)}\", file=sys.stderr)\n            return False\n        except Exception as e:\n            print(f\"Error: Unexpected error - {str(e)}\", file=sys.stderr)\n            return False\n        finally:\n            await browser.close()\n\n\ndef main():\n    \"\"\"\n    Executes the verification process and exits with a status code.\n    \"\"\"\n    result = asyncio.run(verify())\n    sys.exit(0 if result else 1)\n\n\nif __name__ == \"__main__\":\n    main()\n"
}