{
  "task_id": "dispute_review",
  "task_name": "Dispute Review",
  "category_id": "legal_document",
  "category_name": "Legal Document",
  "description": "Analyze multiple versions of legal documents to track clause discussion frequency and generate a comprehensive dispute summary report.",
  "author": "Lingjun Chen",
  "created_at": "2025-08-15",
  "difficulty": "L3",
  "tags": [
    "data extraction",
    "cross-referencing",
    "pattern analysis"
  ],
  "mcp": [
    "filesystem"
  ],
  "metadata": {},
  "instruction": "# Legal Document Dispute Review Task\n\n**Overview**\n\nThe folder \"legal_files/\" contains all versions (Preferred_Stock_Purchase_Agreement_v0.txt  -- Preferred_Stock_Purchase_Agreement_v10.txt) of the Stock Purchase Agreement for a corporate investment project.\n\nThere are comments in it, come from four people:\n\n- **Bill Harvey** (Company CEO)\n- **Michelle Jackson** (Investor)\n- **David Russel** (Company Counsel)\n- **Tony Taylor** (Investor Counsel)\n\nBetween v1 and v9, these four people make comments on the clauses. The comment format is `[name:content]`, where:\n\n- `name` is the commenter's name\n- `content` is the revision note\n\n**Special Note:** If the name is \"All parties\", it represents a joint comment from all parties, which counts as one comment but does not count toward any individual's personal comment count.\n\n## Task\n\nYour task is to review these versions and identify all clauses that have been commented in **v5,6,7 (in folder legal_files/)**. Generate a file named `dispute_review.txt` in the main directory. In this file, list each commented clause on a separate line and indicate the number of comments for each clause in the format \"Clause number:number of comments\". Clause number should be in the format of X.X.\n",
  "verify": "#!/usr/bin/env python3\n\"\"\"\nVerification script for Legal Document Dispute Review Task\n\"\"\"\n\nimport sys\nfrom pathlib import Path\nimport re\nimport os\n\ndef get_test_directory() -> Path:\n    \"\"\"Get the test directory from FILESYSTEM_TEST_DIR env var.\"\"\"\n    test_root = os.environ.get(\"FILESYSTEM_TEST_DIR\")\n    if not test_root:\n        raise ValueError(\"FILESYSTEM_TEST_DIR environment variable is required\")\n    return Path(test_root)\n\ndef verify_output_file_exists(test_dir: Path) -> bool:\n    \"\"\"Verify that the dispute_review.txt file exists.\"\"\"\n    output_file = test_dir / \"dispute_review.txt\"\n    \n    if not output_file.exists():\n        print(\"‚ùå File 'dispute_review.txt' not found\")\n        return False\n    \n    print(\"‚úÖ Output file found\")\n    return True\n\ndef verify_output_format(test_dir: Path) -> bool:\n    \"\"\"Verify that the output file has the correct format.\"\"\"\n    output_file = test_dir / \"dispute_review.txt\"\n    \n    try:\n        content = output_file.read_text().strip()\n        \n        # Check if content is not empty\n        if not content:\n            print(\"‚ùå Output file is empty\")\n            return False\n        \n        # Check format: each line should be \"X.X:number\"\n        lines = content.split('\\n')\n        for i, line in enumerate(lines, 1):\n            line = line.strip()\n            if not line:\n                continue\n                \n            # Check format: X.X:number\n            if not re.match(r'^\\d+\\.\\d+:\\d+$', line):\n                print(f\"‚ùå Line {i} has incorrect format: '{line}'\")\n                print(\"   Expected format: 'X.X:number' (e.g., '1.1:3')\")\n                return False\n        \n        print(\"‚úÖ Output format is correct\")\n        return True\n        \n    except Exception as e:\n        print(f\"‚ùå Error reading output file: {e}\")\n        return False\n\ndef verify_expected_entries(test_dir: Path) -> bool:\n    \"\"\"Verify that the output contains the expected entries with correct counts.\"\"\"\n    output_file = test_dir / \"dispute_review.txt\"\n    \n    try:\n        content = output_file.read_text().strip()\n        lines = content.split('\\n')\n        \n        # Parse the output into a dictionary\n        output_entries = {}\n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n            clause, count_str = line.split(':', 1)\n            output_entries[clause] = int(count_str)\n        \n        # Expected entries based on answer.txt\n        expected_entries = {\n            \"1.1\": 3,\n            \"1.3\": 3,\n            \"4.6\": [5, 6],  # Can be either 5 or 6\n            \"4.16\": 5,\n            \"6.8\": 4\n        }\n        \n        # Check if all expected entries are present\n        missing_entries = []\n        for clause in expected_entries:\n            if clause not in output_entries:\n                missing_entries.append(clause)\n        \n        if missing_entries:\n            print(f\"‚ùå Missing expected entries: {missing_entries}\")\n            return False\n        \n        # Check if there are extra entries\n        extra_entries = []\n        for clause in output_entries:\n            if clause not in expected_entries:\n                extra_entries.append(clause)\n        \n        if extra_entries:\n            print(f\"‚ùå Unexpected extra entries: {extra_entries}\")\n            return False\n        \n        # Check counts for each entry\n        for clause, expected_count in expected_entries.items():\n            actual_count = output_entries[clause]\n            \n            if isinstance(expected_count, list):\n                # For 4.6, accept either 5 or 6\n                if actual_count not in expected_count:\n                    print(f\"‚ùå Clause {clause}: expected {expected_count}, got {actual_count}\")\n                    return False\n            else:\n                if actual_count != expected_count:\n                    print(f\"‚ùå Clause {clause}: expected {expected_count}, got {actual_count}\")\n                    return False\n        \n        print(\"‚úÖ All expected entries with correct counts\")\n        return True\n        \n    except Exception as e:\n        print(f\"‚ùå Error verifying entries: {e}\")\n        return False\n\ndef verify_comment_count_accuracy(test_dir: Path) -> bool:\n    \"\"\"Verify that the comment counts are accurate by checking the actual files.\"\"\"\n    # Since we already verify the expected entries in verify_expected_entries,\n    # and the answer.txt contains the correct counts, we can skip this complex verification\n    # to avoid false negatives due to regex matching issues.\n    \n    print(\"‚úÖ Comment count accuracy check skipped - relying on expected entries verification\")\n    return True\n\ndef main():\n    \"\"\"Main verification function.\"\"\"\n    test_dir = get_test_directory()\n    print(\"üîç Verifying Legal Document Dispute Review Task...\")\n    \n    # Define verification steps\n    verification_steps = [\n        (\"Output File Exists\", verify_output_file_exists),\n        (\"Output Format\", verify_output_format),\n        (\"Expected Entries\", verify_expected_entries),\n        (\"Comment Count Accuracy\", verify_comment_count_accuracy),\n    ]\n    \n    # Run all verification steps\n    all_passed = True\n    for step_name, verify_func in verification_steps:\n        print(f\"\\n--- {step_name} ---\")\n        if not verify_func(test_dir):\n            all_passed = False\n    \n    # Final result\n    print(\"\\n\" + \"=\"*50)\n    if all_passed:\n        print(\"‚úÖ Legal document dispute review completed correctly!\")\n        print(\"üéâ Task verification: PASS\")\n        sys.exit(0)\n    else:\n        print(\"‚ùå Task verification: FAIL\")\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()",
  "model_results": {
    "gemini-2-5-pro": 0,
    "deepseek-chat": 0,
    "qwen-3-coder": 0,
    "o3": 1,
    "gpt-5": 1,
    "k2": 0,
    "claude-4-sonnet": 0
  }
}