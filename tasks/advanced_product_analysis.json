{
  "task_id": "advanced_product_analysis",
  "task_name": "Advanced Product Analysis",
  "category_id": "shopping",
  "category_name": "Shopping",
  "description": "Perform comprehensive product analysis including feature comparisons, price tracking, review aggregation, customer sentiment analysis, and generate detailed recommendation reports for informed purchasing decisions.",
  "author": "Yaoqi Ye",
  "created_at": "2025-08-17",
  "difficulty": "L3",
  "tags": [
    "data extraction",
    "comparative analysis",
    "content submission"
  ],
  "mcp": [
    "playwright"
  ],
  "meta_data": {
    "stateType": "video",
    "stateContent": null,
    "stateUrl": "https://storage.mcpmark.ai/tasks_state/playwright_video/one-stop-market.mp4",
    "stateOriginalUrl": "https://github.com/web-arena-x/webarena/tree/main/environment_docker"
  },
  "instruction": "\n\n**Task Requirements:**\n\n1. Search for products with 'Ginger' in the Product Name field and price range $50.00 to $100.00\n\n2. Add Q Mixers Premium Ginger Ale product to the comparison list\n\n3. Find Intel NUC Kit product in Electronics category and add it to the comparison list\n\n4. From the comparison page:\n   - Record SKU numbers for both products\n   - Add all products to cart\n\n5. Record the total cart value\n\n6. On the Ginger Ale product detail page, record:\n   - Number of customer reviews\n   - Name of the most recent reviewer (on top of the first page)\n\n7. Output your findings in this format:\n\n```\n<answer>\nGingerAleSKU|sku\nIntelNUCSKU|sku\nCartTotal|amount\nReviewCount|count\nLatestReviewer|name\n</answer>\n```\n\n**Example Output:**\n```\n<answer>\nGingerAleSKU|XXXXXXXXX\nIntelNUCSKU|XXXXXXXXX\nCartTotal|$XXX.XX\nReviewCount|XX\nLatestReviewer|name\n</answer>\n```\n\n",
  "verify": "import asyncio\nimport sys\nimport re\nimport os\nimport json\nfrom pathlib import Path\n\n\ndef get_model_response():\n    \"\"\"\n    Get the model's response from the MCP_MESSAGES environment variable.\n    Returns the last assistant message text.\n    \"\"\"\n    messages_path = os.getenv(\"MCP_MESSAGES\")\n    print(f\"MCP_MESSAGES: {messages_path}\")\n    if not messages_path:\n        print(\"Warning: MCP_MESSAGES environment variable not set\", file=sys.stderr)\n        return None\n\n    try:\n        with open(messages_path, \"r\") as f:\n            messages = json.load(f)\n\n        # Find the last assistant message\n        for message in reversed(messages):\n            if (\n                message.get(\"role\") == \"assistant\"\n                and message.get(\"status\") == \"completed\"\n            ):\n                content = message.get(\"content\", [])\n                for item in content:\n                    if item.get(\"type\") == \"output_text\":\n                        return item.get(\"text\", \"\")\n\n        print(\"Warning: No assistant response found in messages\", file=sys.stderr)\n        return None\n    except Exception as e:\n        print(f\"Error reading messages file: {str(e)}\", file=sys.stderr)\n        return None\n\n\ndef parse_answer_format(text):\n    \"\"\"\n    Parse the <answer>xxx</answer> format from the agent's output.\n    Returns a dictionary with the parsed values.\n    \"\"\"\n    if not text:\n        return None\n\n    # Look for <answer>...</answer> pattern\n    match = re.search(r\"<answer>(.*?)</answer>\", text, re.IGNORECASE | re.DOTALL)\n    if not match:\n        return None\n\n    answer_content = match.group(1).strip()\n\n    # Parse each line\n    result = {}\n    lines = answer_content.split(\"\\n\")\n\n    if len(lines) != 5:\n        print(f\"Error: Expected 5 lines in answer, got {len(lines)}\", file=sys.stderr)\n        return None\n\n    for line in lines:\n        if \"|\" in line:\n            key, value = line.split(\"|\", 1)\n            result[key.strip()] = value.strip()\n\n    return result\n\n\ndef load_expected_answer(label_path):\n    \"\"\"\n    Load the expected answer from label.txt file.\n    Returns a dictionary with the expected values.\n    \"\"\"\n    try:\n        with open(label_path, \"r\") as f:\n            lines = f.read().strip().split(\"\\n\")\n\n        expected = {}\n        for line in lines:\n            if \"|\" in line:\n                key, value = line.split(\"|\", 1)\n                expected[key.strip()] = value.strip()\n\n        return expected\n    except Exception as e:\n        print(f\"Error reading label file: {str(e)}\", file=sys.stderr)\n        return None\n\n\ndef compare_answers(model_answer, expected_answer):\n    \"\"\"\n    Compare the model's answer with the expected answer.\n    Returns True if all key information matches, False otherwise.\n    \"\"\"\n    if not model_answer or not expected_answer:\n        return False\n\n    # Check each expected key\n    mismatches = []\n    for key, expected_value in expected_answer.items():\n        model_value = model_answer.get(key, \"\")\n\n        # Special handling for different types of values\n        if key == \"GingerAleSKU\":\n            # Check exact SKU match\n            if model_value != expected_value:\n                mismatches.append(\n                    f\"{key}: expected '{expected_value}', got '{model_value}'\"\n                )\n\n        elif key == \"IntelNUCSKU\":\n            # Check exact SKU match\n            if model_value != expected_value:\n                mismatches.append(\n                    f\"{key}: expected '{expected_value}', got '{model_value}'\"\n                )\n\n        elif key == \"CartTotal\":\n            # For price fields, only support $XX.XX format\n            # Check if model value has correct format\n            if not model_value.startswith(\"$\"):\n                mismatches.append(\n                    f\"{key}: incorrect format - expected '$XX.XX' format, got '{model_value}'\"\n                )\n            else:\n                # Normalize and compare values\n                expected_clean = expected_value.replace(\"$\", \"\").replace(\",\", \"\")\n                model_clean = model_value.replace(\"$\", \"\").replace(\",\", \"\")\n                if expected_clean != model_clean:\n                    mismatches.append(\n                        f\"{key}: expected '{expected_value}', got '{model_value}'\"\n                    )\n\n        elif key == \"ReviewCount\":\n            # Check review count matches\n            if model_value != expected_value:\n                mismatches.append(\n                    f\"{key}: expected '{expected_value}', got '{model_value}'\"\n                )\n\n        elif key == \"LatestReviewer\":\n            # Check reviewer name (allow partial match for names)\n            if expected_value.lower() not in model_value.lower() and model_value.lower() not in expected_value.lower():\n                mismatches.append(\n                    f\"{key}: expected '{expected_value}', got '{model_value}'\"\n                )\n\n        else:\n            # Exact match for other fields\n            if model_value != expected_value:\n                mismatches.append(\n                    f\"{key}: expected '{expected_value}', got '{model_value}'\"\n                )\n\n    if mismatches:\n        print(\"\\n=== Answer Comparison Mismatches ===\", file=sys.stderr)\n        for mismatch in mismatches:\n            print(f\"✗ {mismatch}\", file=sys.stderr)\n        return False\n\n    print(\"\\n=== Answer Comparison ===\", file=sys.stderr)\n    print(\"✓ All key information matches the expected answer\", file=sys.stderr)\n    return True\n\n\nasync def verify() -> bool:\n    \"\"\"\n    Verifies that the advanced product analysis task has been completed correctly.\n    First checks the model's answer against the expected label.\n    \"\"\"\n    # Get the label file path\n    label_path = Path(__file__).parent / \"label.txt\"\n\n    # Load expected answer\n    expected_answer = load_expected_answer(label_path)\n    if not expected_answer:\n        print(\"Error: Could not load expected answer from label.txt\", file=sys.stderr)\n        return False\n\n    # Get model's response from MCP_MESSAGES\n    model_response = get_model_response()\n    if model_response:\n        print(\"Found model response, parsing answer format...\", file=sys.stderr)\n        model_answer = parse_answer_format(model_response)\n\n        if model_answer:\n            print(\"\\n=== Model Answer Parsed ===\", file=sys.stderr)\n            for key, value in model_answer.items():\n                print(f\"{key}: {value}\", file=sys.stderr)\n\n            # Compare answers\n            answer_match = compare_answers(model_answer, expected_answer)\n            if not answer_match:\n                print(\"\\nModel answer does not match expected answer\", file=sys.stderr)\n                return False\n            print(\"\\n✓ Model answer matches expected answer\", file=sys.stderr)\n            return True\n        else:\n            print(\n                \"Warning: Could not parse answer format from model response\",\n                file=sys.stderr,\n            )\n            return False\n    else:\n        print(\"No model response found\", file=sys.stderr)\n        return False\n\n\ndef main():\n    \"\"\"\n    Executes the verification process and exits with a status code.\n    \"\"\"\n    result = asyncio.run(verify())\n    sys.exit(0 if result else 1)\n\n\nif __name__ == \"__main__\":\n    main()"
}