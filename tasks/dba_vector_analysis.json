{
  "task_id": "dba_vector_analysis",
  "task_name": "DBA Vector Analysis",
  "category_id": "vectors",
  "category_name": "Vectors",
  "description": "Analyze pgvector database storage, identify vector columns, assess space utilization and performance for RAG applications.",
  "author": "Fanshi Zhang",
  "created_at": "2025-08-18",
  "difficulty": "L3",
  "tags": [
    "performance optimization",
    "audit and compliance",
    "statistical aggregation"
  ],
  "mcp": [
    "postgres"
  ],
  "metadata": {},
  "instruction": "Analyze vector database storage, identify vector columns, and assess space utilization for a PostgreSQL database with pgvector extension.\n\n## Your Mission:\n\nYou are a PostgreSQL DBA tasked with analyzing a vector database that stores embeddings for RAG (Retrieval-Augmented Generation) applications. The database uses the pgvector extension and contains multiple tables with vector columns storing high-dimensional embeddings.\n\n## Analysis Requirements:\n\n1. **Vector Column Discovery**:\n   - Identify all tables containing vector columns\n   - Determine the dimensions of each vector column\n   - Find which schemas contain vector data\n   - Catalog the vector column data types and constraints\n\n2. **Storage Analysis**:\n   - Calculate storage space used by vector columns\n   - Determine total table sizes including vector data\n   - Analyze storage efficiency and space utilization\n   - Compare vector storage vs. regular column storage\n\n3. **Performance Assessment**:\n   - Identify existing vector indexes (HNSW, IVFFlat)\n   - Analyze index types and their configurations\n   - Assess query performance implications\n   - Review index storage overhead\n\n4. **Data Quality Evaluation**:\n   - Check for NULL vector values\n   - Verify vector dimension consistency\n   - Identify any orphaned or incomplete vector data\n   - Validate vector normalization (if applicable)\n\n5. **Extension Analysis**:\n   - Verify pgvector extension installation and version\n   - Check extension permissions and availability\n   - Review vector-specific functions and operators\n   - Assess extension configuration\n\n## Expected Deliverables:\n\nStore your analysis results in the following database tables:\n\n1. **vector_column_inventory**: Complete inventory of vector columns\n   - Include: table_name, column_name, schema_name, vector_dimensions, data_type\n   \n2. **vector_storage_analysis**: Storage space analysis for vector data\n   - Include: table_name, total_size_bytes, vector_storage_bytes, regular_storage_bytes, record_count\n   \n3. **vector_index_analysis**: Analysis of vector indexes and performance\n   - Include: index_name, table_name, index_type, index_size_bytes, index_method\n   \n4. **vector_data_quality**: Data quality assessment results\n   - Include: table_name, quality_check_type, issue_count, total_records, quality_status\n   \n5. **vector_analysis_summary**: Overall findings and recommendations\n   - Include: analysis_category, finding_description, severity_level, recommendation\n\n## Key Questions to Answer:\n\n1. Which tables and schemas contain vector columns?\n2. What are the dimensions and data types of each vector column?\n3. How much storage space is consumed by vector data vs. regular data?\n4. What types of vector indexes exist and how effective are they?\n5. Are there any data quality issues with the vector columns?\n6. What is the total overhead of the pgvector extension?\n7. How can storage efficiency be improved?\n8. What are the performance characteristics of vector operations?\n\n## Technical Focus Areas:\n\n- **Storage Management**: Understanding vector storage patterns and optimization\n- **Index Strategy**: Analyzing vector index effectiveness and configuration\n- **Performance Monitoring**: Assessing query performance for vector operations\n- **Capacity Planning**: Projecting future storage requirements\n- **Data Governance**: Ensuring vector data quality and consistency\n\nUse PostgreSQL system catalogs, pgvector-specific views, and storage analysis functions to gather comprehensive metrics about the vector database implementation.",
  "verify": "\"\"\"\nVerification script for Vector Database DBA Analysis task.\n\nThis script verifies that the candidate has properly analyzed the vector database\nand stored their findings in appropriate result tables.\n\"\"\"\n\nimport logging\nimport psycopg2\nimport os\nimport sys\nfrom typing import Dict, List, Tuple, Any\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_connection_params():\n    \"\"\"Get database connection parameters from environment variables.\"\"\"\n    return {\n        \"host\": os.getenv(\"POSTGRES_HOST\", \"localhost\"),\n        \"port\": int(os.getenv(\"POSTGRES_PORT\", 5432)),\n        \"database\": os.getenv(\"POSTGRES_DATABASE\"),\n        \"user\": os.getenv(\"POSTGRES_USERNAME\"),\n        \"password\": os.getenv(\"POSTGRES_PASSWORD\"),\n    }\n\n\ndef verify_vector_column_inventory(conn) -> Dict[str, Any]:\n    \"\"\"Verify that vector columns were properly inventoried.\"\"\"\n    results = {'score': 0, 'max_score': 25, 'issues': []}\n    \n    try:\n        with conn.cursor() as cur:\n            # Check if vector column inventory table exists\n            cur.execute(\"\"\"\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables \n                    WHERE table_name = 'vector_column_inventory'\n                );\n            \"\"\")\n            \n            if not cur.fetchone()[0]:\n                results['issues'].append(\"vector_column_inventory table not found\")\n                return results\n            \n            # Get the inventory results\n            cur.execute(\"SELECT * FROM vector_column_inventory ORDER BY table_name, column_name;\")\n            inventory = cur.fetchall()\n            \n            if not inventory:\n                results['issues'].append(\"No vector columns found in inventory\")\n                return results\n            \n            # Expected vector columns in the test database\n            expected_vector_columns = {\n                ('documents', 'embedding'),\n                ('document_chunks', 'embedding'), \n                ('user_queries', 'embedding')\n            }\n            \n            found_columns = set()\n            for row in inventory:\n                # Extract table and column names (adjust based on actual table structure)\n                if len(row) >= 2:\n                    table_name = row[0] if row[0] else row[1]  # flexible column order\n                    column_name = row[1] if row[0] else row[2]\n                    found_columns.add((table_name, column_name))\n            \n            # Calculate score based on coverage\n            matched_columns = found_columns & expected_vector_columns\n            results['score'] = (len(matched_columns) / len(expected_vector_columns)) * results['max_score']\n            \n            if len(matched_columns) < len(expected_vector_columns):\n                missing = expected_vector_columns - matched_columns\n                results['issues'].append(f\"Missing vector columns: {missing}\")\n            \n            print(f\"✅ Vector column inventory: {len(found_columns)} columns found, {len(matched_columns)}/{len(expected_vector_columns)} expected columns\")\n            \n    except psycopg2.Error as e:\n        results['issues'].append(f\"Database error in vector column inventory: {e}\")\n    \n    return results\n\n\ndef verify_storage_analysis(conn) -> Dict[str, Any]:\n    \"\"\"Verify that storage analysis was performed.\"\"\"\n    results = {'score': 0, 'max_score': 25, 'issues': []}\n    \n    try:\n        with conn.cursor() as cur:\n            # Check if storage analysis table exists\n            cur.execute(\"\"\"\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables \n                    WHERE table_name = 'vector_storage_analysis'\n                );\n            \"\"\")\n            \n            if not cur.fetchone()[0]:\n                results['issues'].append(\"vector_storage_analysis table not found\")\n                return results\n            \n            # Get storage analysis results\n            cur.execute(\"SELECT * FROM vector_storage_analysis;\")\n            storage_analysis = cur.fetchall()\n            \n            if not storage_analysis:\n                results['issues'].append(\"No storage analysis results found\")\n                return results\n            \n            # Check that key tables are analyzed\n            expected_tables = {'documents', 'document_chunks', 'user_queries'}\n            analyzed_tables = set()\n            \n            for row in storage_analysis:\n                # Extract table name (flexible based on column structure)\n                if len(row) >= 1:\n                    table_name = str(row[0]).strip() if row[0] else ''\n                    if table_name:\n                        analyzed_tables.add(table_name)\n            \n            matched_tables = analyzed_tables & expected_tables\n            results['score'] = (len(matched_tables) / len(expected_tables)) * results['max_score']\n            \n            if len(matched_tables) < len(expected_tables):\n                missing = expected_tables - matched_tables\n                results['issues'].append(f\"Missing storage analysis for tables: {missing}\")\n            \n            print(f\"✅ Storage analysis: {len(analyzed_tables)} tables analyzed, {len(matched_tables)}/{len(expected_tables)} expected tables\")\n            \n    except psycopg2.Error as e:\n        results['issues'].append(f\"Database error in storage analysis: {e}\")\n    \n    return results\n\n\ndef verify_index_analysis(conn) -> Dict[str, Any]:\n    \"\"\"Verify that vector index analysis was performed.\"\"\"\n    results = {'score': 0, 'max_score': 20, 'issues': []}\n    \n    try:\n        with conn.cursor() as cur:\n            # Check if index analysis table exists\n            cur.execute(\"\"\"\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables \n                    WHERE table_name = 'vector_index_analysis'\n                );\n            \"\"\")\n            \n            if not cur.fetchone()[0]:\n                results['issues'].append(\"vector_index_analysis table not found\")\n                return results\n            \n            # Get index analysis results\n            cur.execute(\"SELECT * FROM vector_index_analysis;\")\n            index_analysis = cur.fetchall()\n            \n            if not index_analysis:\n                results['issues'].append(\"No index analysis results found\")\n                return results\n            \n            # Check that vector indexes were identified\n            # The test database should have at least 3 vector indexes\n            if len(index_analysis) >= 3:\n                results['score'] = results['max_score']\n                print(f\"✅ Index analysis: {len(index_analysis)} indexes analyzed\")\n            else:\n                results['score'] = (len(index_analysis) / 3) * results['max_score']\n                results['issues'].append(f\"Expected at least 3 vector indexes, found {len(index_analysis)}\")\n            \n    except psycopg2.Error as e:\n        results['issues'].append(f\"Database error in index analysis: {e}\")\n    \n    return results\n\n\ndef verify_data_quality_analysis(conn) -> Dict[str, Any]:\n    \"\"\"Verify that data quality analysis was performed.\"\"\"\n    results = {'score': 0, 'max_score': 15, 'issues': []}\n    \n    try:\n        with conn.cursor() as cur:\n            # Check if data quality analysis table exists\n            cur.execute(\"\"\"\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables \n                    WHERE table_name = 'vector_data_quality'\n                );\n            \"\"\")\n            \n            if not cur.fetchone()[0]:\n                results['issues'].append(\"vector_data_quality table not found\")\n                return results\n            \n            # Get data quality results\n            cur.execute(\"SELECT * FROM vector_data_quality;\")\n            quality_analysis = cur.fetchall()\n            \n            if not quality_analysis:\n                results['issues'].append(\"No data quality analysis found\")\n                return results\n            \n            # Check for analysis of null values, dimension consistency, etc.\n            if len(quality_analysis) >= 3:  # Expect analysis of at least 3 quality aspects\n                results['score'] = results['max_score']\n                print(f\"✅ Data quality analysis: {len(quality_analysis)} quality checks performed\")\n            else:\n                results['score'] = (len(quality_analysis) / 3) * results['max_score']\n                results['issues'].append(f\"Expected at least 3 quality checks, found {len(quality_analysis)}\")\n            \n    except psycopg2.Error as e:\n        results['issues'].append(f\"Database error in data quality analysis: {e}\")\n    \n    return results\n\n\ndef verify_analysis_summary(conn) -> Dict[str, Any]:\n    \"\"\"Verify that an overall analysis summary was created.\"\"\"\n    results = {'score': 0, 'max_score': 15, 'issues': []}\n    \n    try:\n        with conn.cursor() as cur:\n            # Check if analysis summary table exists\n            cur.execute(\"\"\"\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables \n                    WHERE table_name = 'vector_analysis_summary'\n                );\n            \"\"\")\n            \n            if not cur.fetchone()[0]:\n                results['issues'].append(\"vector_analysis_summary table not found\")\n                return results\n            \n            # Get summary results\n            cur.execute(\"SELECT * FROM vector_analysis_summary;\")\n            summary = cur.fetchall()\n            \n            if not summary:\n                results['issues'].append(\"No analysis summary found\")\n                return results\n            \n            # Check for comprehensive summary\n            if len(summary) >= 1:  # At least one summary record\n                results['score'] = results['max_score']\n                print(f\"✅ Analysis summary: {len(summary)} summary records created\")\n            else:\n                results['issues'].append(\"Analysis summary is incomplete\")\n            \n    except psycopg2.Error as e:\n        results['issues'].append(f\"Database error in analysis summary: {e}\")\n    \n    return results\n\n\ndef main():\n    \"\"\"Main verification function.\"\"\"\n    print(\"=\" * 60)\n    print(\"Verifying Vector Database DBA Analysis Results\")\n    print(\"=\" * 60)\n    \n    conn_params = get_connection_params()\n    \n    if not conn_params[\"database\"]:\n        print(\"❌ No database specified\")\n        sys.exit(1)\n    \n    try:\n        conn = psycopg2.connect(**conn_params)\n        \n        # Run all verification checks\n        total_score = 0\n        max_total_score = 100\n        all_issues = []\n        \n        print(\"\\n1. Verifying vector column inventory...\")\n        inventory_results = verify_vector_column_inventory(conn)\n        total_score += inventory_results['score']\n        all_issues.extend(inventory_results['issues'])\n        \n        print(\"\\n2. Verifying storage analysis...\")\n        storage_results = verify_storage_analysis(conn)\n        total_score += storage_results['score']\n        all_issues.extend(storage_results['issues'])\n        \n        print(\"\\n3. Verifying index analysis...\")\n        index_results = verify_index_analysis(conn)\n        total_score += index_results['score']\n        all_issues.extend(index_results['issues'])\n        \n        print(\"\\n4. Verifying data quality analysis...\")\n        quality_results = verify_data_quality_analysis(conn)\n        total_score += quality_results['score']\n        all_issues.extend(quality_results['issues'])\n        \n        print(\"\\n5. Verifying analysis summary...\")\n        summary_results = verify_analysis_summary(conn)\n        total_score += summary_results['score']\n        all_issues.extend(summary_results['issues'])\n        \n        conn.close()\n        \n        # Calculate percentage score\n        percentage_score = (total_score / max_total_score) * 100\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"Verification Results:\")\n        print(f\"Vector Column Inventory: {inventory_results['score']:.1f}/{inventory_results['max_score']}\")\n        print(f\"Storage Analysis: {storage_results['score']:.1f}/{storage_results['max_score']}\")\n        print(f\"Index Analysis: {index_results['score']:.1f}/{index_results['max_score']}\")\n        print(f\"Data Quality Analysis: {quality_results['score']:.1f}/{quality_results['max_score']}\")\n        print(f\"Analysis Summary: {summary_results['score']:.1f}/{summary_results['max_score']}\")\n        print(f\"Total Score: {total_score:.1f}/{max_total_score} ({percentage_score:.1f}%)\")\n        \n        # Show issues if any\n        if all_issues:\n            print(f\"\\nIssues found:\")\n            for issue in all_issues:\n                print(f\"  - {issue}\")\n        \n        # Determine overall result\n        if percentage_score >= 80:\n            print(\"\\n🎉 Vector DBA Analysis verification: PASS\")\n            print(\"Comprehensive analysis completed successfully.\")\n            sys.exit(0)\n        elif percentage_score >= 60:\n            print(\"\\n⚠️ Vector DBA Analysis verification: PARTIAL PASS\")\n            print(\"Analysis completed but some aspects were missed.\")\n            sys.exit(0)\n        else:\n            print(\"\\n❌ Vector DBA Analysis verification: FAIL\")\n            print(\"Analysis incomplete or missing critical components.\")\n            sys.exit(1)\n            \n    except psycopg2.Error as e:\n        print(f\"❌ Database connection error: {e}\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"❌ Verification error: {e}\")\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()"
}