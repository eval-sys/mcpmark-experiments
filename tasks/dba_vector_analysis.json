{
  "task_id": "dba_vector_analysis",
  "task_name": "DBA Vector Analysis",
  "category_id": "vectors",
  "category_name": "Vectors",
  "description": "Analyze pgvector database storage, identify vector columns, assess space utilization and performance for RAG applications.",
  "author": "Fanshi Zhang",
  "created_at": "2025-08-18",
  "difficulty": "L3",
  "tags": [
    "performance optimization",
    "audit and compliance",
    "statistical aggregation"
  ],
  "mcp": [
    "postgres"
  ],
  "meta_data": {
    "stateType": "text",
    "stateContent": "Table \"documents\" {\n  \"id\" int4 [pk, not null, increment]\n  \"title\" text [not null]\n  \"content\" text [not null]\n  \"source_url\" text\n  \"document_type\" varchar(50) [default: 'article']\n  \"created_at\" timestamp [default: `CURRENT_TIMESTAMP`]\n  \"updated_at\" timestamp [default: `CURRENT_TIMESTAMP`]\n  \"word_count\" int4\n  \"embedding\" public.vector\n\n  Indexes {\n    created_at [type: btree, name: \"documents_created_idx\"]\n    embedding [type: hnsw, name: \"documents_embedding_idx\"]\n    title [type: btree, name: \"documents_title_idx\"]\n    document_type [type: btree, name: \"documents_type_idx\"]\n  }\n}\n\nTable \"document_chunks\" {\n  \"id\" int4 [pk, not null, increment]\n  \"document_id\" int4\n  \"chunk_index\" int4 [not null]\n  \"chunk_text\" text [not null]\n  \"chunk_size\" int4\n  \"overlap_size\" int4 [default: 0]\n  \"created_at\" timestamp [default: `CURRENT_TIMESTAMP`]\n  \"embedding\" public.vector\n\n  Indexes {\n    document_id [type: btree, name: \"chunks_doc_id_idx\"]\n    embedding [type: hnsw, name: \"chunks_embedding_idx\"]\n    chunk_index [type: btree, name: \"chunks_index_idx\"]\n  }\n}\n\nTable \"user_queries\" {\n  \"id\" int4 [pk, not null, increment]\n  \"query_text\" text [not null]\n  \"user_id\" varchar(100)\n  \"session_id\" varchar(100)\n  \"created_at\" timestamp [default: `CURRENT_TIMESTAMP`]\n  \"response_time_ms\" int4\n  \"embedding\" public.vector\n\n  Indexes {\n    created_at [type: btree, name: \"queries_created_idx\"]\n    embedding [type: hnsw, name: \"queries_embedding_idx\"]\n    user_id [type: btree, name: \"queries_user_idx\"]\n  }\n}\n\nTable \"embedding_models\" {\n  \"id\" int4 [pk, not null, increment]\n  \"model_name\" varchar(100) [unique, not null]\n  \"provider\" varchar(50) [not null]\n  \"dimensions\" int4 [not null]\n  \"max_tokens\" int4\n  \"cost_per_token\" numeric(10,8)\n  \"created_at\" timestamp [default: `CURRENT_TIMESTAMP`]\n  \"is_active\" bool [default: true]\n}\n\nTable \"knowledge_base\" {\n  \"id\" int4 [pk, not null, increment]\n  \"kb_name\" varchar(100) [not null]\n  \"description\" text\n  \"domain\" varchar(50)\n  \"language\" varchar(10) [default: 'en']\n  \"total_documents\" int4 [default: 0]\n  \"total_chunks\" int4 [default: 0]\n  \"total_storage_mb\" numeric(10,2)\n  \"created_at\" timestamp [default: `CURRENT_TIMESTAMP`]\n  \"updated_at\" timestamp [default: `CURRENT_TIMESTAMP`]\n}\n\nTable \"search_cache\" {\n  \"id\" int4 [pk, not null, increment]\n  \"query_hash\" varchar(64) [not null]\n  \"query_text\" text [not null]\n  \"results_json\" jsonb\n  \"result_count\" int4\n  \"search_time_ms\" int4\n  \"similarity_threshold\" numeric(4,3)\n  \"created_at\" timestamp [default: `CURRENT_TIMESTAMP`]\n  \"expires_at\" timestamp\n\n  Indexes {\n    expires_at [type: btree, name: \"cache_expires_idx\"]\n    query_hash [type: btree, name: \"cache_hash_idx\"]\n  }\n}\n\nRef \"document_chunks_document_id_fkey\":\"documents\".\"id\" < \"document_chunks\".\"document_id\" [delete: cascade]\n",
    "stateUrl": null,
    "stateOriginalUrl": null
  },
  "instruction": "# PostgreSQL Vector Database Analysis\n\n> Analyze and optimize a pgvector-powered database to understand storage patterns, performance characteristics, and data quality for embeddings in production workloads.\n\n## What's this about?\n\nYou've got a PostgreSQL database running with the vector extension that stores embeddings for RAG (document similarity search, image recognition), or other ML workloads.\nYour job is to dive deep into this vector database and figure out what's going on under the hood.\nYou need to understand:\n\n- how vectors are stored\n- how much space they're taking up\n- whether indexes are working properly\n- if there are any data quality issues lurking around\n\n## What you need to investigate\n\nFirst, get familiar with what you're working with:\n\n- Check vector extension status: ensuring it's installed properly, check version, identify any configuration issues\n- Identify all vector columns across entire database: providing me columns, types of columns, and vector dim (dimensions)\n- Map the vector landscape: understand relationships between vector tables and regular tables, foreign keys, dependencies\n\nVectors can eat up a lot of storage, so let's see where the bytes are going:\n\n- Calculate vector storage overhead: measure how much space vectors take compared to regular columns in same tables\n- Analyze table sizes: identify which vector tables are biggest storage consumers, break down by table\n- Understand growth patterns: examine record counts and project future storage needs based on current data\n\nVectors without proper indexes are painfully slow, so investigate:\n\n- Catalog vector indexes: find all HNSW and IVFFlat indexes, document their configurations and parameters\n- Measure index effectiveness: determine if indexes are actually being used and helping query performance\n- Identify optimization opportunities: spot missing indexes, suboptimal configurations, unused indexes\n\nBad vector data makes everything worse:\n\n- Hunt for data issues: locate NULL vectors, dimension mismatches, corrupted embeddings that could break queries\n- Validate consistency: ensure vectors in each column have consistent dimensions across all rows\n- Check for outliers: find vectors that might be skewing similarity calculations or causing performance issues\n\n## Your deliverables\n\nCreate these analysis tables and populate them with your findings:\n\n### `vector_analysis_columns`\n\nComplete catalog of every vector column you find:\n\n```sql\nCREATE TABLE vector_analysis_columns (\n    schema VARCHAR(50),\n    table_name VARCHAR(100),\n    column_name VARCHAR(100),\n    dimensions INTEGER,\n    data_type VARCHAR(50),\n    has_constraints BOOLEAN,\n    rows BIGINT\n);\n```\n\n### `vector_analysis_storage_consumption`\n\nShow exactly where storage is being consumed:\n\n```sql\nCREATE TABLE vector_analysis_storage_consumption (\n    schema VARCHAR(50),\n    table_name VARCHAR(100),\n    total_size_bytes BIGINT,\n    vector_data_bytes BIGINT,\n    regular_data_bytes BIGINT,\n    vector_storage_pct NUMERIC(5,2),\n    row_count BIGINT\n);\n```\n\n### `vector_analysis_indices`\n\nDocument all vector indexes and their characteristics:\n```sql\nCREATE TABLE vector_analysis_indices (\n    schema VARCHAR(50),\n    table_name VARCHAR(100),\n    column_name VARCHAR(100),\n    index_name VARCHAR(100),\n    index_type VARCHAR(50), -- 'hnsw', 'ivfflat', etc.\n    index_size_bytes BIGINT\n);\n```\n\nUse PostgreSQL system catalogs, pgvector-specific views, and storage analysis functions to gather comprehensive metrics about the vector database implementation.\n",
  "verify": "\"\"\"\nVerification script for Vector Database DBA Analysis task.\n\nThis script verifies that the candidate has properly analyzed the vector database\nand stored their findings in appropriate result tables.\n\"\"\"\n\nimport logging\nimport psycopg2\nimport os\nimport sys\nfrom typing import Dict, Any\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_connection_params():\n    \"\"\"Get database connection parameters from environment variables.\"\"\"\n    return {\n        \"host\": os.getenv(\"POSTGRES_HOST\", \"localhost\"),\n        \"port\": int(os.getenv(\"POSTGRES_PORT\", 5432)),\n        \"database\": os.getenv(\"POSTGRES_DATABASE\"),\n        \"user\": os.getenv(\"POSTGRES_USER\"),\n        \"password\": os.getenv(\"POSTGRES_PASSWORD\"),\n    }\n\n\ndef verify_vector_analysis_columns(conn) -> Dict[str, Any]:\n    \"\"\"Verify the vector_analysis_columns table exists, has correct columns, and contains actual vector columns from the database.\"\"\"\n    results = {'passed': False, 'issues': []}\n    expected_columns = [\n        'schema', 'table_name', 'column_name', 'dimensions', 'data_type', 'has_constraints', 'rows'\n    ]\n    try:\n        with conn.cursor() as cur:\n            # Check if table exists\n            cur.execute(\"\"\"\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables\n                    WHERE table_name = 'vector_analysis_columns'\n                );\n            \"\"\")\n            if not cur.fetchone()[0]:\n                results['issues'].append(\"vector_analysis_columns table not found\")\n                return results\n\n            # Check columns\n            cur.execute(\"\"\"\n                SELECT column_name FROM information_schema.columns\n                WHERE table_name = 'vector_analysis_columns'\n                ORDER BY column_name;\n            \"\"\")\n            actual_columns = {row[0] for row in cur.fetchall()}\n            missing = set(expected_columns) - actual_columns\n            extra = actual_columns - set(expected_columns)\n            if missing:\n                results['issues'].append(f\"Missing columns: {missing}\")\n            if extra:\n                results['issues'].append(f\"Unexpected columns: {extra}\")\n\n            # Check for data\n            cur.execute(\"SELECT COUNT(*) FROM vector_analysis_columns;\")\n            count = cur.fetchone()[0]\n            if count == 0:\n                results['issues'].append(\"No rows found in vector_analysis_columns\")\n                return results\n\n            # Get actual vector columns from the database\n            cur.execute(\"\"\"\n                SELECT table_name, column_name\n                FROM information_schema.columns\n                WHERE data_type = 'USER-DEFINED'\n                AND udt_name = 'vector'\n                ORDER BY table_name, column_name;\n            \"\"\")\n            actual_vector_columns = set(cur.fetchall())\n\n            # Get what the agent found\n            cur.execute(\"\"\"\n                SELECT table_name, column_name\n                FROM vector_analysis_columns\n                ORDER BY table_name, column_name;\n            \"\"\")\n            found_vector_columns = set(cur.fetchall())\n\n            # Check if agent found the actual vector columns\n            missing_vectors = actual_vector_columns - found_vector_columns\n            extra_vectors = found_vector_columns - actual_vector_columns\n\n            if missing_vectors:\n                results['issues'].append(f\"Missing: {missing_vectors}\")\n            if extra_vectors:\n                results['issues'].append(f\"Non-existing: {extra_vectors}\")\n\n            if not missing and not extra and count > 0 and not missing_vectors and not extra_vectors:\n                results['passed'] = True\n\n    except psycopg2.Error as e:\n        results['issues'].append(f\"Database error: {e}\")\n    except Exception as e:\n        results['issues'].append(f\"Verification error: {e}\")\n    return results\n\n\ndef verify_vector_analysis_storage_consumption(conn) -> Dict[str, Any]:\n    \"\"\"Verify the vector_analysis_storage_consumption table exists, has correct columns, and analyzes actual vector tables.\"\"\"\n    results = {'passed': False, 'issues': []}\n    expected_columns = [\n        'schema', 'table_name', 'total_size_bytes', 'vector_data_bytes', 'regular_data_bytes', 'vector_storage_pct', 'row_count'\n    ]\n    try:\n        with conn.cursor() as cur:\n            cur.execute(\"\"\"\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables\n                    WHERE table_name = 'vector_analysis_storage_consumption'\n                );\n            \"\"\")\n            if not cur.fetchone()[0]:\n                results['issues'].append(\"vector_analysis_storage_consumption table not found\")\n                return results\n\n            cur.execute(\"\"\"\n                SELECT column_name FROM information_schema.columns\n                WHERE table_name = 'vector_analysis_storage_consumption'\n                ORDER BY column_name;\n            \"\"\")\n            actual_columns = {row[0] for row in cur.fetchall()}\n            missing = set(expected_columns) - actual_columns\n            extra = actual_columns - set(expected_columns)\n            if missing:\n                results['issues'].append(f\"Missing columns: {missing}\")\n            if extra:\n                results['issues'].append(f\"Unexpected columns: {extra}\")\n\n            cur.execute(\"SELECT COUNT(*) FROM vector_analysis_storage_consumption;\")\n            count = cur.fetchone()[0]\n            if count == 0:\n                results['issues'].append(\"No rows found in vector_analysis_storage_consumption\")\n                return results\n\n            # Get actual tables with vector columns\n            cur.execute(\"\"\"\n                SELECT DISTINCT table_name\n                FROM information_schema.columns\n                WHERE data_type = 'USER-DEFINED'\n                AND udt_name = 'vector'\n                ORDER BY table_name;\n            \"\"\")\n            actual_vector_tables = {row[0] for row in cur.fetchall()}\n\n            # Get what the agent analyzed\n            cur.execute(\"\"\"\n                SELECT DISTINCT table_name\n                FROM vector_analysis_storage_consumption\n                ORDER BY table_name;\n            \"\"\")\n            analyzed_tables = {row[0] for row in cur.fetchall()}\n\n            # Check if agent analyzed the actual vector tables\n            missing_tables = actual_vector_tables - analyzed_tables\n            if missing_tables:\n                results['issues'].append(f\"Agent missed analyzing vector tables: {missing_tables}\")\n\n            # Check that analyzed tables actually have vector columns\n            extra_tables = analyzed_tables - actual_vector_tables\n            if extra_tables:\n                results['issues'].append(f\"Agent analyzed non-vector tables: {extra_tables}\")\n\n            if not missing and not extra and count > 0 and not missing_tables and not extra_tables:\n                results['passed'] = True\n\n    except psycopg2.Error as e:\n        results['issues'].append(f\"Database error: {e}\")\n    except Exception as e:\n        results['issues'].append(f\"Verification error: {e}\")\n    return results\n\n\ndef verify_vector_analysis_indices(conn) -> Dict[str, Any]:\n    \"\"\"Verify the vector_analysis_indices table exists, has correct columns, and identifies actual vector indexes.\"\"\"\n    results = {'passed': False, 'issues': []}\n    expected_columns = [\n        'schema', 'table_name', 'column_name', 'index_name', 'index_type', 'index_size_bytes'\n    ]\n    try:\n        with conn.cursor() as cur:\n            cur.execute(\"\"\"\n                SELECT EXISTS (\n                    SELECT FROM information_schema.tables\n                    WHERE table_name = 'vector_analysis_indices'\n                );\n            \"\"\")\n            if not cur.fetchone()[0]:\n                results['issues'].append(\"vector_analysis_indices table not found\")\n                return results\n\n            cur.execute(\"\"\"\n                SELECT column_name FROM information_schema.columns\n                WHERE table_name = 'vector_analysis_indices'\n                ORDER BY column_name;\n            \"\"\")\n            actual_columns = {row[0] for row in cur.fetchall()}\n            missing = set(expected_columns) - actual_columns\n            extra = actual_columns - set(expected_columns)\n            if missing:\n                results['issues'].append(f\"Missing columns: {missing}\")\n            if extra:\n                results['issues'].append(f\"Unexpected columns: {extra}\")\n\n            cur.execute(\"SELECT COUNT(*) FROM vector_analysis_indices;\")\n            count = cur.fetchone()[0]\n            if count == 0:\n                results['issues'].append(\"No rows found in vector_analysis_indices\")\n                return results\n\n            # Get actual vector indexes from the database (exclude ground truth table indexes)\n            cur.execute(\"\"\"\n                SELECT schemaname, tablename, indexname\n                FROM pg_indexes\n                WHERE (indexdef ILIKE '%hnsw%' OR indexdef ILIKE '%ivfflat%')\n                AND tablename NOT LIKE '%analysis%'\n                ORDER BY tablename, indexname;\n            \"\"\")\n            actual_vector_indexes = set(cur.fetchall())\n\n            # Get what the agent found\n            cur.execute(\"\"\"\n                SELECT schema, table_name, index_name\n                FROM vector_analysis_indices\n                ORDER BY table_name, index_name;\n            \"\"\")\n            found_indexes = set(cur.fetchall())\n\n            # Check if agent found the actual vector indexes\n            missing_indexes = actual_vector_indexes - found_indexes\n            if missing_indexes:\n                results['issues'].append(f\"Agent missed vector indexes: {missing_indexes}\")\n\n            # Allow agent to find more indexes than just vector ones (they might include related indexes)\n            # but at least they should find the vector-specific ones\n\n            if not missing and not extra and count > 0 and not missing_indexes:\n                results['passed'] = True\n\n    except psycopg2.Error as e:\n        results['issues'].append(f\"Database error: {e}\")\n    except Exception as e:\n        results['issues'].append(f\"Verification error: {e}\")\n    return results\n\n\ndef verify_no_extra_analysis_tables(conn) -> Dict[str, Any]:\n    \"\"\"Check that only the required analysis tables exist (no legacy/extra analysis tables).\"\"\"\n    results = {'passed': True, 'issues': []}  # Start with passed=True, more lenient\n    required = {\n        'vector_analysis_columns',\n        'vector_analysis_storage_consumption',\n        'vector_analysis_indices',\n    }\n    try:\n        with conn.cursor() as cur:\n            cur.execute(\"\"\"\n                SELECT table_name FROM information_schema.tables\n                WHERE table_schema = 'public'\n                AND table_name LIKE 'vector_analysis_%';\n            \"\"\")\n            analysis_tables = {row[0] for row in cur.fetchall()}\n\n            # Only flag as issue if there are analysis tables that don't match our required set\n            # Exclude ground truth tables from this check\n            analysis_tables_filtered = {t for t in analysis_tables if not t.startswith('expected_') and not t.startswith('vector_analysis_results')}\n            extra = analysis_tables_filtered - required\n            if extra:\n                results['issues'].append(f\"Found unexpected analysis tables: {extra}\")\n                results['passed'] = False\n\n    except Exception as e:\n        results['issues'].append(f\"Verification error: {e}\")\n        results['passed'] = False\n    return results\n\n\n\ndef main():\n    \"\"\"Main verification function for vector analysis deliverables.\"\"\"\n\n    conn_params = get_connection_params()\n    if not conn_params[\"database\"]:\n        print(\"No database specified\")\n        sys.exit(1)\n    try:\n        conn = psycopg2.connect(**conn_params)\n        checks = [\n            (\"vector_analysis_columns\", verify_vector_analysis_columns),\n            (\"vector_analysis_storage_consumption\", verify_vector_analysis_storage_consumption),\n            (\"vector_analysis_indices\", verify_vector_analysis_indices),\n            (\"no_extra_analysis_tables\", verify_no_extra_analysis_tables),\n        ]\n        passed_checks = 0\n        all_issues = []\n        for i, (desc, check_func) in enumerate(checks, 1):\n            result = check_func(conn)\n            if result['passed']:\n                print(f\"  PASSED\")\n                passed_checks += 1\n            else:\n                print(f\"  FAILED\")\n                for issue in result['issues']:\n                    print(f\"    - {issue}\")\n                all_issues.extend(result['issues'])\n            print()\n        conn.close()\n        total_checks = len(checks)\n        print(f\"Results: {passed_checks}/{total_checks} checks passed\")\n        if passed_checks == total_checks:\n            sys.exit(0)\n        elif passed_checks >= total_checks * 0.75:\n            sys.exit(0)\n        else:\n            sys.exit(1)\n    except psycopg2.Error as e:\n        print(f\"Database connection error: {e}\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"Verification error: {e}\")\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n"
}